# Cohesive Subgroups

```{r}
#| label: libraries
#| message: false
library(igraph)
library(networkdata)
```

```{r}
#| label: libraries_silent
#| include: false
library(ggraph)
```


## Cliques 

A *clique* in a network is a set of nodes that form a complete subnetwork within a network (called a complete **subgraph**). A **maximal clique** is a clique that cannot be extended to a bigger clique by addding more nodes to it. 

```{r}
#| label: load-clique-graph
data("clique_graph")
```

All maximal cliques can be calculated with `max_cliques()` (only feasible for fairly small networks). The min parameter can be used to set a minimum size. Here, we want to ignore all cliques of size $2$.

```{r}
#| label: calc_max_cliques
# only return cliques with three or more nodes
cl <- max_cliques(clique_graph, min = 3)
cl
```

The figure below shows the network and the found maximal cliques.

```{r}
#| label: plot-clique-graph
#| echo: FALSE

xy <- graphlayouts::layout_with_stress(clique_graph)

cl_df <- as.data.frame(do.call(
    "rbind",
    lapply(seq_along(cl), function(x) {
        cbind(xy[cl[[x]], ], x)
    })
))

ggraph(clique_graph, "stress") +
    geom_edge_link0(edge_color = "grey66") +
    geom_node_point(shape = 21, size = 8, fill = "grey25") +
    ggforce::geom_mark_hull(data = cl_df, aes(V1, V2, fill = as.factor(x), group = x), show.legend = FALSE) +
    scale_fill_manual(values = c(
        "#E69F00", "#000000", "#56B4E9", "#009E73", "#F0E442", "#0072B2",
        "#D55E00", "#CC79A7", "#666666"
    )) +
    theme_void()

```

Related to cliques is the **k-core decomposition** of a network. A k-core is a subgraph in which every node has at least k neighbors within the subgraph. A k-core is thus a relaxed version of a clique.  
The function `coreness()` can be used to calculate the k-core membership for each node.
```{r}
#| label: kcore
kcore <- coreness(clique_graph)
kcore
```


```{r}
#| label: kcore_plot
#| echo: FALSE

cl_df <- as.data.frame(do.call(
    "rbind",
    lapply(sort(unique(kcore))[c(2, 3, 4)], function(x) {
        cbind(xy[kcore >= x, ], x)
    })
))

ggraph(clique_graph, "stress") +
    geom_edge_link0(edge_color = "grey66") +
    geom_node_point(shape = 21, size = 8, fill = "grey25") +
    ggforce::geom_mark_hull(data = cl_df, aes(V1, V2, fill = as.factor(x), group = x), show.legend = FALSE) +
    scale_fill_manual(values = c("red", "blue", "green")) +
    theme_void()

```

Cliques are the prototypical and most strict definition of a cohesive subgroup in a network. In empirical networks, however, we rarely encounter situations where we can partition the whole network into a set of 
cliques. The relaxed version of this problem is that of clustering, also referred to as **comunity detection**. 

## Comunity detection

A cluster is loosely defined as a group of nodes which are internally densely and externally sparsely connected. The network below shows an example for a network with a visible and intuitive cluster structure.

```{r}
# labeL; clustered-graph
#| echo: FALSE

n1 <- 5
n2 <- 20
set.seed(1234)
g <- sample_islands(n1, n2, 0.9, 5)
g <- simplify(g)
V(g)$grp <- rep(LETTERS[1:n1], each = n2)
ggraph(g, "stress") +
    geom_edge_link0(edge_linewidth = 0.2, edge_color = "grey66") +
    geom_node_point(shape = 21, size = 5, aes(fill = grp), show.legend = FALSE) +
    theme_void()
```

In contrast, the network below does not really seem to have any well defined cluster structure.

```{r}
#| label: random-graph
#| echo: FALSE
n1 <- 5
n2 <- 20

set.seed(1234)
g <- sample_islands(n1, n2, 0.25, 15)
g <- simplify(g)
V(g)$grp <- rep(LETTERS[1:n1], each = n2)
ggraph(g, "stress") +
    geom_edge_link0(edge_linewidth = 0.2, edge_color = "grey66") +
    geom_node_point(shape = 21, size = 5, fill = "grey66", show.legend = FALSE) +
    theme_void()

```

The following algorithms for graph clustering are implemented in `igraph`.

```{r}
#| label: cluster_alg
#| echo: FALSE
algs <- as.character(lsf.str("package:igraph"))
algs[stringr::str_detect(algs, "cluster_")]
```

Most of these algorithms are based on "modularity maximization". Modularity is defined as the fraction of edges that fall within given groups minus the expected fraction if edges were distributed at random.

The workflow of a cluster analysis is always the same, independent from the chosen method. We illustrate the workflow using the infamous karate club network.
```{r}
#| label: karate
data("karate")
```

```{r}
#| label: karate-plot
#| echo: false
ggraph(karate, "stress") +
    geom_edge_link0(edge_color = "grey66") +
    geom_node_point(shape = 21, size = 5, fill = "grey66") +
    theme_void() +
    coord_equal()
```

```{r}
#| label: cluster_ex
# compute clustering
clu <- cluster_louvain(karate)

# cluster membership vector
mem <- membership(clu)
mem

# clusters as list
com <- communities(clu)
com
```

To compare the quality of clusterings, we can compute the modularity score for each output.

```{r}
#| label: karate-cluster
imc <- cluster_infomap(karate)
lec <- cluster_leading_eigen(karate)
loc <- cluster_louvain(karate)
sgc <- cluster_spinglass(karate)
wtc <- cluster_walktrap(karate)
scores <- c(
    infomap = modularity(karate, membership(imc)),
    eigen = modularity(karate, membership(lec)),
    louvain = modularity(karate, membership(loc)),
    spinglass = modularity(karate, membership(sgc)),
    walk = modularity(karate, membership(wtc))
)
scores
```

For the karate network, `cluster_spinglass()` produces the highest modularity score.
The corresponding clustering is shown below.

```{r}
#| label: karate-plot-clu
#| echo: false
V(karate)$clu <- membership(sgc)
ggraph(karate, "stress") +
    geom_edge_link0(edge_color = "grey66") +
    geom_node_point(shape = 21, size = 5, aes(fill = as.factor(clu)), show.legend = FALSE) +
    theme_void() +
    coord_equal()
```

Modularity maximization is still widely considered as the state-of-the-art clustering method
for networks. There are, however, some technical shortcomings that one should be aware of.
One of those is the so called "resolution limit". When modularity is being maximized, it can happen
that smaller clusters are merged together to form bigger clusters. The prime example is the graph that
consists of cliques connected in a ring.

```{r}
#| label: prepare-Kn-graph
#| echo: false
n1 <- 5
n2 <- 50
A <- matrix(1, n1, n1)
lst <- vector("list", n2)
lst <- lapply(lst, function(x) A)
AA <- Matrix::bdiag(lst)
for (i in 1:(n2 - 1)) {
    AA[i * n1, i * n1 + 1] <- AA[i * n1 + 1, i * n1] <- 1
}
AA[1, n1 * n2] <- AA[n1 * n2, 1] <- 1
K50 <- graph_from_adjacency_matrix(AA, "undirected", diag = FALSE)
```

The figure below shows such a graph, consisting of 50 cliques of size 5. 

```{r}
#| label: plot-K50-blank
#| echo: false
#| width: 8
#| height: 8
ggraph(K50, "stress") +
    geom_edge_link0(edge_linewidth = 0.6, edge_color = "grey66") +
    geom_node_point(shape = 21, fill = "grey66", size = 2, show.legend = FALSE) +
    theme_void() +
    coord_fixed()
```

Intuitively, any clustering method should return a cluster for each clique.

```{r}
#| label: clu-louvain-K50
clu_louvain <- cluster_louvain(K50)
table(membership(clu_louvain))

```

A clustering algorithm that fixes this issue is the leiden algorithm.

```{r}
#| label: clu-leiden-K50
clu_leiden <- cluster_leiden(K50, objective_function = "CPM", resolution_parameter = 0.5)
table(membership(clu_leiden))

```

The figure below shows the clusters computed with the louvain method in grey and the leiden method in red.

```{r}
#| label: plot-K50-clu
#| echo: false
#| width: 8
#| height: 8
V(K50)$louvain <- membership(clu_louvain)
V(K50)$leiden <- membership(clu_leiden)
ggraph(K50, "stress") +
    geom_edge_link0(edge_linewidth = 0.6, edge_color = "grey66") +
    geom_node_point(shape = 21, fill = "grey66", size = 1.5, show.legend = FALSE) +
    ggforce::geom_mark_ellipse(aes(x, y, group = louvain), expand = unit(4.5, "pt"), fill = "black", alpha = 0.25) +
    ggforce::geom_mark_circle(aes(x, y, group = leiden), expand = unit(4.5, "pt"), col = "firebrick3", size = 1) +
    theme_void() +
    coord_fixed()

```

If you are interested in the technical details of the Leiden method, check out the [original paper](https://www.nature.com/articles/s41598-019-41695-z). 

The `netUtils` package includes the function `sample_lfr()` which implements the well-known [Lancichinetti–Fortunato–Radicchi benchmark algorithm](https://en.wikipedia.org/wiki/Lancichinetti%E2%80%93Fortunato%E2%80%93Radicchi_benchmark) to generate artificial networks with a priori known communities and they can be used to compare different community detection methods.

## Blockmodeling

Blockmodeling is a more formal approach that aims to simplify the network's structure into blocks based on patterns of connections between nodes. Instead of focusing on the density of connections, it categorizes the relationships between different groups (or blocks) of nodes according to the roles they play in the network.

The goal is to reduce the complexity of the network by identifying roles and positions within the network, where nodes in the same block have similar patterns of connections to other blocks, rather than necessarily being densely connected to each other.

Blockmodeling involves partitioning the network into blocks and then modeling the connections between these blocks. It can be done through conventional (deterministic) or stochastic approaches, including k-block modeling and stochastic blockmodeling.

Blockmodeling is particularly useful in sociology for role analysis and in organizational studies, where it's important to understand how different groups (e.g., departments, hierarchies) interact, regardless of the density of the connections within each group.

There are several packages that implement different kinds of (stochastic) blockmodels. The most basic approaches are implemented in the package `blockmodeling`.

```{r}
#| label: lib-blockmodel
#| message: false
library(blockmodeling)
```

In principle, blockmodels can also be used for clustering, as we will illustrate on this random network with 3 dense blocks of size 20.

```{r}
#| label: random-graph2
#| echo: FALSE
n1 <- 3
n2 <- 20

set.seed(1234)
g <- sample_islands(n1, n2, 0.75, 5)
g <- simplify(g)
V(g)$grp <- rep(LETTERS[1:n1], each = n2)
ggraph(g, "stress") +
    geom_edge_link0(edge_linewidth = 0.2, edge_color = "grey66") +
    geom_node_point(shape = 21, size = 5, fill = "grey66", show.legend = FALSE) +
    theme_void()

```

The disadvantage is that we need to specify a lot more parameters than for community detection. 
```{r}
#| label: calc-rnd-block
A <- as_adj(g)
blk <- matrix(
    c(
        "com", "nul", "nul",
        "nul", "com", "nul",
        "nul", "nul", "com"
    ),
    nrow = 3
)
blk
res <- optRandomParC(
    M = A, k = 3, approaches = "bin",
    blocks = blk, rep = 5, mingr = 20, maxgr = 20
)
```

- `k`: number of blocks needs to be specified beforehand
- `approaches`: defines the type of blockmodel approach to be used. "bin" is for binary and "val" for valued blockmodeling. There are several more possibilities available in the help of the function
- `blocks`: allowed block types. Basically, what defines a block in the network. In our example we give a strict patterning that corresponds to a clustering. The diagonal blocks should be complete ("com") and offdiagonals should be empty ("nul"). So in the best case, we have 3 disconnected cliques. Again, consult the help for more available block options.
- `rep`: number of random starting partitions to start the iteration from
- `mingr` and `maxgr`: min and max size of the blocks. 

the result can be accessed with `clu`.
```{r}
clu(res)
```

Note that this type of Blockmodeling is computationally expensive and best suited for small networks. 

Looking at a more realistic dataset, we load the `baker` dataset from the `blockmodeling` package. The dataset includes citation data between social work journals for 1985-86.

```{r}
#| label: load-baker
data("baker")
diag(baker) <- 0

plotMat(baker,
    main = "Baker Network Data",
    mar = c(1, 1, 3, 1), title.line = 2
)
```

First, we run a binary blockmodel. This time we increase the number of repetions to 1000 and instead of giving a lear block structure, we just specify, what type of blocks we want our result to include. How they are distributed, we do not care and let the algorithm decide. We run the optimization in parallel (`nCores = 0`), which requires the packages `doParallel` and `doRNG` to be installed.
```{r}
#| label: baker_binary
baker_binary <- baker
baker_binary[baker_binary > 0] <- 1

res_baker_binary <- optRandomParC(
    M = baker_binary, k = 3, rep = 1000,
    nCores = 0, blocks = c("nul", "com"), approach = "bin"
)
```

The obtained optimal block structure can be accessed via `IM`.

```{r}
#| label: baker_binary_IM
IM(res_baker_binary)
``` 

The resulting blocks can be visualized via the `plot` function.
```{r}
#| label: baker-bin
plot(
    res_baker_binary,
    main = "Baker Binary Network Data",
    mar = c(1, 2, 3, 1), title.line = 2
)
```

Now we run a valued blockmodel on the original data. The parameter `preSpecM` is set to the median of the non-zero entries and defines a kind of cutoff for when to consider a value high enough to be a block internal tie.
```{r}
#| label: baker_valued
res_baker_valued <- optRandomParC(
    M = baker, k = 3, rep = 1000,
    preSpecM = 13, approach = "val", blocks = c("nul", "com"),
    nCores = 0
)

```


```{r}
#| label: baker_valued_IM
IM(res_baker_valued)
``` 


```{r}
#| label: baker-val
plot(
    res_baker_valued,
    main = "Baker Valued Network Data",
    mar = c(1, 2, 3, 1), title.line = 2
)
```

### References

Žiberna, A. (2007). Generalized Blockmodeling of Valued Networks. Social Networks, 29(1), 105-126. doi: 10.1016/j.socnet.2006.04.002

Žiberna, A. (2008). Direct and indirect approaches to blockmodeling of valued networks in terms of regular equivalence. Journal of Mathematical Sociology, 32(1), 57-84. doi: 10.1080/00222500701790207

Žiberna, A. (2014). Blockmodeling of multilevel networks. Social Networks, 39(1), 46-61. doi: 10.1016/j.socnet.2014.04.002

## Core-Periphery

```{r}
#| label: load_netUtils
library(netUtils)
```


```{r}
#| label: split-graph
set.seed(1234)
sg <- split_graph(n = 50, p = 0.3, core = 0.3)
```

```{r}
#| label: split-graph-img
#| echo: false
ggraph(sg, "stress") +
    geom_edge_link0(edge_color = "grey66", edge_linewidth = 0.1) +
    geom_node_point(shape = 21, fill = "grey25") +
    theme_graph()
```

```{r}
#| label: split-cp
core_periphery(sg, method = "rk1_dc")
```


```{r}
#| label: baker-cp
g_baker <- graph_from_adjacency_matrix(
    baker_binary,
    mode = "max"
)
core_periphery(g_baker, method = "rk1_dc")
core_periphery(g_baker, method = "rk1_ec")
core_periphery(g_baker, method = "GA")
```

### References

Borgatti, Stephen P., and Martin G. Everett. "Models of core/periphery structures." Social networks 21.4 (2000): 375-395.