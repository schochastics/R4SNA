[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "R for Social Network Analysis",
    "section": "",
    "text": "Welcome\nThis book is work in progress. See the announcement post for more details.",
    "crumbs": [
      "Welcome"
    ]
  },
  {
    "objectID": "intro.html",
    "href": "intro.html",
    "title": "Introduction",
    "section": "",
    "text": "What you will learn",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "intro.html#how-this-book-is-organized",
    "href": "intro.html#how-this-book-is-organized",
    "title": "Introduction",
    "section": "How this book is organized",
    "text": "How this book is organized",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "intro.html#what-you-wont-learn",
    "href": "intro.html#what-you-wont-learn",
    "title": "Introduction",
    "section": "What you won’t learn",
    "text": "What you won’t learn",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "intro.html#prerequisites",
    "href": "intro.html#prerequisites",
    "title": "Introduction",
    "section": "Prerequisites",
    "text": "Prerequisites\n(This section is taken and slightly adapted from R for Data Science 2e)\nWe’ve made a few assumptions about what you already know to get the most out of this book. You should be generally numerically literate, and it’s helpful if you have some basic programming experience already. If you’ve never programmed before, you might find Hands on Programming with R by Garrett to be a valuable adjunct to this book.\nYou need four things to run the code in this book: R, an IDE, a collection of “base” R packages, and a handful of other packages. Packages are the fundamental units of reproducible R code. They include reusable functions, documentation that describes how to use them, and sample data.\n\nR\nTo download R, go to CRAN, the comprehensive R archive network, https://cloud.r-project.org. A new major version of R comes out once a year, and there are 2-3 minor releases each year. It’s a good idea to update regularly. Upgrading can be a bit of a hassle, especially for major versions that require you to re-install all your packages, but putting it off only makes it worse. We recommend R 4.3.0 or later for this book.\n\n\nIDE\nWe make no assumption about your integrated development environment. RStudio is a popular choice for R programming, which you can download from https://posit.co/download/rstudio-desktop/. RStudio is updated a couple of times a year, and it will automatically let you know when a new version is out, so there’s no need to check back. It’s a good idea to upgrade regularly to take advantage of the latest and greatest features. For this book, make sure you have at least RStudio 2022.02.0.\nBut nothing will stop you to use other environments, such as vscode, emacs, or vi if you are already used to them.\n\n\nThe “base” Packages\nWhen it comes to performing data science tasks with R, there are many excellent packages (or even package ecosystems) to choose from. There is for instance the tidyverse, data.table, or rpolars. All have there advantages and disadvantages but ultimately, they allow to do similar tasks with varying syntax.\nIn the R world for networks, there are far less choices and for the most basic network analytic tasks, there are essentially two packages: igraph and sna.\nWe will discuss these packages here and motivate why we would recommend igraph as the goto package for standard network analytic tasks. But that does not render sna and its companion network void. On the contrary, there are some essential modelling tasks which can only be done within the realm of network.\nBoth igraph and network also provide data structures which facilitate to store and process network data. There also once existed the package graph but that is not available on CRAN anymore, only via Bioconductor.\nThe figure below shows how many packages on CRAN rely on those three packages (i.e. they are mentioned in Depends, Imports, or Suggests).\n\n\n\n\n\n\n\n\n\nThe figure was produced with the help of the cranet package. igraph seems to be clearly favored by the R community. So if you install a package for, say, signed network analysis, chances are high that it depends on the graph structures provided by igraph. Besides the data structures, the package offers a large variety of network analytic methods which are all implemented in C. The methods are well optimized and also work nicely for large graphs.\nThe network package historically shares some commonalities with igraphs data structures. The package itself, though is really only providing the data structure and no analytic methods. The sna package (link) implements network analytic tools using the data structures of network. Overall, the syntax and provided methods are very much comparable between igraph and sna and they are almost interchangeable in this regard. The advantage of igraph is its speed. I have run several benchmark tasks and igraph usually comes out on top. That being said, there is no real case to be made against network/sna. If you are into statistical modelling of networks, then that should actually be the preferred choice since the ergm package is build on top of network. In this case you probably also want to look at the meta package statnet (link) which includes network, sna, and ergm (among other packages).\nNote that the package intergraph (link) can be used to quickly switch representations between igraph and network.\nThe igraph package will play a major role in the Part on Descripitve Network Analysis, while network will be more prominent for Inferential Network Analysis.\n\n\nOther packages\n[LIST USED PACKAGES WITH DESCRIPTION]",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "network-data.html",
    "href": "network-data.html",
    "title": "Network Data",
    "section": "",
    "text": "Introduction\nThis introductory chapter will give a short explanation of key terminology and how network data can be represented. The larger part of the chapter is concerned with representing networks in R using igraph and how to construct or read network data.\nAfter reading this chapter, you should have a basic understanding of what networks and network data is and how to create network objects in R using igraph.",
    "crumbs": [
      "Network Data"
    ]
  },
  {
    "objectID": "network-data.html#what-is-a-network",
    "href": "network-data.html#what-is-a-network",
    "title": "Network Data",
    "section": "What is a network?",
    "text": "What is a network?\nIn the context of social network analysis, a network is a conceptual and analytical construct to understand, visualize, and examine the (social) relationships and structures that emerge from the interactions among individuals, groups, organizations, or even entire societies. At its core, a network consists of nodes (which represent the actors, whether individuals or organizations) and edges (which signify the relationships or connections between these actors). These relationships can embody various types of interactions such as communication, friendship, professional ties, or social influence, among others.\nNetworks in the social sciences are tools for mapping and quantifying the patterns of social connections, helping to reveal the underlying dynamics of social cohesion, influence, and information flow within a community or society. Through the lens of network theory, analysts can explore how social structures influence behaviors, opportunities, and outcomes for individuals and groups, making it an invaluable approach in sociology, anthropology, political science, and many other disciplines that study social systems and interactions.",
    "crumbs": [
      "Network Data"
    ]
  },
  {
    "objectID": "network-data.html#network-representations",
    "href": "network-data.html#network-representations",
    "title": "Network Data",
    "section": "Network representations",
    "text": "Network representations\nThere are several possible ways to express network data. All come with a set of advantages and disadvantages.\n\nAdjacency Matrix\nAn adjacency matrix is a square matrix where the elements indicate whether pairs of vertices in the graph are adjacent or not—meaning, whether they are directly connected by an edge. If the graph has \\(n\\) vertices, the matrix \\(A\\) will be an \\(n \\times n\\) matrix where the entry \\(A_{ij}\\) is \\(1\\) if there is an edge from vertex \\(i\\) to vertex \\(j\\), and \\(0\\) if there is no edge. In the case of weighted graphs the weight of the edge is used. This matrix is symmetric for undirected graphs, indicating that an edge is bidirectional.\nPros:\n\nSimple Representation: It provides a straightforward and compact way to represent graphs, especially useful for dense graphs where many or most pairs of vertices are connected.\nEfficient for Edge Lookups: Checking whether an edge exists between two vertices can be done in constant time, making it efficient for operations that require frequent edge lookups.\nEasy Implementation of Algorithms: Many graph algorithms can be easily implemented using adjacency matrices, making it a preferred choice for certain computational tasks.\n\nCons:\n\nSpace Inefficiency: For sparse graphs, where the number of edges is much less than the square of the number of vertices, an adjacency matrix uses a lot of memory to represent a relatively small number of edges.\nPoor Scalability: As the number of vertices grows, the size of the matrix grows quadratically, which can quickly become impractical for large graphs.\n\n\n\nEdge List\nAn edge list is a matrix where each row indicates an edge. In an undirected graph, an edge is represented by a pair \\((i,j)\\), indicating a connection between vertices \\(i\\) and \\(j\\). For directed graphs, the order of the vertices in each pair denotes the direction of the edge, from the first vertex to the second. In weighted graphs, a third column can be added to each pair to represent the weight of the edge.\nPros:\n\nSpace Efficiency for Sparse Graphs: Edge lists are particularly space-efficient for representing sparse graphs where the number of edges is much lower than the square of the number of vertices, as they only store the existing edges.\nSimplicity: The structure is straightforward and easy to understand, making it suitable for simple graph operations and for initial graph representation before processing.\n\nCons:\n\nInefficient for Edge Lookups: Checking whether an edge exists between two specific vertices can be time-consuming, as it may require scanning through the entire list, leading to an operation that is linear in the number of edges.\nInefficiency in Graph Operations: Operations like finding all vertices adjacent to a given vertex or checking for connectivity between vertices can be inefficient compared to other representations like adjacency matrices or adjacency lists, especially for dense graphs.\nLess Suitable for Dense Graphs: As the number of edges grows, the edge list can become large and less efficient in terms of both space and operation time compared to an adjacency matrix for dense graphs, where the number of edges is close to the maximum possible number of edges.\n\n\n\nAdjacency List\nAn adjacency list is a collection of lists, with each list corresponding to the set of adjacent vertices of a given vertex. This means that for every vertex \\(i\\) in the graph, there is an associated list that contains all the vertices \\(j\\) to which \\(i\\) is directly connected.\nPros:\n\nSpace Efficiency: Adjacency lists are more space-efficient than adjacency matrices in sparse graphs, as they only store information about the actual connections.\nScalability: This representation scales better with the number of edges, especially for graphs where the number of edges is far less than the square of the number of vertices.\nEfficiency in Graph Traversal: For operations like graph traversal or finding all neighbors of a vertex, adjacency lists provide more efficient operations compared to adjacency matrices, particularly in sparse graphs.\n\nCons:\n\nEdge Lookups: Checking whether an edge exists between two specific vertices can be less efficient than with an adjacency matrix, as it may require traversing a list of neighbors.\nVariable Edge Access Time: The time to access a specific edge or to check for its existence can vary depending on the degree of the vertices involved, leading to potentially inefficient operations in certain scenarios.\nHigher Complexity for Dense Graphs: In very dense graphs, where the number of edges approaches the number of vertex pairs, adjacency lists can become less efficient in terms of space and time compared to adjacency matrices, due to the overhead of storing a list for each vertex.",
    "crumbs": [
      "Network Data"
    ]
  },
  {
    "objectID": "network-data.html#importing-network-data",
    "href": "network-data.html#importing-network-data",
    "title": "Network Data",
    "section": "Importing Network Data",
    "text": "Importing Network Data\n\nForeign Formats\nigraph can deal with many different foreign network formats with the function read_graph. (The rgexf package can be used to import Gephi files.)\n\nread_graph(\n  file,\n  format = c(\n    \"edgelist\",\n    \"pajek\",\n    \"ncol\",\n    \"lgl\",\n    \"graphml\",\n    \"dimacs\",\n    \"graphdb\",\n    \"gml\",\n    \"dl\"\n  ),\n  ...\n)\n\nIf your network data is in one of the above formats you will find it easy to import your network.\n\n\nNodes, Edges, and Attributes\nIf your data is not in a network file format, you will need one of the following functions to turn raw network data into an igraph object: graph_from_edgelist(), graph_from_adjacency_matrix(), graph_from_adj_list(), or graph_from_data_frame().\nBefore using these functions, however, you still need to get the raw data into R. The concrete procedure depends on the file format. If your data is stored as an excel spreadsheet, you need additional packages. If you are familiar with the tidyverse, you can use the readxl package. Other options are, e.g. the xlsx package.\nMost network data you’ll find is in a plain text format (csv or tsv), either as an edgelist or adjacency matrix. To read in such data, you can use base R’s read.table().\nMake sure you check the following before trying to load a file: Does it contain a header (e.g. row/column names of an adjacency matrix)? How are values delimited (comma, whitespace or tab)? This is important to set the parameters header, sep to read the data properly.",
    "crumbs": [
      "Network Data"
    ]
  },
  {
    "objectID": "network-data.html#networks-in-igraph",
    "href": "network-data.html#networks-in-igraph",
    "title": "Network Data",
    "section": "Networks in igraph",
    "text": "Networks in igraph\nBelow, we represent friendship relations between Bob, Ann, and Steve as a matrix and an edgelist.\n\n# adjacency matrix\nA &lt;- matrix(\n  c(0, 1, 1, 1, 0, 1, 1, 1, 0),\n  nrow = 3,\n  ncol = 3,\n  byrow = TRUE\n)\n\nrownames(A) &lt;- colnames(A) &lt;- c(\"Bob\", \"Ann\", \"Steve\")\nA\n\n      Bob Ann Steve\nBob     0   1     1\nAnn     1   0     1\nSteve   1   1     0\n\n# edgelist\nel &lt;- matrix(\n  c(\"Bob\", \"Ann\", \"Bob\", \"Steve\", \"Ann\", \"Steve\"),\n  nrow = 3,\n  ncol = 2,\n  byrow = TRUE\n)\nel\n\n     [,1]  [,2]   \n[1,] \"Bob\" \"Ann\"  \n[2,] \"Bob\" \"Steve\"\n[3,] \"Ann\" \"Steve\"\n\n\nOnce we have defined an edgelist or an adjacency matrix, we can turn them into igraph objects as follows.\n\ng1 &lt;- graph_from_adjacency_matrix(A, mode = \"undirected\", diag = FALSE)\n\ng2 &lt;- graph_from_edgelist(el, directed = FALSE)\n# g1 and g2 are the same graph so only printing g1\ng1\n\nIGRAPH de86b43 UN-- 3 3 -- \n+ attr: name (v/c)\n+ edges from de86b43 (vertex names):\n[1] Bob--Ann   Bob--Steve Ann--Steve\n\n\nThe printed summary shows some general descriptives of the graph. The string “UN–” in the first line indicates that the network is Undirected (D for directed graphs) and has a Name attribute (we named the nodes Bob, Ann, and Steve). The third and forth character are W, if there is a edge weight attribute, and B if the network is bipartite (there exists a node attribute “type”). The following number indicate the number of nodes and edges. The second line lists all graph, node and edge variables. Here, we only have a node attribute “name”.\nThe conversion from edgelist/adjacency matrix into an igraph object is quite straightforward. The only difficulty is setting the parameters correctly (Is the network directed or not?), especially for edgelists where it may not immediately be obvious if the network is directed or not.\n\nImport via snahelper\nThe R package snahelper implements several Addins for RStudio that facilitate working with network data by providing a GUI for various tasks. One of these is the Netreader which allows to import network data.\nThe first two tabs allow you to import raw data (edges and attributes). Make sure to specify file delimiters, etc. according to the shown preview.\n\nUsing the Netreader should comes with a learning effect (hopefully). The last tab shows the R code to produce the network with the chosen data without using the Addin.\n\nThe network will be saved in your global environment once you click “Done”.",
    "crumbs": [
      "Network Data"
    ]
  },
  {
    "objectID": "network-data.html#scientific-reading",
    "href": "network-data.html#scientific-reading",
    "title": "Network Data",
    "section": "Scientific reading",
    "text": "Scientific reading\nWasserman, S., & Faust, K. (1994). Social network analysis: Methods and applications.\nScott, J. (2012). What is Social Network Analysis? Bloomsbury Academic.",
    "crumbs": [
      "Network Data"
    ]
  },
  {
    "objectID": "descriptive/descriptives-basic.html",
    "href": "descriptive/descriptives-basic.html",
    "title": "2  Basic Network Statistics",
    "section": "",
    "text": "2.1 Packages Needed for this Chapter\nlibrary(igraph)\nlibrary(networkdata)",
    "crumbs": [
      "Descriptive Network Analysis",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Basic Network Statistics</span>"
    ]
  },
  {
    "objectID": "descriptive/descriptives-basic.html#simple-network-characteristics",
    "href": "descriptive/descriptives-basic.html#simple-network-characteristics",
    "title": "2  Basic Network Statistics",
    "section": "2.2 Simple Network Characteristics",
    "text": "2.2 Simple Network Characteristics\nIn the following, we use a network from the networkdata package to introduce some basic network statistics.\n\ndata(\"greys\")\n\n\n\n\n\n\n\n\n\n\nThe “greys” network consists of characters from the show “Grey’s Anatomy” and links indicate who hooked up with whom (up to about 2022).\n\ngreys\n\nIGRAPH f7716f1 UN-- 54 57 -- \n+ attr: name (v/c), sex (v/c), race (v/c), birthyear (v/n), position\n| (v/c), season (v/n), sign (v/c)\n+ edges from f7716f1 (vertex names):\n [1] Arizona Robbins--Leah Murphy     Alex Karev     --Leah Murphy    \n [3] Arizona Robbins--Lauren Boswell  Arizona Robbins--Callie Torres  \n [5] Erica Hahn     --Callie Torres   Alex Karev     --Callie Torres  \n [7] Mark Sloan     --Callie Torres   George O'Malley--Callie Torres  \n [9] Izzie Stevens  --George O'Malley Meredith Grey  --George O'Malley\n[11] Denny Duqutte  --Izzie Stevens   Izzie Stevens  --Alex Karev     \n[13] Derek Sheperd  --Meredith Grey   Preston Burke  --Cristina Yang  \n+ ... omitted several edges\n\n\nThe density of a network is defined as the fraction of the potential edges in a network that are actually present.\n\nc(\n    edge_density(make_empty_graph(10)),\n    edge_density(greys),\n    edge_density(make_full_graph(10))\n)\n\n[1] 0.00000000 0.03983229 1.00000000\n\n\nThe density of an empty network is \\(0\\) and for the full network it is \\(1\\). The density of empirical network is somewhere in between but as the number of nodes increases, we’d expect the density to decrease and the network becomes quite sparse.\nA shortest path is a path that connects two nodes in a network with a minimal number of edges. The length of a shortest path is called the distance between two nodes.\n\nshortest_paths(greys,from = \"Alex Karev\",to = \"Owen Hunt\",output = \"vpath\")\n\n$vpath\n$vpath[[1]]\n+ 5/54 vertices, named, from f7716f1:\n[1] Alex Karev         Addison Montgomery Mark Sloan         Teddy Altman      \n[5] Owen Hunt         \n\n\n$epath\nNULL\n\n$predecessors\nNULL\n\n$inbound_edges\nNULL\n\n\n\n\n\n\n\n\n\n\n\n\ndistances(greys)[1:10, 1:10]\n\n                   Addison Montgomery Adele Webber Teddy Altman Amelia Shepherd\nAddison Montgomery                  0          Inf            2               2\nAdele Webber                      Inf            0          Inf             Inf\nTeddy Altman                        2          Inf            0               2\nAmelia Shepherd                     2          Inf            2               0\nArizona Robbins                     3          Inf            3               3\nRebecca Pope                        2          Inf            4               4\nJackson Avery                       3          Inf            3               3\nMiranda Bailey                    Inf          Inf          Inf             Inf\nBen Warren                        Inf          Inf          Inf             Inf\nHenry Burton                        3          Inf            1               3\n                   Arizona Robbins Rebecca Pope Jackson Avery Miranda Bailey\nAddison Montgomery               3            2             3            Inf\nAdele Webber                   Inf          Inf           Inf            Inf\nTeddy Altman                     3            4             3            Inf\nAmelia Shepherd                  3            4             3            Inf\nArizona Robbins                  0            3             4            Inf\nRebecca Pope                     3            0             3            Inf\nJackson Avery                    4            3             0            Inf\nMiranda Bailey                 Inf          Inf           Inf              0\nBen Warren                     Inf          Inf           Inf              1\nHenry Burton                     4            5             4            Inf\n                   Ben Warren Henry Burton\nAddison Montgomery        Inf            3\nAdele Webber              Inf          Inf\nTeddy Altman              Inf            1\nAmelia Shepherd           Inf            3\nArizona Robbins           Inf            4\nRebecca Pope              Inf            5\nJackson Avery             Inf            4\nMiranda Bailey              1          Inf\nBen Warren                  0          Inf\nHenry Burton              Inf            0\n\n\nThe Grey’s Anatomy network is disconnected and consists of \\(4\\) connected components. There are no shortest paths between components, which means that the distance is not measurable and set to infinity.\nThe length of the longest shortest path is called the diameter of the network.\n\ndiameter(greys)\n\n[1] 8\n\n\n\n\n\n\n\n\n\n\n\nTransitivity measures the probability that the neighbors of a node are also connected. This is also called the clustering coefficient.\n\ntransitivity(greys, type = \"global\")\n\n[1] 0\n\ntransitivity(greys, type = \"local\", isolates = \"zero\")\n\nAddison Montgomery       Adele Webber       Teddy Altman    Amelia Shepherd \n                 0                  0                  0                  0 \n   Arizona Robbins       Rebecca Pope      Jackson Avery     Miranda Bailey \n                 0                  0                  0                  0 \n        Ben Warren       Henry Burton    Catherine Avery       Colin Marlow \n                 0                  0                  0                  0 \n     Denny Duqutte      Derek Sheperd         Ellis Grey     Finn Dandridge \n                 0                  0                  0                  0 \n     Meredith Grey         Erica Hahn               Hank      Izzie Stevens \n                 0                  0                  0                  0 \n        Alex Karev       April Kepner         Lexie Grey              Lloyd \n                 0                  0                  0                  0 \n       Lucy Fields      Megan Nowland       Steve Mostow       Dana Seabury \n                 0                  0                  0                  0 \n     Nancy Shepard       Nurse Olivia    George O'Malley          Owen Hunt \n                 0                  0                  0                  0 \n    Andrew Perkins      Pierce Halley      Preston Burke       Reed Adamson \n                 0                  0                  0                  0 \n        Mark Sloan       Steve Murphy         Susan Grey      Thatcher Grey \n                 0                  0                  0                  0 \n     Callie Torres       Tucker Jones      Cristina Yang     Heather Brooks \n                 0                  0                  0                  0 \n         Jo Wilson     Lauren Boswell        Leah Murphy          Eli James \n                 0                  0                  0                  0 \n              Emma  Stephanie Edwards         Shane Ross     Richard Webber \n                 0                  0                  0                  0 \n              Rose       Nathan Riggs \n                 0                  0 \n\n\nThe global transitivity of an undirected network is the ratio of the triangles and the connected triples in the network. Local transitivity of a node is the ratio of the triangles connected to the node and the triples centered on the node itself. In social networks, we generally assume that the transitivity is quite high (“the friend of my friend is also my friend”). In our example, we have zero for all values. This is due to the fact that a triangle would require a same sex hook-up which did not occur (Disclaimer: I never watched the show and gathered the hook ups from various internet resources. So this may well be wrong.).\nFor directed networks, a measure of importance is reciprocity, which is defined as the proportion of mutual edges between nodes. To illustrate the measure, we use a network of grooming relations among a group of rhesus monkeys.\n\ndata(\"rhesus\")\nreciprocity(rhesus)\n\n[1] 0.7567568\n\n\nAbout 76% of edges are reciprocated in the network. The figure below highlights the reciprocated edges.\n\n\nWarning: `is.mutual()` was deprecated in igraph 2.0.0.\nℹ Please use `which_mutual()` instead.",
    "crumbs": [
      "Descriptive Network Analysis",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Basic Network Statistics</span>"
    ]
  },
  {
    "objectID": "descriptive/descriptives-basic.html#dyad-and-triad-census",
    "href": "descriptive/descriptives-basic.html#dyad-and-triad-census",
    "title": "2  Basic Network Statistics",
    "section": "2.3 Dyad and Triad Census",
    "text": "2.3 Dyad and Triad Census\nThe dyad census categorize all possible dyads within a network based on their mutual connection status. It classifies dyads into three categories: mutual (both nodes have a directed edge to the other, i.e. reciprocated), asymmetric (only one node has a directed edge to the other), and null (no directed edges between the nodes). The census provides insights into the overall reciprocity and directionality of relationships in the network, helping to understand the balance between mutual cooperation, one-sided relationships, and absence of direct interaction.\n\ndyad_census(rhesus)\n\n$mut\n[1] 42\n\n$asym\n[1] 27\n\n$null\n[1] 51\n\n\nMore important than the dyad census is usually the triad census. In a directed network, there are 16 possible configurations of edges that can occur between three nodes. \nThe triad census of a network gives the number of occurrences of each of these triad. Triads are labelled xyzL where x is the number of reciprocated ties, y is the number of unreciprocated ties and z is the number of null ties. The L term is a letter (U,C,D or T) which allows to differentiate between triads where these numbers are the same.\n\ntriad_census(rhesus)\n\n [1]  49  72 115  16  12  11  50  50   2   0  54  13  12   7  58  39",
    "crumbs": [
      "Descriptive Network Analysis",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Basic Network Statistics</span>"
    ]
  },
  {
    "objectID": "descriptive/descriptives-basic.html#use-case-triad-census",
    "href": "descriptive/descriptives-basic.html#use-case-triad-census",
    "title": "2  Basic Network Statistics",
    "section": "2.4 Use case: Triad Census",
    "text": "2.4 Use case: Triad Census\nOne of the many applications of the triad census is to compare a set of networks. In this example, we are tackling the question of “how transitive is football?” and assess structural differences among a set of football leagues.\n\ndata(\"football_triad\")\n\nfootball_triad is a list which contains networks of 112 football leagues as igraph objects. A directed link between team A and B indicates that A won a match against B. Note that there can also be an edge from B to A, since most leagues play a double round robin. For the sake of simplicity, all draws were deleted so that there could also be null ties between two teams if both games ended in a draw.\nBelow, we calculate the triad census for all network at once using lapply(). The function returns the triad census for each network as a list, which we turn into a matrix in the second step. Afterwards, we manually add the row and column names of the matrix.\n\nfooty_census &lt;- lapply(football_triad, triad_census)\nfooty_census &lt;- matrix(unlist(footy_census), ncol = 16, byrow = T)\nrownames(footy_census) &lt;- sapply(football_triad, function(x) x$name)\ncolnames(footy_census) &lt;- c(\n    \"003\", \"012\", \"102\", \"021D\", \"021U\", \"021C\", \"111D\", \"111U\",\n    \"030T\", \"030C\", \"201\", \"120D\", \"120U\", \"120C\", \"210\", \"300\"\n)\n\n# normalize to make proportions comparable across leagues\nfooty_census_norm &lt;- footy_census / rowSums(footy_census)\n\n# check the Top 5 leagues\nidx &lt;- which(rownames(footy_census) %in% c(\n    \"england\", \"spain\", \"germany\",\n    \"italy\", \"france\"\n))\nfooty_census[idx, ]\n\n        003 012 102 021D 021U 021C 111D 111U 030T 030C 201 120D 120U 120C 210\nengland   2  10   0   58   31   40   34   44  338   29  19  118  129  143 131\nfrance    1  23   5   30   33   44   48   40  332   41  16  132  108  160 114\ngermany   0  21   6   27   19   49   38   46  165   16  23   77   79  117 120\nitaly     1   4   2   35   43   30   30   22  419   38   5  164  116  118  99\nspain     0   8   4   27   42   45   32   35  364   43  11  126  105  148 130\n        300\nengland  14\nfrance   13\ngermany  13\nitaly    14\nspain    20\n\n\nNotice how the transitive triad (030T) has the largest count in the top leagues, hinting toward the childhood wisdom: “If A wins against B and B wins against C, then A must win against C”.\nIn empirical studies, we are not necessarily only interested in transitive triads, but rather how the triad census profiles compare across networks. We follow Kathrine Faust’s suggestion and do a singular value decomposition (SVD) on the normalized triad census matrix.\n\nfooty_svd &lt;- svd(footy_census_norm)\n\nSVDs are used to reduce the dimensionality of the data, but retaining most of the information. In our case, the data is 16 dimensional, which is impossible to visualize to compare the networks. With an SVD, we can reduce it to two dimensions and get a better visual overview.\n\n\nWarning: ggrepel: 35 unlabeled data points (too many overlaps). Consider\nincreasing max.overlaps\n\n\n\n\n\n\n\n\n\nHow to interpret the dimensions? To investigate this question, we take a closer look at the first two dimensions and compare it to some network descriptives. For the sake of brevity, we here only look at the density and proportion of 030T triads. In general, any node/dyad/triad level statistic could be used.\n\n\n\n\n\n\n\n\n\nDensity doesn’t really seem to be related to the first dimension in this case (in many cases it is!). Might be worthwhile to explore this further\n\n\n\n\n\n\n\n\n\nFor the second dimension, we get a clearer association. It seems that the fraction of transitive triads is a good indicator for structural differences among leagues.",
    "crumbs": [
      "Descriptive Network Analysis",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Basic Network Statistics</span>"
    ]
  },
  {
    "objectID": "descriptive/descriptives-basic.html#dyadtriad-census-with-attributes",
    "href": "descriptive/descriptives-basic.html#dyadtriad-census-with-attributes",
    "title": "2  Basic Network Statistics",
    "section": "2.5 Dyad/Triad Census with Attributes",
    "text": "2.5 Dyad/Triad Census with Attributes\nThe R package netUtils implements a version of the dyad and triad census which can account for node attributes.\n\nlibrary(netUtils)\n\nThe node attribute should be integers from 1 to max(attr). The output of dyad_census_attr() is a data.frame where each row corresponds to a pair of attribute values and the count of asymmetric, symmetric and null dyads.\nThe output of triad_census_attr() is a named vector where the names are of the form Txxx-abc, where xxx corresponds to the standard triad census notation and “abc” are the attributes of the involved nodes.\n\nset.seed(112)\ng &lt;- sample_gnp(20, p = 0.3, directed = TRUE)\n# add a vertex attribute\nV(g)$type &lt;- rep(1:2, each = 10)\n\ndyad_census_attr(g, \"type\")\n\n  from_attr to_attr asym_ab asym_ba sym null\n1         1       1       0       0   4   41\n2         1       2      20      28  14   38\n3         2       2       0       0   8   37\n\ntriad_census_attr(g, \"type\")\n\n T003-111  T003-112  T003-122  T003-222  T012-111  T012-121  T012-112  T012-122 \n        8        33        28         7        32        40        31        19 \n T012-211  T012-221  T012-212  T012-222 T021D-111 T021D-211 T021D-112 T021D-212 \n       27        41        25        26         9        19        19        21 \nT021D-122 T021D-222  T102-111  T102-112  T102-122  T102-211  T102-212  T102-222 \n        7        10        11        18        16         5        19        10 \nT021C-111 T021C-211 T021C-121 T021C-221 T021C-112 T021C-212 T021C-122 T021C-222 \n       17        23        29        17        19         7        24        10 \nT111U-111 T111U-121 T111U-112 T111U-122 T111U-211 T111U-221 T111U-212 T111U-222 \n        9        16         7        21         5        13        10         6 \nT021U-111 T021U-112 T021U-122 T021U-211 T021U-212 T021U-222 T030T-111 T030T-121 \n       11        19        13         3        14         7        11        11 \nT030T-112 T030T-122 T030T-211 T030T-221 T030T-212 T030T-222 T120U-111 T120U-112 \n       11        13        10        14         8         5         1         8 \nT120U-122 T120U-211 T120U-212 T120U-222 T111D-111 T111D-121 T111D-112 T111D-122 \n        6         0         4         4         4        12         8        13 \nT111D-211 T111D-221 T111D-212 T111D-222  T201-111  T201-112  T201-121  T201-122 \n       14        20        10        15         0         5         3         5 \n T201-221  T201-222 T030C-111 T030C-112 T030C-122 T030C-222 T120C-111 T120C-121 \n        3         3         2        12        14         3         3         8 \nT120C-211 T120C-221 T120C-112 T120C-122 T120C-212 T120C-222 T120D-111 T120D-112 \n        7         5         5         7         7         6         0         9 \nT120D-211 T120D-212 T120D-122 T120D-222  T210-111  T210-121  T210-211  T210-221 \n        1         9         4         1         2         8         3         5 \n T210-112  T210-122  T210-212  T210-222  T300-111  T300-112  T300-122  T300-222 \n        1         3         5         5         0         1         0         2",
    "crumbs": [
      "Descriptive Network Analysis",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Basic Network Statistics</span>"
    ]
  },
  {
    "objectID": "descriptive/descriptives-basic.html#degree-distributions",
    "href": "descriptive/descriptives-basic.html#degree-distributions",
    "title": "2  Basic Network Statistics",
    "section": "2.6 Degree distributions",
    "text": "2.6 Degree distributions",
    "crumbs": [
      "Descriptive Network Analysis",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Basic Network Statistics</span>"
    ]
  },
  {
    "objectID": "descriptive/descriptives-basic.html#scientific-reading",
    "href": "descriptive/descriptives-basic.html#scientific-reading",
    "title": "2  Basic Network Statistics",
    "section": "2.7 Scientific Reading",
    "text": "2.7 Scientific Reading",
    "crumbs": [
      "Descriptive Network Analysis",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Basic Network Statistics</span>"
    ]
  },
  {
    "objectID": "descriptive/centrality-basic.html",
    "href": "descriptive/centrality-basic.html",
    "title": "3  Centrality",
    "section": "",
    "text": "3.1 Packages Needed for this Chapter\nIn this chapter, we learn about network centrality, a key concept for identifying the most influential nodes within networks. In a nutshell, a measure of centrality is an index that assigns a numeric values to the nodes of the network. The higher the value, the more central the node. “Being central” is a very ambiguous term and it is thus no surprise that there exists a large variety of indices that assess centrality with very different structural properties of the network.\n[INSERT PERIODIC TABLE]\nlibrary(igraph)\nlibrary(netrankr)\nlibrary(networkdata)",
    "crumbs": [
      "Descriptive Network Analysis",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Centrality</span>"
    ]
  },
  {
    "objectID": "descriptive/centrality-basic.html#centrality-indices-igraph",
    "href": "descriptive/centrality-basic.html#centrality-indices-igraph",
    "title": "3  Centrality",
    "section": "3.2 Centrality Indices igraph",
    "text": "3.2 Centrality Indices igraph\nThe package igraph implements the following 10 indices:\n\ndegree (degree())\nweighted degree (strength())\nbetweenness (betweenness())\ncloseness (closeness())\neigenvector (eigen_centrality())\nalpha centrality (alpha_centrality())\npower centrality (power_centrality())\nPageRank (page_rank())\neccentricity (eccentricity())\nhubs and authorities (authority_score() and hub_score())\nsubgraph centrality (subgraph_centrality())\n\nTo illustrate some of the indices, we use the “dbces11” graph which is part of the netrankr package.\n\ndata(\"dbces11\")\n\n\n\n\n\n\n\n\n\n\ndegree simply counts the number of neighbors a node has.\n\ndegree(dbces11)\n\nA B C D E F G H I J K \n1 1 2 2 3 4 4 4 4 4 5 \n\n\n\n\n\n\n\n\n\n\n\ncloseness computes the shortest path distances among nodes. The most central node has the minimum distance to all other nodes (Since high scores are associated with central nodes, the distances are inverted).\n\ncloseness(dbces11)\n\n         A          B          C          D          E          F          G \n0.03703704 0.02941176 0.04000000 0.04000000 0.05000000 0.05882353 0.05263158 \n         H          I          J          K \n0.05555556 0.05555556 0.05263158 0.05555556 \n\n\nThe animation below gives an intuition on the calculation for one node. \nbetweeness is the number of shortest paths that pass through a node (divided by the total number of shortest paths)\n\nbetweenness(dbces11)\n\n        A         B         C         D         E         F         G         H \n 0.000000  0.000000  0.000000  9.000000  3.833333  9.833333  2.666667 16.333333 \n        I         J         K \n 7.333333  1.333333 14.666667 \n\n\nTo get an intuition what it means to have a high betweenness, check the network below.\n\n\nWarning: `add.vertices()` was deprecated in igraph 2.0.0.\nℹ Please use `add_vertices()` instead.\n\n\nWarning: `add.edges()` was deprecated in igraph 2.0.0.\nℹ Please use `add_edges()` instead.\n\n\n\n\n\n\n\n\n\nAny shortest path from the right will pass through the red node and vice versa. The red note is thus a sort of “gatekeeper” for any information that is passed from left to right.\neigenvector centrality extends the idea of degree by assuming that a node is central if it is connected to other central nodes.\n\neigen_centrality(dbces11)$vector\n\n        A         B         C         D         E         F         G         H \n0.2259630 0.0645825 0.3786244 0.2415182 0.5709057 0.9846544 1.0000000 0.8386195 \n        I         J         K \n0.9113529 0.9986474 0.8450304 \n\n\nsubgraph centrality is a bit more abstract but what it does is summing up all closed walks weighting them by the inverse factorial of its length.\n\nsubgraph_centrality(dbces11)\n\n       A        B        C        D        E        F        G        H \n1.825100 1.595400 3.148571 2.423091 4.387127 7.807257 7.939410 6.672783 \n       I        J        K \n7.032672 8.242124 7.389559 \n\n\nThe remaining indices are mostly designed for directed networks, page rank being the prime example. Note, though that the indices above can also be applied to directed networks.\nIf we highlight the most central node for the calculated indices, we get the following.\n\n\n\n\n\n\n\n\n\nSo each index picks a different node as most central. While this is just a toy example, it highlights how influential the choice of indices can be in empirical settings.",
    "crumbs": [
      "Descriptive Network Analysis",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Centrality</span>"
    ]
  },
  {
    "objectID": "descriptive/centrality-basic.html#centrality-indices-sna",
    "href": "descriptive/centrality-basic.html#centrality-indices-sna",
    "title": "3  Centrality",
    "section": "3.3 Centrality indices sna",
    "text": "3.3 Centrality indices sna\nThe sna package implements roughly the same indices as igraph but adds:\n\nflow betweenness (flowbet())\nload centrality (loadcent())\nGil-Schmidt Power Index (gilschmidt())\ninformation centrality (infocent())\nstress centrality (stresscent())",
    "crumbs": [
      "Descriptive Network Analysis",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Centrality</span>"
    ]
  },
  {
    "objectID": "descriptive/centrality-basic.html#other-centrality-packages",
    "href": "descriptive/centrality-basic.html#other-centrality-packages",
    "title": "3  Centrality",
    "section": "3.4 Other Centrality Packages",
    "text": "3.4 Other Centrality Packages\nThere are also some dedicated centrality packages, such as centiserve, CINNA, influenceR and keyplayer. The biggest in terms of implemented indices is currently centiserve with a total of 33 indices.\n\nlibrary(centiserve)\n\nLoading required package: Matrix\n\nas.character(lsf.str(\"package:centiserve\"))\n\n [1] \"averagedis\"            \"barycenter\"            \"bottleneck\"           \n [4] \"centroid\"              \"closeness.currentflow\" \"closeness.freeman\"    \n [7] \"closeness.latora\"      \"closeness.residual\"    \"closeness.vitality\"   \n[10] \"clusterrank\"           \"communibet\"            \"communitycent\"        \n[13] \"crossclique\"           \"decay\"                 \"diffusion.degree\"     \n[16] \"dmnc\"                  \"entropy\"               \"epc\"                  \n[19] \"geokpath\"              \"hubbell\"               \"katzcent\"             \n[22] \"laplacian\"             \"leaderrank\"            \"leverage\"             \n[25] \"lincent\"               \"lobby\"                 \"markovcent\"           \n[28] \"mnc\"                   \"pairwisedis\"           \"radiality\"            \n[31] \"salsa\"                 \"semilocal\"             \"topocoefficient\"      \n\n\nThe description of CINNA says “Functions for computing, comparing and demonstrating top informative centrality measures within a network.” Most of the indices in the package are imported from other package, such as centiserve. In addition, there are:\n\nDangalchev closeness (dangalchev_closeness_centrality())\ngroup centrality (group_centrality())\nharmonic closeness (harmonic_centrality())\nlocal bridging centrality (local_bridging_centrality())\n\nThe function calculate_centralities() can be used to calculate all applicable indices to a network. The primary purpose of the package is to facilitate the choice of indices by visual and statistical tools. If you are interested in the details, see this tutorial and this vignette.\ninfluenceR and keyplayer are comparably small packages which implement only a small number of indices.",
    "crumbs": [
      "Descriptive Network Analysis",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Centrality</span>"
    ]
  },
  {
    "objectID": "descriptive/centrality-basic.html#thoughts",
    "href": "descriptive/centrality-basic.html#thoughts",
    "title": "3  Centrality",
    "section": "3.5 Thoughts",
    "text": "3.5 Thoughts\nThe choice of indices can be overwhelming and little guidelines exist on when to choose what. The worst thing to do in any case is to apply a handful of indices and pick the result that suits your interpretation best. In best case, you have substantive arguments to apply an index and the result does match the hypothesis (or not).",
    "crumbs": [
      "Descriptive Network Analysis",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Centrality</span>"
    ]
  },
  {
    "objectID": "descriptive/centrality-basic.html#use-case-florentine-families",
    "href": "descriptive/centrality-basic.html#use-case-florentine-families",
    "title": "3  Centrality",
    "section": "3.6 Use case: Florentine Families",
    "text": "3.6 Use case: Florentine Families\nA classic example application of centrality indices is the “Florentine Families” dataset, which is included in the networkdata package.\n\ndata(\"flo_marriage\")\n\n\n\n\n\n\n\n\n\n\nTh network shows marriage ties among Renaissance Families in Florence. Marriages at that time were strategic to improve the standing of families in society. The size of the names is proportional to the wealth of the families. Although the Strozzi were the wealthiest family, it was ultimately the Medici who became the most powerful family. This is in part due to their central position within this marriage network.\nThe table bellow shows the ranking for the four most commonly used centrality indices (1=top rank).\n\n\n\n\n\nname\ndegree\nbetweenness\ncloseness\neigen\n\n\n\n\nAcciaiuoli\n13.5\n14\n11.5\n12\n\n\nAlbizzi\n6.5\n3\n3.5\n9\n\n\nBarbadori\n10.5\n8\n6.5\n10\n\n\nBischeri\n6.5\n6\n8.0\n6\n\n\nCastellani\n6.5\n10\n9.5\n8\n\n\nGinori\n13.5\n14\n13.0\n14\n\n\nGuadagni\n2.5\n2\n5.0\n5\n\n\nLamberteschi\n13.5\n14\n14.0\n13\n\n\nMedici\n1.0\n1\n1.0\n1\n\n\nPazzi\n13.5\n14\n15.0\n15\n\n\nPeruzzi\n6.5\n11\n11.5\n7\n\n\nPucci\n16.0\n14\n16.0\n16\n\n\nRidolfi\n6.5\n5\n2.0\n3\n\n\nSalviati\n10.5\n4\n9.5\n11\n\n\nStrozzi\n2.5\n7\n6.5\n2\n\n\nTornabuoni\n6.5\n9\n3.5\n4\n\n\n\n\n\nNo matter what structural feature we consider to be important, the Medici always have the most advantageous position.",
    "crumbs": [
      "Descriptive Network Analysis",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Centrality</span>"
    ]
  },
  {
    "objectID": "descriptive/centrality-advanced.html",
    "href": "descriptive/centrality-advanced.html",
    "title": "4  Advanced Centrality Concepts",
    "section": "",
    "text": "4.1 Packages Needed for this Chapter\nWhen looking at the vast amount of centrality indices, it may be reasonable to ask if there is any natural limit for what can be considered a centrality index. Concretely, are there any theoretical properties that an index has to have in order to be called a centrality index? There exist several axiomatic systems for centrality, which define some desirable properties that a proper index should have. While these systems are able to shed some light on specific groups of indices, they are in most cases not comprehensive. That is, it is often possible to construct counterexamples for most indices such that they do not fulfill the properties. Instead of the rather normative axiomatic approach, we explore a more descriptive approach. We will address the following questions:\nlibrary(igraph)\nlibrary(netrankr)",
    "crumbs": [
      "Descriptive Network Analysis",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Advanced Centrality Concepts</span>"
    ]
  },
  {
    "objectID": "descriptive/centrality-advanced.html#neighborhood-inclusion",
    "href": "descriptive/centrality-advanced.html#neighborhood-inclusion",
    "title": "4  Advanced Centrality Concepts",
    "section": "4.2 Neighborhood-inclusion",
    "text": "4.2 Neighborhood-inclusion\nLet us start by looking at the following two small examples.\n\ng1 &lt;- readRDS(\"data/example_1.rds\")\ng2 &lt;- readRDS(\"data/example_2.rds\")\n\n[PLOT]\n[APPLY INDICES]\nIt turns out that there actually is a very intuitive structural property that underlies many centrality indices. If a node has exactly the same neighbors as another and potentially some more, it will never be less central, independent of the choice of index. This property is called neighborhood-inclusion.\nAn illustration is given below. \nWe can calculate all pairs of neighborhood-inclusion with the function neighborhood_inclusion() in the netrankr package.\n\nP1 &lt;- neighborhood_inclusion(g1)\n\nThis graph was created by an old(er) igraph version.\nℹ Call `igraph::upgrade_graph()` on it to use with the current igraph version.\nFor now we convert it on the fly...\n\nP2 &lt;- neighborhood_inclusion(g2)\n\nThis graph was created by an old(er) igraph version.\nℹ Call `igraph::upgrade_graph()` on it to use with the current igraph version.\nFor now we convert it on the fly...\n\n\nAn entry P[i,j] is one if the neighborhood of i is included in the neighborhood of j and zero otherwise. With the function comparable_pairs(), we can check the fraction of comparable pairs. Let us start with the first network.\n\ncomparable_pairs(P1)\n\n[1] 0.1636364\n\n\nOnly 16% of pairs are comparable with neighborhood-inclusion. For a better understanding of the dominance relations, we can also visualize them as a graph.\nd1 &lt;- dominance_graph(P1)\n\nAn edge (i,j) is present, if P[i,j]=1. Centrality indices will always put these comparable pairs in the same order.\n[ADD EXAMPLE WITH INDEX PRESERVATION]\nMoving on to the second network.\n\ncomparable_pairs(P2)\n\n[1] 1\n\n\nSo all pairs are comparable by neighborhood-inclusion. Hence, all indices will induce the same ranking (up to some potential tied ranks, but no discordant pairs), as we already observed in the previous post.",
    "crumbs": [
      "Descriptive Network Analysis",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Advanced Centrality Concepts</span>"
    ]
  },
  {
    "objectID": "descriptive/centrality-advanced.html#threshold-graphs-and-correlation-among-indices",
    "href": "descriptive/centrality-advanced.html#threshold-graphs-and-correlation-among-indices",
    "title": "4  Advanced Centrality Concepts",
    "section": "4.3 Threshold graphs and correlation among indices",
    "text": "4.3 Threshold graphs and correlation among indices\nThe second example network is part of the class of threshold graphs. One of their defining features is that the partial ranking induced by neighborhood-inclusion is in fact a ranking. A random threshold graph can be created with the threshold_graph() function. The function takes two parameters, one for the number of nodes, and one (approximately) for the density. The class includes some well known graphs, such as the two below.\n\ntg1 &lt;- threshold_graph(n = 10, p = 1)\ntg2 &lt;- threshold_graph(n = 10, p = 0)\n\n\nWe know from the previous section that centrality indices will always produce the same ranking on these graphs. This allows us to reason about another topic that is frequently investigated: correlations among indices. Correlations are often attributed to the definitions of indices. Take closeness and betweenness. On first glance, they measure very different things: Being close to all nodes and being “in between” all nodes. Hence, we would expect them to be only weakly correlated. But threshold graphs give us a reason to believe, that correlations are not entirely dependent on the definitions but rather on structural features of the network. (This article gives more details and references on that topic. Let me know if you can’t access it).\nAs an illustration, we compare betweenness and closeness on a threshold graph and a threshold graph with added noise from a random graph.\n\n#threshold graph\ntg3 &lt;- threshold_graph(100,0.2)\n#noise graph\ngnp &lt;- sample_gnp(100,0.01)\nA1 &lt;- as_adjacency_matrix(tg3, sparse = FALSE)\nA2 &lt;- as_adjacency_matrix(gnp, sparse = FALSE)\n\n#construct a noise threshold graph\ntg3_noise &lt;- graph_from_adjacency_matrix(xor(A1,A2),mode = \"undirected\")\n\n#calculate discordant pairs for betweenness and closeness in both networks\ndisc1 &lt;- compare_ranks(betweenness(tg3),closeness(tg3))$discordant\ndisc2 &lt;- compare_ranks(betweenness(tg3_noise),closeness(tg3_noise))$discordant\nc(disc1,disc2)\n\n[1]   0 549\n\n\nOn the threshold graph we do not observe any discordant pairs for the two indices. However, the little noise we added to the threshold graph was enough to introduce 549 pairs of nodes that are now ranked differently. In general, we can say that\nThe closer a network is to be a threshold graph, the higher we expect the correlation of any pair of centrality indices to be, independent of their definition.\nBut how to define being close to a threshold graph? One obvious choice is to use the function comparable_pairs(). The more pairs are comparable, the less possibilities for indices to rank the nodes differently. Hence, we are close to a unique ranking obtained for threshold graphs. A second option is to use an appropriate distance measure for graphs. netrankr implements the so called majorization gap which operates on the degree sequences of graphs. In its essence, it returns the number of edges that need to be rewired, in order to turn an arbitrary graph into a threshold graph.\n\nmg1 &lt;- majorization_gap(tg3)\nmg2 &lt;- majorization_gap(tg3_noise)\nc(mg1,mg2)\n\n[1] 0.00000000 0.02525651\n\n\nThe result is given as a fraction of the total number of edges. So 12% of edges need to be rewired in the noisy graph to turn it into a threshold graph. To get the raw count, set norm=FALSE.\n\nmajorization_gap(tg3_noise,norm = FALSE)\n\n[1] 32",
    "crumbs": [
      "Descriptive Network Analysis",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Advanced Centrality Concepts</span>"
    ]
  },
  {
    "objectID": "descriptive/centrality-advanced.html#partial-centrality",
    "href": "descriptive/centrality-advanced.html#partial-centrality",
    "title": "4  Advanced Centrality Concepts",
    "section": "4.4 Partial Centrality",
    "text": "4.4 Partial Centrality\nThe function rank_intervals() is used to calculate the maximal and minimal possible rank for each node in any ranking that is in accordance with a given partial ranking.\n\ndata(\"dbces11\")\n\n#neighborhood inclusion \nP &lt;- neighborhood_inclusion(dbces11, sparse = FALSE)\n\nrank_intervals(P)\n\n node:A rank interval: [1, 6]\n node:B rank interval: [1, 9]\n node:C rank interval: [2, 9]\n node:D rank interval: [2, 11]\n node:E rank interval: [3, 11]\n node:F rank interval: [2, 11]\n node:G rank interval: [2, 11]\n node:H rank interval: [2, 11]\n node:I rank interval: [1, 11]\n node:J rank interval: [1, 11]\n node:K rank interval: [3, 11]\n\n\nThe package uses the convention, that higher numerical ranks correspond to top ranked position. The lowest possible rank is thus 1. The column mid_point should not be confused with the expected rank of nodes, which is calculated with the function exact_rank_prob().\n\nRank intervals are useful to assess the ambiguity of ranking nodes. The bigger the intervals are, the more freedom exists, e.g. for centrality indices, to rank nodes differently.\n\nThe intervals can be visualized with its own plot() function. The function can take a data frame of centrality scores as an additional parameter cent_scores. The ranks of each node for each index are then plotted within each interval. Again, the higher the numerical rank the higher ranked the node is according to the index.\n\ncent_scores &lt;- data.frame(\n   degree=degree(dbces11),\n   betweenness=round(betweenness(dbces11),4),\n   closeness=round(closeness(dbces11),4),\n   eigenvector=round(eigen_centrality(dbces11)$vector,4))\n\nrk_int &lt;- rank_intervals(P)\nplot(rk_int,cent_scores = cent_scores)\n\n\n\n\n\n\n\n\nA small jitter effect is added to the points to reduce over-plotting.\n\nNote that you may encounter situations, where ranks of centralities may fall outside of interval. This can happen in cases of ties in rankings, especially for betweenness centrality. Betweenness is, so far, the only index that does not strictly preserve neighborhood-inclusion. That is, while \\[\nN(u)\\subseteq N[v] \\text{ and } N(v)\\not\\subseteq N[u] \\implies c(u)&lt;c(v)\n\\] holds for most indices, betweenness fails to fulfill this property.\n\nThe intervals reduce to single points for threshold graphs, since all nodes are pairwise comparable by neighborhood-inclusion.\n\nset.seed(123)\ntg &lt;- threshold_graph(20,0.2)\n\n#neighborhood inclusion \nP &lt;- tg %&gt;% neighborhood_inclusion(sparse = FALSE)\n\n#without %&gt;% operator:\n# P &lt;- neighborhood_inclusion(tg,sparse = FALSE)\nplot(rank_intervals(P))\n\n\n\n\n\n\n\n\nThe described betweenness inconsistancy is most evident for threshold graphs as shown in the rank intervals below.\n\ncent_scores &lt;- data.frame(\n   degree=degree(tg),\n   betweenness=round(betweenness(tg),4),\n   closeness=round(closeness(tg),4),\n   eigenvector=round(eigen_centrality(tg)$vector,4))\n\n\nplot(rank_intervals(P),cent_scores = cent_scores)",
    "crumbs": [
      "Descriptive Network Analysis",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Advanced Centrality Concepts</span>"
    ]
  },
  {
    "objectID": "descriptive/centrality-advanced.html#exact-probabilities",
    "href": "descriptive/centrality-advanced.html#exact-probabilities",
    "title": "4  Advanced Centrality Concepts",
    "section": "4.5 Exact Probabilities",
    "text": "4.5 Exact Probabilities\nBefore calculating any probabilities consider the following example graph and the rankings induced by various centrality indices, shown as rank intervals.\n[MORE TEXT]\nBut first, let us briefly look at all the return values.\n\nres &lt;- exact_rank_prob(P)\n\nWarning in exact_rank_prob(P): P is already a ranking.\nExpected Ranks correspond to the only possible ranking.\n\nres\n\nNumber of possible centrality rankings:  1 \nEquivalence Classes (max. possible): 8 (20)\n- - - - - - - - - - \nRank Probabilities (rows:nodes/cols:ranks)\n    1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20\nV1  0 0 0 0 0 0 0 0 0  0  0  0  0  0  0  1  0  0  0  0\nV2  0 0 0 0 0 0 0 0 0  0  0  0  0  0  0  1  0  0  0  0\nV3  0 0 0 0 0 0 0 0 0  0  0  0  0  0  0  1  0  0  0  0\nV4  0 0 0 0 0 0 0 0 0  0  0  0  0  0  0  1  0  0  0  0\nV5  0 0 0 0 0 0 0 0 0  0  0  0  0  0  0  1  0  0  0  0\nV6  0 0 0 0 0 0 0 0 0  0  0  0  0  0  0  0  1  0  0  0\nV7  0 0 0 0 0 0 0 0 0  0  1  0  0  0  0  0  0  0  0  0\nV8  0 0 0 0 0 0 0 0 0  0  1  0  0  0  0  0  0  0  0  0\nV9  0 0 0 0 0 0 0 0 0  0  1  0  0  0  0  0  0  0  0  0\nV10 0 0 0 0 0 0 0 0 0  0  1  0  0  0  0  0  0  0  0  0\nV11 0 0 0 0 0 0 0 0 0  0  1  0  0  0  0  0  0  0  0  0\nV12 0 0 0 0 0 0 0 0 0  0  1  0  0  0  0  0  0  0  0  0\nV13 0 0 0 0 0 0 0 0 0  0  1  0  0  0  0  0  0  0  0  0\nV14 0 0 0 0 0 0 0 0 0  0  1  0  0  0  0  0  0  0  0  0\nV15 0 0 0 0 0 0 0 0 0  0  0  0  0  0  0  0  0  1  0  0\nV16 0 0 1 0 0 0 0 0 0  0  0  0  0  0  0  0  0  0  0  0\nV17 0 0 1 0 0 0 0 0 0  0  0  0  0  0  0  0  0  0  0  0\nV18 0 0 0 0 0 0 0 0 0  0  0  0  0  0  0  0  0  0  1  0\nV19 1 0 0 0 0 0 0 0 0  0  0  0  0  0  0  0  0  0  0  0\nV20 0 0 0 0 0 0 0 0 0  0  0  0  0  0  0  0  0  0  0  1\n- - - - - - - - - - \nRelative Rank Probabilities (row ranked lower than col)\n    V1 V2 V3 V4 V5 V6 V7 V8 V9 V10 V11 V12 V13 V14 V15 V16 V17 V18 V19 V20\nV1   0  0  0  0  0  1  0  0  0   0   0   0   0   0   1   0   0   1   0   1\nV2   0  0  0  0  0  1  0  0  0   0   0   0   0   0   1   0   0   1   0   1\nV3   0  0  0  0  0  1  0  0  0   0   0   0   0   0   1   0   0   1   0   1\nV4   0  0  0  0  0  1  0  0  0   0   0   0   0   0   1   0   0   1   0   1\nV5   0  0  0  0  0  1  0  0  0   0   0   0   0   0   1   0   0   1   0   1\nV6   0  0  0  0  0  0  0  0  0   0   0   0   0   0   1   0   0   1   0   1\nV7   1  1  1  1  1  1  0  0  0   0   0   0   0   0   1   0   0   1   0   1\nV8   1  1  1  1  1  1  0  0  0   0   0   0   0   0   1   0   0   1   0   1\nV9   1  1  1  1  1  1  0  0  0   0   0   0   0   0   1   0   0   1   0   1\nV10  1  1  1  1  1  1  0  0  0   0   0   0   0   0   1   0   0   1   0   1\nV11  1  1  1  1  1  1  0  0  0   0   0   0   0   0   1   0   0   1   0   1\nV12  1  1  1  1  1  1  0  0  0   0   0   0   0   0   1   0   0   1   0   1\nV13  1  1  1  1  1  1  0  0  0   0   0   0   0   0   1   0   0   1   0   1\nV14  1  1  1  1  1  1  0  0  0   0   0   0   0   0   1   0   0   1   0   1\nV15  0  0  0  0  0  0  0  0  0   0   0   0   0   0   0   0   0   1   0   1\nV16  1  1  1  1  1  1  1  1  1   1   1   1   1   1   1   0   0   1   0   1\nV17  1  1  1  1  1  1  1  1  1   1   1   1   1   1   1   0   0   1   0   1\nV18  0  0  0  0  0  0  0  0  0   0   0   0   0   0   0   0   0   0   0   1\nV19  1  1  1  1  1  1  1  1  1   1   1   1   1   1   1   1   1   1   0   1\nV20  0  0  0  0  0  0  0  0  0   0   0   0   0   0   0   0   0   0   0   0\n- - - - - - - - - - \nExpected Ranks (higher values are better)\n V1  V2  V3  V4  V5  V6  V7  V8  V9 V10 V11 V12 V13 V14 V15 V16 V17 V18 V19 V20 \n 16  16  16  16  16  17  11  11  11  11  11  11  11  11  18   3   3  19   1  20 \n- - - - - - - - - - \nSD of Rank Probabilities\n V1  V2  V3  V4  V5  V6  V7  V8  V9 V10 V11 V12 V13 V14 V15 V16 V17 V18 V19 V20 \n  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0 \n- - - - - - - - - - \n\n\nThe function returns an object of type which contains the result of a full probabilistic rank analysis. The specific list entries are discussed in the following subsections.\n\n4.5.1 Rank Probabilities\nInstead of insisting on fixed ranks of nodes as given by indices, we can use rank probabilities to assess the likelihood of certain rank. Formally, rank probabilities are simply defined as \\[\nP(rk(u)=k)=\\frac{\\lvert \\{rk \\in  \\mathcal{R}(\\leq) \\; : \\; rk(u)=k\\} \\rvert}{\\lvert \\mathcal{R}(\\leq) \\rvert}.\n\\] Rank probabilities are given by the return value rank.prob of the exact_rank_prob() function.\n\nrp &lt;- round(res$rank.prob,2)\nrp\n\n    1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20\nV1  0 0 0 0 0 0 0 0 0  0  0  0  0  0  0  1  0  0  0  0\nV2  0 0 0 0 0 0 0 0 0  0  0  0  0  0  0  1  0  0  0  0\nV3  0 0 0 0 0 0 0 0 0  0  0  0  0  0  0  1  0  0  0  0\nV4  0 0 0 0 0 0 0 0 0  0  0  0  0  0  0  1  0  0  0  0\nV5  0 0 0 0 0 0 0 0 0  0  0  0  0  0  0  1  0  0  0  0\nV6  0 0 0 0 0 0 0 0 0  0  0  0  0  0  0  0  1  0  0  0\nV7  0 0 0 0 0 0 0 0 0  0  1  0  0  0  0  0  0  0  0  0\nV8  0 0 0 0 0 0 0 0 0  0  1  0  0  0  0  0  0  0  0  0\nV9  0 0 0 0 0 0 0 0 0  0  1  0  0  0  0  0  0  0  0  0\nV10 0 0 0 0 0 0 0 0 0  0  1  0  0  0  0  0  0  0  0  0\nV11 0 0 0 0 0 0 0 0 0  0  1  0  0  0  0  0  0  0  0  0\nV12 0 0 0 0 0 0 0 0 0  0  1  0  0  0  0  0  0  0  0  0\nV13 0 0 0 0 0 0 0 0 0  0  1  0  0  0  0  0  0  0  0  0\nV14 0 0 0 0 0 0 0 0 0  0  1  0  0  0  0  0  0  0  0  0\nV15 0 0 0 0 0 0 0 0 0  0  0  0  0  0  0  0  0  1  0  0\nV16 0 0 1 0 0 0 0 0 0  0  0  0  0  0  0  0  0  0  0  0\nV17 0 0 1 0 0 0 0 0 0  0  0  0  0  0  0  0  0  0  0  0\nV18 0 0 0 0 0 0 0 0 0  0  0  0  0  0  0  0  0  0  1  0\nV19 1 0 0 0 0 0 0 0 0  0  0  0  0  0  0  0  0  0  0  0\nV20 0 0 0 0 0 0 0 0 0  0  0  0  0  0  0  0  0  0  0  1\n\n\nEntries rp[u,k] correspond to \\(P(rk(u)=k)\\).\n\nThe most interesting probabilities are certainly \\(P(rk(u)=n)\\), that is how likely is it for a node to be the most central.\n\nrp[,11]\n\n V1  V2  V3  V4  V5  V6  V7  V8  V9 V10 V11 V12 V13 V14 V15 V16 V17 V18 V19 V20 \n  0   0   0   0   0   0   1   1   1   1   1   1   1   1   0   0   0   0   0   0 \n\n\nRecall from the previous section that we found five indices that ranked \\(6,7,8,10\\) and \\(11\\) on top. The probability tell us now, how likely it is to find an index that rank these nodes on top. In this case, node \\(11\\) has the highest probability to be the most central node.\n\n\n4.5.2 Relative Rank Probabilities\nIn some cases, we might not necessarily be interested in a complete ranking of nodes, but only in the relative position of a subset of nodes. This idea leads to relative rank probabilities, that is formally defined as \\[\nP(rk(u)\\leq rk(v))=\\frac{\\lvert \\{rk \\in  \\mathcal{R}(\\leq) \\; : \\; rk(u)\\leq rk(v)\\} \\rvert}{\\lvert \\mathcal{R}(\\leq) \\rvert}.\n\\] Relative rank probabilities are given by the return value relative.rank of the exact_rank_prob() function.\n\nrrp &lt;- round(res$relative.rank,2)\nrrp\n\n    V1 V2 V3 V4 V5 V6 V7 V8 V9 V10 V11 V12 V13 V14 V15 V16 V17 V18 V19 V20\nV1   0  0  0  0  0  1  0  0  0   0   0   0   0   0   1   0   0   1   0   1\nV2   0  0  0  0  0  1  0  0  0   0   0   0   0   0   1   0   0   1   0   1\nV3   0  0  0  0  0  1  0  0  0   0   0   0   0   0   1   0   0   1   0   1\nV4   0  0  0  0  0  1  0  0  0   0   0   0   0   0   1   0   0   1   0   1\nV5   0  0  0  0  0  1  0  0  0   0   0   0   0   0   1   0   0   1   0   1\nV6   0  0  0  0  0  0  0  0  0   0   0   0   0   0   1   0   0   1   0   1\nV7   1  1  1  1  1  1  0  0  0   0   0   0   0   0   1   0   0   1   0   1\nV8   1  1  1  1  1  1  0  0  0   0   0   0   0   0   1   0   0   1   0   1\nV9   1  1  1  1  1  1  0  0  0   0   0   0   0   0   1   0   0   1   0   1\nV10  1  1  1  1  1  1  0  0  0   0   0   0   0   0   1   0   0   1   0   1\nV11  1  1  1  1  1  1  0  0  0   0   0   0   0   0   1   0   0   1   0   1\nV12  1  1  1  1  1  1  0  0  0   0   0   0   0   0   1   0   0   1   0   1\nV13  1  1  1  1  1  1  0  0  0   0   0   0   0   0   1   0   0   1   0   1\nV14  1  1  1  1  1  1  0  0  0   0   0   0   0   0   1   0   0   1   0   1\nV15  0  0  0  0  0  0  0  0  0   0   0   0   0   0   0   0   0   1   0   1\nV16  1  1  1  1  1  1  1  1  1   1   1   1   1   1   1   0   0   1   0   1\nV17  1  1  1  1  1  1  1  1  1   1   1   1   1   1   1   0   0   1   0   1\nV18  0  0  0  0  0  0  0  0  0   0   0   0   0   0   0   0   0   0   0   1\nV19  1  1  1  1  1  1  1  1  1   1   1   1   1   1   1   1   1   1   0   1\nV20  0  0  0  0  0  0  0  0  0   0   0   0   0   0   0   0   0   0   0   0\n\n\nEntries rrp[u,v] correspond to \\(P(rk(u)\\leq rk(v))\\).\n\nThe more a value rrp[u,v] deviates from \\(0.5\\) towards \\(1\\), the more confidence we gain that a node \\(v\\) is more central than a node \\(u\\).\n###Expected Ranks The expected rank of a node in centrality rankings is defined as the expected value of the rank probability distribution. That is, \\[\n\\rho(u)=\\sum_{k=1}^n k\\cdot P(rk(u)=k).\n\\] Expected ranks are given by the return value expected.rank of the exact_rank_prob() function.\n\nex_rk &lt;- round(res$expected.rank,2)\nex_rk\n\n V1  V2  V3  V4  V5  V6  V7  V8  V9 V10 V11 V12 V13 V14 V15 V16 V17 V18 V19 V20 \n 16  16  16  16  16  17  11  11  11  11  11  11  11  11  18   3   3  19   1  20 \n\n\nAs a reminder, the higher the numeric rank, the more central a node is. In this case, node \\(11\\) has the highest expected rank in any centrality ranking.",
    "crumbs": [
      "Descriptive Network Analysis",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Advanced Centrality Concepts</span>"
    ]
  },
  {
    "objectID": "descriptive/clustering.html",
    "href": "descriptive/clustering.html",
    "title": "5  Cohesive Subgroups",
    "section": "",
    "text": "5.1 Packages Needed for this Chapter\nIn this chapter, we explore cohesive subgroups within networks, focusing on cliques, community detection, blockmodeling, and core-periphery structures. Each concept offers a different way to examine how nodes cluster based on connectivity, from tightly-knit cliques, where every member is connected, to broader communities grouped by denser internal links. Blockmodeling abstracts these patterns into roles and relationships, while core-periphery structures reveal hierarchical organization with central and peripheral actors.\nlibrary(igraph)\nlibrary(networkdata)",
    "crumbs": [
      "Descriptive Network Analysis",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Cohesive Subgroups</span>"
    ]
  },
  {
    "objectID": "descriptive/clustering.html#cliques",
    "href": "descriptive/clustering.html#cliques",
    "title": "5  Cohesive Subgroups",
    "section": "5.2 Cliques",
    "text": "5.2 Cliques\nA clique in a network is a set of nodes that form a complete subnetwork within a network (called a complete subgraph). A maximal clique is a clique that cannot be extended to a bigger clique by addding more nodes to it.\n\ndata(\"clique_graph\")\n\nAll maximal cliques can be calculated with max_cliques() (only feasible for fairly small networks). The min parameter can be used to set a minimum size. Here, we want to ignore all cliques of size \\(2\\).\n\n# only return cliques with three or more nodes\ncl &lt;- max_cliques(clique_graph, min = 3)\ncl\n\n[[1]]\n+ 3/30 vertices, from 0193e05:\n[1]  9 17 18\n\n[[2]]\n+ 3/30 vertices, from 0193e05:\n[1] 7 4 5\n\n[[3]]\n+ 3/30 vertices, from 0193e05:\n[1] 7 4 8\n\n[[4]]\n+ 3/30 vertices, from 0193e05:\n[1] 10  2 11\n\n[[5]]\n+ 3/30 vertices, from 0193e05:\n[1] 16 12 15\n\n[[6]]\n+ 3/30 vertices, from 0193e05:\n[1] 6 1 5\n\n[[7]]\n+ 4/30 vertices, from 0193e05:\n[1] 12 13 15 14\n\n[[8]]\n+ 3/30 vertices, from 0193e05:\n[1] 12  2  1\n\n[[9]]\n+ 5/30 vertices, from 0193e05:\n[1] 1 2 5 4 3\n\n\nThe figure below shows the network and the found maximal cliques.\n\n\n\n\n\n\n\n\n\nRelated to cliques is the k-core decomposition of a network. A k-core is a subgraph in which every node has at least k neighbors within the subgraph. A k-core is thus a relaxed version of a clique.\nThe function coreness() can be used to calculate the k-core membership for each node.\n\nkcore &lt;- coreness(clique_graph)\nkcore\n\n [1] 4 4 4 4 4 3 2 2 2 2 2 3 3 3 3 3 2 2 1 1 1 1 1 1 1 1 1 1 1 1\n\n\n\n\n\n\n\n\n\n\n\nCliques are the prototypical and most strict definition of a cohesive subgroup in a network. In empirical networks, however, we rarely encounter situations where we can partition the whole network into a set of cliques. The relaxed version of this problem is that of clustering, also referred to as comunity detection.",
    "crumbs": [
      "Descriptive Network Analysis",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Cohesive Subgroups</span>"
    ]
  },
  {
    "objectID": "descriptive/clustering.html#comunity-detection",
    "href": "descriptive/clustering.html#comunity-detection",
    "title": "5  Cohesive Subgroups",
    "section": "5.3 Comunity detection",
    "text": "5.3 Comunity detection\nA cluster is loosely defined as a group of nodes which are internally densely and externally sparsely connected. The network below shows an example for a network with a visible and intuitive cluster structure.\n\n# labeL; clustered-graph\n#| echo: FALSE\n\nn1 &lt;- 5\nn2 &lt;- 20\nset.seed(1234)\ng &lt;- sample_islands(n1, n2, 0.9, 5)\ng &lt;- simplify(g)\nV(g)$grp &lt;- rep(LETTERS[1:n1], each = n2)\nggraph(g, \"stress\") +\n  geom_edge_link0(edge_linewidth = 0.2, edge_color = \"grey66\") +\n  geom_node_point(shape = 21, size = 5, aes(fill = grp), show.legend = FALSE) +\n  theme_void()\n\n\n\n\n\n\n\n\nIn contrast, the network below does not really seem to have any well defined cluster structure.\n\n\n\n\n\n\n\n\n\nThe following algorithms for graph clustering are implemented in igraph.\n\n\n [1] \"cluster_edge_betweenness\"  \"cluster_fast_greedy\"      \n [3] \"cluster_fluid_communities\" \"cluster_infomap\"          \n [5] \"cluster_label_prop\"        \"cluster_leading_eigen\"    \n [7] \"cluster_leiden\"            \"cluster_louvain\"          \n [9] \"cluster_optimal\"           \"cluster_spinglass\"        \n[11] \"cluster_walktrap\"         \n\n\nMost of these algorithms are based on “modularity maximization”. Modularity is defined as the fraction of edges that fall within given groups minus the expected fraction if edges were distributed at random.\nThe workflow of a cluster analysis is always the same, independent from the chosen method. We illustrate the workflow using the infamous karate club network.\n\ndata(\"karate\")\n\n\n\n\n\n\n\n\n\n\n\n# compute clustering\nclu &lt;- cluster_louvain(karate)\n\n# cluster membership vector\nmem &lt;- membership(clu)\nmem\n\n [1] 1 2 2 2 1 1 1 2 3 2 1 1 2 2 3 3 1 2 3 2 3 2 3 3 4 4 3 3 4 3 3 4 3 3\n\n# clusters as list\ncom &lt;- communities(clu)\ncom\n\n$`1`\n[1]  1  5  6  7 11 12 17\n\n$`2`\n [1]  2  3  4  8 10 13 14 18 20 22\n\n$`3`\n [1]  9 15 16 19 21 23 24 27 28 30 31 33 34\n\n$`4`\n[1] 25 26 29 32\n\n\nTo compare the quality of clusterings, we can compute the modularity score for each output.\n\nimc &lt;- cluster_infomap(karate)\nlec &lt;- cluster_leading_eigen(karate)\nloc &lt;- cluster_louvain(karate)\nsgc &lt;- cluster_spinglass(karate)\nwtc &lt;- cluster_walktrap(karate)\nscores &lt;- c(\n  infomap = modularity(karate, membership(imc)),\n  eigen = modularity(karate, membership(lec)),\n  louvain = modularity(karate, membership(loc)),\n  spinglass = modularity(karate, membership(sgc)),\n  walk = modularity(karate, membership(wtc))\n)\nscores\n\n  infomap     eigen   louvain spinglass      walk \n0.4020381 0.3934089 0.4151052 0.4197896 0.3532216 \n\n\nFor the karate network, cluster_spinglass() produces the highest modularity score. The corresponding clustering is shown below.\n\n\n\n\n\n\n\n\n\nModularity maximization is still widely considered as the state-of-the-art clustering method for networks. There are, however, some technical shortcomings that one should be aware of. One of those is the so called “resolution limit”. When modularity is being maximized, it can happen that smaller clusters are merged together to form bigger clusters. The prime example is the graph that consists of cliques connected in a ring.\nThe figure below shows such a graph, consisting of 50 cliques of size 5.\n\n\n\n\n\n\n\n\n\nIntuitively, any clustering method should return a cluster for each clique.\n\nclu_louvain &lt;- cluster_louvain(K50)\ntable(membership(clu_louvain))\n\n\n 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 \n15 10 10 10 10 15 10 10 10 10 10 15 10 10 10 10 10 10 10 10 15 10 10 \n\n\nA clustering algorithm that fixes this issue is the leiden algorithm.\n\nclu_leiden &lt;- cluster_leiden(\n  K50,\n  objective_function = \"CPM\",\n  resolution_parameter = 0.5\n)\n\nWarning: The `resolution_parameter` argument of `cluster_leiden()` is deprecated as of\nigraph 2.1.0.\nℹ Please use the `resolution` argument instead.\n\ntable(membership(clu_leiden))\n\n\n 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 \n 5  5  5  5  5  5  5  5  5  5  5  5  5  5  5  5  5  5  5  5  5  5  5  5  5  5 \n27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 \n 5  5  5  5  5  5  5  5  5  5  5  5  5  5  5  5  5  5  5  5  5  5  5  5 \n\n\nThe figure below shows the clusters computed with the louvain method in grey and the leiden method in red.\n\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\n\nDon't know how to automatically pick scale for object of type &lt;membership&gt;.\nDefaulting to continuous.\nDon't know how to automatically pick scale for object of type &lt;membership&gt;.\nDefaulting to continuous.\n\n\n\n\n\n\n\n\n\nIf you are interested in the technical details of the Leiden method, check out the original paper.\nThe netUtils package includes the function sample_lfr() which implements the well-known Lancichinetti–Fortunato–Radicchi benchmark algorithm to generate artificial networks with a priori known communities and they can be used to compare different community detection methods.",
    "crumbs": [
      "Descriptive Network Analysis",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Cohesive Subgroups</span>"
    ]
  },
  {
    "objectID": "descriptive/clustering.html#blockmodeling",
    "href": "descriptive/clustering.html#blockmodeling",
    "title": "5  Cohesive Subgroups",
    "section": "5.4 Blockmodeling",
    "text": "5.4 Blockmodeling\nBlockmodeling is a more formal approach that aims to simplify the network’s structure into blocks based on patterns of connections between nodes. Instead of focusing on the density of connections, it categorizes the relationships between different groups (or blocks) of nodes according to the roles they play in the network.\nThe goal is to reduce the complexity of the network by identifying roles and positions within the network, where nodes in the same block have similar patterns of connections to other blocks, rather than necessarily being densely connected to each other.\nBlockmodeling involves partitioning the network into blocks and then modeling the connections between these blocks. It can be done through conventional (deterministic) or stochastic approaches, including k-block modeling and stochastic blockmodeling.\nBlockmodeling is particularly useful in sociology for role analysis and in organizational studies, where it’s important to understand how different groups (e.g., departments, hierarchies) interact, regardless of the density of the connections within each group.\nThere are several packages that implement different kinds of (stochastic) blockmodels. The most basic approaches are implemented in the package blockmodeling.\n\nlibrary(blockmodeling)\n\nIn principle, blockmodels can also be used for clustering, as we will illustrate on this random network with 3 dense blocks of size 20.\n\n\n\n\n\n\n\n\n\nThe disadvantage is that we need to specify a lot more parameters than for community detection.\n\nA &lt;- as_adj(g)\n\nWarning: `as_adj()` was deprecated in igraph 2.1.0.\nℹ Please use `as_adjacency_matrix()` instead.\n\nblk &lt;- matrix(\n  c(\n    \"com\",\n    \"nul\",\n    \"nul\",\n    \"nul\",\n    \"com\",\n    \"nul\",\n    \"nul\",\n    \"nul\",\n    \"com\"\n  ),\n  nrow = 3\n)\nblk\n\n     [,1]  [,2]  [,3] \n[1,] \"com\" \"nul\" \"nul\"\n[2,] \"nul\" \"com\" \"nul\"\n[3,] \"nul\" \"nul\" \"com\"\n\nres &lt;- optRandomParC(\n  M = A,\n  k = 3,\n  approaches = \"bin\",\n  blocks = blk,\n  rep = 5,\n  mingr = 20,\n  maxgr = 20\n)\n\n\n\nStarting optimization of the partiton 1 of 5 partitions.\nStarting partition: 3 2 2 2 3 3 2 1 2 1 2 3 3 1 3 2 3 3 2 2 1 1 3 3 3 2 3 2 3 2 3 1 2 1 3 1 1 1 1 1 1 1 3 1 3 1 2 1 3 2 1 1 1 3 3 2 2 2 2 2 \nFinal error: 342 \nFinal partition:    3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 \n\n\nStarting optimization of the partiton 2 of 5 partitions.\nStarting partition: 1 1 2 1 3 1 1 1 1 2 3 2 1 1 3 1 3 2 1 3 2 2 3 1 3 3 2 2 2 3 1 1 2 3 1 3 1 3 2 2 3 1 2 1 3 1 3 3 2 2 3 2 3 2 2 2 1 3 2 3 \nFinal error: 342 \nFinal partition:    1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 \n\n\nStarting optimization of the partiton 3 of 5 partitions.\nStarting partition: 3 1 2 3 2 3 1 1 1 2 2 1 3 2 3 3 2 1 2 3 3 3 3 1 2 3 1 1 2 2 2 1 3 1 2 3 1 2 2 3 2 2 1 1 1 1 1 1 3 3 3 2 2 1 1 2 3 3 3 2 \nFinal error: 342 \nFinal partition:    1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 \n\n\nStarting optimization of the partiton 4 of 5 partitions.\nStarting partition: 1 3 3 1 3 3 3 2 1 2 1 3 2 3 3 1 2 3 1 1 2 3 2 2 1 2 1 1 2 1 3 2 2 1 2 2 3 3 2 3 1 3 3 2 3 1 3 1 1 2 1 1 3 2 1 3 2 2 1 2 \nFinal error: 342 \nFinal partition:    1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 \n\n\nStarting optimization of the partiton 5 of 5 partitions.\nStarting partition: 3 2 2 2 2 3 3 1 2 2 1 3 1 3 3 1 2 2 1 3 2 1 2 1 1 2 3 2 1 3 1 2 1 3 3 3 3 3 2 2 1 1 1 3 1 2 2 1 3 2 3 2 1 3 3 2 1 3 1 1 \nFinal error: 342 \nFinal partition:    2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 \n\n\nOptimization of all partitions completed\nAll 5 solutions have err 342 \n\n\n\nk: number of blocks needs to be specified beforehand\napproaches: defines the type of blockmodel approach to be used. “bin” is for binary and “val” for valued blockmodeling. There are several more possibilities available in the help of the function\nblocks: allowed block types. Basically, what defines a block in the network. In our example we give a strict patterning that corresponds to a clustering. The diagonal blocks should be complete (“com”) and offdiagonals should be empty (“nul”). So in the best case, we have 3 disconnected cliques. Again, consult the help for more available block options.\nrep: number of random starting partitions to start the iteration from\nmingr and maxgr: min and max size of the blocks.\n\nthe result can be accessed with clu.\n\nclu(res)\n\n [1] 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n[39] 1 1 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n\n\nNote that this type of Blockmodeling is computationally expensive and best suited for small networks.\nLooking at a more realistic dataset, we load the baker dataset from the blockmodeling package. The dataset includes citation data between social work journals for 1985-86.\n\ndata(\"baker\")\ndiag(baker) &lt;- 0\n\nplotMat(baker, main = \"Baker Network Data\", mar = c(1, 1, 3, 1), title.line = 2)\n\n\n\n\n\n\n\n\nFirst, we run a binary blockmodel. This time we increase the number of repetions to 1000 and instead of giving a lear block structure, we just specify, what type of blocks we want our result to include. How they are distributed, we do not care and let the algorithm decide. We run the optimization in parallel (nCores = 0), which requires the packages doParallel and doRNG to be installed.\n\nbaker_binary &lt;- baker\nbaker_binary[baker_binary &gt; 0] &lt;- 1\n\nres_baker_binary &lt;- optRandomParC(\n  M = baker_binary,\n  k = 3,\n  rep = 1000,\n  nCores = 0,\n  blocks = c(\"nul\", \"com\"),\n  approach = \"bin\"\n)\n\nLoading required namespace: doParallel\n\n\nLoading required namespace: doRNG\n\n\nWarning in optRandomParC(M = baker_binary, k = 3, rep = 1000, nCores = 0, :\nuseLB not set and now set to TRUE. parLapplyLB will be used. Results will not\nbe reproducible.\n\n\n\n\nOptimization of all partitions completed\n1 solution(s) with minimal error = 47 found. \n\n\nThe obtained optimal block structure can be accessed via IM.\n\nIM(res_baker_binary)\n\n     [,1]  [,2]  [,3] \n[1,] \"com\" \"nul\" \"com\"\n[2,] \"com\" \"nul\" \"nul\"\n[3,] \"com\" \"nul\" \"com\"\n\n\nThe resulting blocks can be visualized via the plot function.\n\nplot(\n  res_baker_binary,\n  main = \"Baker Binary Network Data\",\n  mar = c(1, 2, 3, 1),\n  title.line = 2\n)\n\n\n\n\n\n\n\n\nNow we run a valued blockmodel on the original data. The parameter preSpecM is set to the median of the non-zero entries and defines a kind of cutoff for when to consider a value high enough to be a block internal tie.\n\nres_baker_valued &lt;- optRandomParC(\n  M = baker,\n  k = 3,\n  rep = 1000,\n  preSpecM = 13,\n  approach = \"val\",\n  blocks = c(\"nul\", \"com\"),\n  nCores = 0\n)\n\nLoading required namespace: doRNG\n\n\nWarning in optRandomParC(M = baker, k = 3, rep = 1000, preSpecM = 13, approach\n= \"val\", : useLB not set and now set to TRUE. parLapplyLB will be used. Results\nwill not be reproducible.\n\n\n\n\nOptimization of all partitions completed\n1 solution(s) with minimal error = 626 found. \n\n\n\nIM(res_baker_valued)\n\n     [,1]  [,2]  [,3] \n[1,] \"nul\" \"nul\" \"com\"\n[2,] \"nul\" \"nul\" \"nul\"\n[3,] \"com\" \"nul\" \"com\"\n\n\n\nplot(\n  res_baker_valued,\n  main = \"Baker Valued Network Data\",\n  mar = c(1, 2, 3, 1),\n  title.line = 2\n)",
    "crumbs": [
      "Descriptive Network Analysis",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Cohesive Subgroups</span>"
    ]
  },
  {
    "objectID": "descriptive/clustering.html#core-periphery",
    "href": "descriptive/clustering.html#core-periphery",
    "title": "5  Cohesive Subgroups",
    "section": "5.5 Core-Periphery",
    "text": "5.5 Core-Periphery\nCommunity detection aims to find clusters or groups of nodes within the network that are more densely connected to each other than to nodes outside the group. The core-periphery structure, in contrast, posits that a network is organized into a densely connected core and a sparsely connected periphery. The core consists of central nodes that are highly connected to each other and also to peripheral nodes. Peripheral nodes, on the other hand, have fewer connections and are mostly connected to nodes in the core rather than to each other.\nthe netutils package includes a function core_periphery() which allows to fit a discrete core-periphery model to a network.\n\nlibrary(netUtils)\n\nTo illustrate the function, we construct a graph which has a perfect core-periphery structure, also known as a split graph.\n\nset.seed(1234)\nsg &lt;- split_graph(n = 50, p = 0.35, core = 0.3)\n\nThe created graph has 50 nodes and 30% of all nodes are in the core. The probability that a periphery node connects to a core node is 0.35.\n\n\n\n\n\n\n\n\n\nRunning the core_periphery() function on this idealized graph should give the optimal result.\n\ncore_periphery(sg)\n\n$vec\n [1] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n[39] 0 0 0 0 0 0 0 0 0 0 0 0\n\n$corr\n[1] 1\n\n\nThe function returns a list with two entries. The first, vec, returns the membership of nodes. It is one if a node is in the core and 0 if not. The second entry corr is the correlation of the adjacency matrix with the ideal adjacency matrix given from the vec memberships. In this case, the correlation is one meaning that we have discovered the perfect core-periphery structure. In empirical applications, we rarely expect such a good fit.\nThe function also has an argument which allows to use different optimization techniques to find core-periphery structures (the problem of finding the optimal solution is too complex). For illustration, we rewire some edges of the graph sg to destroy the idealized structure and use all implemented optimization methods.\n\nset.seed(45)\nsg1 &lt;- rewire(sg, each_edge(0.25))\ncore_periphery(sg1, method = \"rk1_dc\")\n\n$vec\n [1] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n[39] 0 0 0 0 0 0 0 0 0 0 0 0\n\n$corr\n[1] 0.6473649\n\ncore_periphery(sg1, method = \"rk1_ec\")\n\n$vec\n [1] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n[39] 0 0 0 0 0 0 0 0 0 0 0 0\n\n$corr\n[1] 0.6473649\n\ncore_periphery(sg1, method = \"GA\")\n\n$vec\n [1] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n[39] 0 0 0 0 0 0 0 0 0 0 0 0\n\n$corr\n[1] 0.6473649\n\n\nrk1_dc and rk1_ec are so called rank-one matix approximation methods which infer an idealized structure via the degrees and eigenvectors of the adjacency matrix. These methods are extremely fast but might result in a lower quality of the result compared to GA which runs a genetic algorithm.\nThere are several extensions to this simple discrete core-periphery model. An extension for weighted networks is implemented in the package ITNr.",
    "crumbs": [
      "Descriptive Network Analysis",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Cohesive Subgroups</span>"
    ]
  },
  {
    "objectID": "descriptive/clustering.html#scientific-reading",
    "href": "descriptive/clustering.html#scientific-reading",
    "title": "5  Cohesive Subgroups",
    "section": "5.6 Scientific Reading",
    "text": "5.6 Scientific Reading\nClauset, A.; Newman, M. E. J. & Moore, C. Finding community structure in very large networks, Physical Review E 2004, 70, 066111\nVincent D. Blondel, Jean-Loup Guillaume, Renaud Lambiotte, Etienne Lefebvre: Fast unfolding of communities in large networks. J. Stat. Mech. (2008) P10008\nTraag, V. A., Waltman, L., & van Eck, N. J. (2019). From Louvain to Leiden: guaranteeing well-connected communities. Scientific reports, 9(1), 5233.\nŽiberna, A. (2007). Generalized Blockmodeling of Valued Networks. Social Networks, 29(1), 105-126. doi: 10.1016/j.socnet.2006.04.002\nŽiberna, A. (2008). Direct and indirect approaches to blockmodeling of valued networks in terms of regular equivalence. Journal of Mathematical Sociology, 32(1), 57-84. doi: 10.1080/00222500701790207\nŽiberna, A. (2014). Blockmodeling of multilevel networks. Social Networks, 39(1), 46-61. doi: 10.1016/j.socnet.2014.04.002\nBorgatti, Stephen P., and Martin G. Everett. “Models of core/periphery structures.” Social networks 21.4 (2000): 375-395.",
    "crumbs": [
      "Descriptive Network Analysis",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Cohesive Subgroups</span>"
    ]
  },
  {
    "objectID": "descriptive/two-mode-networks.html",
    "href": "descriptive/two-mode-networks.html",
    "title": "6  Two-Mode Networks",
    "section": "",
    "text": "6.1 Packages Needed for this Chapter\nA two-mode network is a network that consists of two disjoint sets of nodes (like people and events). Ties connect the two sets, for example participation of people in events. Other examples are\nThere are two ways of analysing a two-mode network. Either directly by using methods specifically created for such networks, or by projecting it to a regular one-mode network. The advantage of the former is that there is no information loss and the advantage of the latter is that we are working with more familiar data structures. The projection approach is more popular these days, but we will still introduce some direct methods to analyse two-mode networks. The main part of this chapter will however deal with the projection approach.\nlibrary(igraph)\nlibrary(networkdata)",
    "crumbs": [
      "Descriptive Network Analysis",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Two-Mode Networks</span>"
    ]
  },
  {
    "objectID": "descriptive/two-mode-networks.html#two-mode-data-structure",
    "href": "descriptive/two-mode-networks.html#two-mode-data-structure",
    "title": "6  Two-Mode Networks",
    "section": "6.2 Two-mode data structure",
    "text": "6.2 Two-mode data structure\nWe will discuss some methods tailored for two-mode networks via the famous “southern women” dataset consisting of 18 women who attended a series of 14 events. The network is included in the networkdata package.\n\ndata(\"southern_women\")\nsouthern_women\n\nIGRAPH 1074643 UN-B 32 89 -- \n+ attr: type (v/l), name (v/c)\n+ edges from 1074643 (vertex names):\n [1] EVELYN   --6/27 EVELYN   --3/2  EVELYN   --4/12 EVELYN   --9/26\n [5] EVELYN   --2/25 EVELYN   --5/19 EVELYN   --9/16 EVELYN   --4/8 \n [9] LAURA    --6/27 LAURA    --3/2  LAURA    --4/12 LAURA    --2/25\n[13] LAURA    --5/19 LAURA    --3/15 LAURA    --9/16 THERESA  --3/2 \n[17] THERESA  --4/12 THERESA  --9/26 THERESA  --2/25 THERESA  --5/19\n[21] THERESA  --3/15 THERESA  --9/16 THERESA  --4/8  BRENDA   --6/27\n[25] BRENDA   --4/12 BRENDA   --9/26 BRENDA   --2/25 BRENDA   --5/19\n[29] BRENDA   --3/15 BRENDA   --9/16 CHARLOTTE--4/12 CHARLOTTE--9/26\n+ ... omitted several edges\n\n\nigraph interprets a network as a two-mode network if it has a logical node attribute called type.\n\ntable(V(southern_women)$type)\n\n\nFALSE  TRUE \n   18    14 \n\n\n\n\n\n\n\n\n\n\n\nThe adjacency matrix of a two-mode network is referred to as biadjacency matrix and can be obtained via as_biadjacency_matrix().\n\nA &lt;- as_biadjacency_matrix(southern_women)\nA\n\n          6/27 3/2 4/12 9/26 2/25 5/19 3/15 9/16 4/8 6/10 2/23 4/7 11/21 8/3\nEVELYN       1   1    1    1    1    1    0    1   1    0    0   0     0   0\nLAURA        1   1    1    0    1    1    1    1   0    0    0   0     0   0\nTHERESA      0   1    1    1    1    1    1    1   1    0    0   0     0   0\nBRENDA       1   0    1    1    1    1    1    1   0    0    0   0     0   0\nCHARLOTTE    0   0    1    1    1    0    1    0   0    0    0   0     0   0\nFRANCES      0   0    1    0    1    1    0    1   0    0    0   0     0   0\nELEANOR      0   0    0    0    1    1    1    1   0    0    0   0     0   0\nPEARL        0   0    0    0    0    1    0    1   1    0    0   0     0   0\nRUTH         0   0    0    0    1    0    1    1   1    0    0   0     0   0\nVERNE        0   0    0    0    0    0    1    1   1    0    0   1     0   0\nMYRNA        0   0    0    0    0    0    0    1   1    1    0   1     0   0\nKATHERINE    0   0    0    0    0    0    0    1   1    1    0   1     1   1\nSYLVIA       0   0    0    0    0    0    1    1   1    1    0   1     1   1\nNORA         0   0    0    0    0    1    1    0   1    1    1   1     1   1\nHELEN        0   0    0    0    0    0    1    1   0    1    1   1     0   0\nDOROTHY      0   0    0    0    0    0    0    1   1    0    0   0     0   0\nOLIVIA       0   0    0    0    0    0    0    0   1    0    1   0     0   0\nFLORA        0   0    0    0    0    0    0    0   1    0    1   0     0   0",
    "crumbs": [
      "Descriptive Network Analysis",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Two-Mode Networks</span>"
    ]
  },
  {
    "objectID": "descriptive/two-mode-networks.html#direct-approach",
    "href": "descriptive/two-mode-networks.html#direct-approach",
    "title": "6  Two-Mode Networks",
    "section": "6.3 Direct Approach",
    "text": "6.3 Direct Approach\nThe tnet and bipartite packages offer some methods to analyse two mode networks directly, by adapting tools for standard (one-mode) networks, like the methods described in previous sections.\n\nlibrary(tnet)\n\ntnet implements a version of the clustering coefficient for two-mode networks. Remember that its one-mode equivalent is based on triangle counts, a structure that cannot exist in two-mode networks (think about it for a second).\n\ntransitivity(southern_women)\n\n[1] 0\n\ntransitivity(southern_women, type = \"local\")\n\n   EVELYN     LAURA   THERESA    BRENDA CHARLOTTE   FRANCES   ELEANOR     PEARL \n        0         0         0         0         0         0         0         0 \n     RUTH     VERNE     MYRNA KATHERINE    SYLVIA      NORA     HELEN   DOROTHY \n        0         0         0         0         0         0         0         0 \n   OLIVIA     FLORA      6/27       3/2      4/12      9/26      2/25      5/19 \n        0         0         0         0         0         0         0         0 \n     3/15      9/16       4/8      6/10      2/23       4/7     11/21       8/3 \n        0         0         0         0         0         0         0         0 \n\n\nThe version implemented in tnet is based on cycles of length 6, which involves three nodes of each mode.\n\nel_women &lt;- as_edgelist(southern_women, names = FALSE)\n\nclustering_tm(el_women)\n\n[1] 0.7718968\n\n# coefficient for first mode\nclustering_local_tm(el_women)\n\n   node        lc\n1     1 0.7666667\n2     2 0.8421751\n3     3 0.7523437\n4     4 0.8387909\n5     5 1.0000000\n6     6 0.8690476\n7     7 0.7959184\n8     8 0.6462585\n9     9 0.6702509\n10   10 0.6740891\n11   11 0.7138810\n12   12 0.7695560\n13   13 0.7461929\n14   14 0.8379501\n15   15 0.8159204\n16   16 0.5407407\n17   17 0.5806452\n18   18 0.5806452\n\n# coefficient for second mode\nclustering_local_tm(el_women[, 2:1])\n\n   node        lc\n1     1       NaN\n2     2       NaN\n3     3       NaN\n4     4       NaN\n5     5       NaN\n6     6       NaN\n7     7       NaN\n8     8       NaN\n9     9       NaN\n10   10       NaN\n11   11       NaN\n12   12       NaN\n13   13       NaN\n14   14       NaN\n15   15       NaN\n16   16       NaN\n17   17       NaN\n18   18       NaN\n19   19 1.0000000\n20   20 0.9487179\n21   21 0.9532967\n22   22 0.9644970\n23   23 0.9628253\n24   24 0.8135593\n25   25 0.7171825\n26   26 0.7791580\n27   27 0.7353630\n28   28 0.8544601\n29   29 0.9555556\n30   30 0.8844765\n31   31 0.8709677\n32   32 0.8709677\n\n\nNote that it is very cumbersome to count these cycles. It is advisable to run this function only on fairly small networks.\nThe package does include some more two-mode specific functions (look for *_tm()), but the outcomes are equivalent to using its counterpart in igraph.\nThe bipartite package is tailored towards ecological network analysis. Relevant functions for standard two-mode networks are the same as in tnet.",
    "crumbs": [
      "Descriptive Network Analysis",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Two-Mode Networks</span>"
    ]
  },
  {
    "objectID": "descriptive/two-mode-networks.html#projection-approach",
    "href": "descriptive/two-mode-networks.html#projection-approach",
    "title": "6  Two-Mode Networks",
    "section": "6.4 Projection Approach",
    "text": "6.4 Projection Approach\n\n6.4.1 Weighted Projection\nBesides analyzing a two-mode network as-is, there is also the possibility to project it to one mode. Mathematically, this is done by calculating \\(AA^T\\) or \\(A^TA\\), depending which mode we project on. As an example, consider the southern women dataset again.\n\nB &lt;- A %*% t(A)\nB\n\n          EVELYN LAURA THERESA BRENDA CHARLOTTE FRANCES ELEANOR PEARL RUTH\nEVELYN         8     6       7      6         3       4       3     3    3\nLAURA          6     7       6      6         3       4       4     2    3\nTHERESA        7     6       8      6         4       4       4     3    4\nBRENDA         6     6       6      7         4       4       4     2    3\nCHARLOTTE      3     3       4      4         4       2       2     0    2\nFRANCES        4     4       4      4         2       4       3     2    2\nELEANOR        3     4       4      4         2       3       4     2    3\nPEARL          3     2       3      2         0       2       2     3    2\nRUTH           3     3       4      3         2       2       3     2    4\nVERNE          2     2       3      2         1       1       2     2    3\nMYRNA          2     1       2      1         0       1       1     2    2\nKATHERINE      2     1       2      1         0       1       1     2    2\nSYLVIA         2     2       3      2         1       1       2     2    3\nNORA           2     2       3      2         1       1       2     2    2\nHELEN          1     2       2      2         1       1       2     1    2\nDOROTHY        2     1       2      1         0       1       1     2    2\nOLIVIA         1     0       1      0         0       0       0     1    1\nFLORA          1     0       1      0         0       0       0     1    1\n          VERNE MYRNA KATHERINE SYLVIA NORA HELEN DOROTHY OLIVIA FLORA\nEVELYN        2     2         2      2    2     1       2      1     1\nLAURA         2     1         1      2    2     2       1      0     0\nTHERESA       3     2         2      3    3     2       2      1     1\nBRENDA        2     1         1      2    2     2       1      0     0\nCHARLOTTE     1     0         0      1    1     1       0      0     0\nFRANCES       1     1         1      1    1     1       1      0     0\nELEANOR       2     1         1      2    2     2       1      0     0\nPEARL         2     2         2      2    2     1       2      1     1\nRUTH          3     2         2      3    2     2       2      1     1\nVERNE         4     3         3      4    3     3       2      1     1\nMYRNA         3     4         4      4    3     3       2      1     1\nKATHERINE     3     4         6      6    5     3       2      1     1\nSYLVIA        4     4         6      7    6     4       2      1     1\nNORA          3     3         5      6    8     4       1      2     2\nHELEN         3     3         3      4    4     5       1      1     1\nDOROTHY       2     2         2      2    1     1       2      1     1\nOLIVIA        1     1         1      1    2     1       1      2     2\nFLORA         1     1         1      1    2     1       1      2     2\n\n\nThis matrix can now be interpreted as a weighted network among the 18 women. Each entry corresponds to the number of times two women went to the same event.\nThe same can be achieved with the function bipartite_projection(), which returns both projections.\n\nprojs &lt;- bipartite_projection(southern_women)\nprojs\n\n$proj1\nIGRAPH a2c8a0f UNW- 18 139 -- \n+ attr: name (v/c), weight (e/n)\n+ edges from a2c8a0f (vertex names):\n [1] EVELYN --LAURA     EVELYN --BRENDA    EVELYN --THERESA   EVELYN --CHARLOTTE\n [5] EVELYN --FRANCES   EVELYN --ELEANOR   EVELYN --RUTH      EVELYN --PEARL    \n [9] EVELYN --NORA      EVELYN --VERNE     EVELYN --MYRNA     EVELYN --KATHERINE\n[13] EVELYN --SYLVIA    EVELYN --HELEN     EVELYN --DOROTHY   EVELYN --OLIVIA   \n[17] EVELYN --FLORA     LAURA  --BRENDA    LAURA  --THERESA   LAURA  --CHARLOTTE\n[21] LAURA  --FRANCES   LAURA  --ELEANOR   LAURA  --RUTH      LAURA  --PEARL    \n[25] LAURA  --NORA      LAURA  --VERNE     LAURA  --SYLVIA    LAURA  --HELEN    \n[29] LAURA  --MYRNA     LAURA  --KATHERINE LAURA  --DOROTHY   THERESA--BRENDA   \n+ ... omitted several edges\n\n$proj2\nIGRAPH e448615 UNW- 14 66 -- \n+ attr: name (v/c), weight (e/n)\n+ edges from e448615 (vertex names):\n [1] 6/27--3/2   6/27--4/12  6/27--9/26  6/27--2/25  6/27--5/19  6/27--9/16 \n [7] 6/27--4/8   6/27--3/15  3/2 --4/12  3/2 --9/26  3/2 --2/25  3/2 --5/19 \n[13] 3/2 --9/16  3/2 --4/8   3/2 --3/15  4/12--9/26  4/12--2/25  4/12--5/19 \n[19] 4/12--9/16  4/12--4/8   4/12--3/15  9/26--2/25  9/26--5/19  9/26--9/16 \n[25] 9/26--4/8   9/26--3/15  2/25--5/19  2/25--9/16  2/25--4/8   2/25--3/15 \n[31] 5/19--9/16  5/19--4/8   5/19--3/15  5/19--6/10  5/19--2/23  5/19--4/7  \n[37] 5/19--11/21 5/19--8/3   3/15--9/16  3/15--4/8   3/15--4/7   3/15--6/10 \n[43] 3/15--11/21 3/15--8/3   3/15--2/23  9/16--4/8   9/16--4/7   9/16--6/10 \n+ ... omitted several edges\n\n\n\n\n\n\n\n\n\n\n\nAs you can see, the network is weighted and very dense. In principle it is possible to analyze the network as is, but a very common step is to binarize the network. In doing so, we basically turn the network into a simple undirected one-mode network. This makes all methods we described in the first few sections applicable to the network (at least in theory).\n\n\n6.4.2 Simple Binary Projections\nThe simplest way of binarizing a weighted projection is to define a global threshold and remove a tie if its weight is below the global threshold. A popular choice is to take the mean edge weight (sometimes also plus the 1-2 times the standard deviation).\n\nwomen_proj &lt;- projs$proj1\nthreshold &lt;- mean(E(projs$proj1)$weight)\nwomen_bin &lt;- delete_edges(women_proj, which(E(women_proj)$weight &lt;= threshold))\nwomen_bin &lt;- delete_edge_attr(women_bin, \"weight\")\nwomen_bin\n\nIGRAPH af676e5 UN-- 18 46 -- \n+ attr: name (v/c)\n+ edges from af676e5 (vertex names):\n [1] EVELYN --LAURA     EVELYN --BRENDA    EVELYN --THERESA   EVELYN --CHARLOTTE\n [5] EVELYN --FRANCES   EVELYN --ELEANOR   EVELYN --RUTH      EVELYN --PEARL    \n [9] LAURA  --BRENDA    LAURA  --THERESA   LAURA  --CHARLOTTE LAURA  --FRANCES  \n[13] LAURA  --ELEANOR   LAURA  --RUTH      THERESA--BRENDA    THERESA--CHARLOTTE\n[17] THERESA--FRANCES   THERESA--ELEANOR   THERESA--RUTH      THERESA--PEARL    \n[21] THERESA--NORA      THERESA--VERNE     THERESA--SYLVIA    BRENDA --CHARLOTTE\n[25] BRENDA --FRANCES   BRENDA --ELEANOR   BRENDA --RUTH      FRANCES--ELEANOR  \n[29] ELEANOR--RUTH      RUTH   --VERNE     RUTH   --SYLVIA    VERNE  --SYLVIA   \n+ ... omitted several edges\n\n\n\n\n\n\n\n\n\n\n\n\n\n6.4.3 Model-based Binary Projections\nThe global threshold method is very simple but in many cases leads to undesirable structural features. More sophisticated tools work with statistical models in the background which determine if an edge weight differs enough from the expected value of an underlying null model. If so, the edge is kept in the binary projection. Many of such models are implemented in the backbone package.\n\nlibrary(backbone)\n\nThe idea behind all of the models is always the same:\n\nCreate the weighted projection of interest, e.g. B &lt;- A%*%t(A)\nGenerate random two-mode networks according to a given model.\nCompare if the values B[i,j] differ significantly from the distribution of values in the random projections.\n\nThe only difference in all models is the construction of the random two-mode networks which follow different rules:\n\nFixed Degree Sequence Model fdsm(): Create random two-mode networks with the same row and column sums as A.\nFixed Column Model fixedcol(): Create random two-mode networks with the same column sums as A.\nFixed Row Model fixedrow(): Create random two-mode networks with the same row sums as A.\nFixed Fill Model fixedfill(): Create random two-mode networks with the same number of ones as A.\nStochastic Degree Sequence Model sdsm(): Create random two-mode networks with approximately the same row and column sums as A.\n\nBefore we move to an actual use case, you may ask: So which model is the right one for me? That is actually quite a tricky question. There is some guidance available but in general you can follow these rough guidelines:\n\nUse the model that fits you empirical setting or a known link formation process. If that link formation process dictates that row sums are fixed but column sums not, then choose fixedow().\nUse fdsm() if your network is small enough. Sampling from the FDSM is quite expensive.\nUse the sdsm() for large networks.\n\nGiven that there is never a “ground-truth” binary projection, any choice of model is fine as long as it is motivated substantively and not merely because it fits the papers narrative best.\nTo illustrate the model fitting, we use a bill cosponsorship of the Senate 2015. A link between a senator and a bill exists, if they sponsored it. We are no interested in how the binary projection of Senators looks like.\n\ndata(\"cosponsor\")\ncosponsor\n\nIGRAPH 6eddec8 UN-B 3984 26392 -- \n+ attr: name (v/c), type (v/l), party (v/c)\n+ edges from 6eddec8 (vertex names):\n [1] 115s1   --Enzi, Michael B.    115s10  --Cardin, Benjamin L.\n [3] 115s10  --Wicker, Roger F.    115s100 --Alexander, Lamar   \n [5] 115s1000--Franken, Al         115s1000--Murray, Patty      \n [7] 115s1000--Brown, Sherrod      115s1000--Warren, Elizabeth  \n [9] 115s1000--Markey, Edward J.   115s1001--Crapo, Mike        \n[11] 115s1001--Blumenthal, Richard 115s1001--Murphy, Christopher\n[13] 115s1001--Cassidy, Bill       115s1001--Alexander, Lamar   \n[15] 115s1001--Bennet, Michael F.  115s1002--Moran, Jerry       \n+ ... omitted several edges\n\n\nGiven that the network is fairly large, we will use the SDSM. Note that all models create the projection for the mode where type == FALSE. If you want to project on the TRUE mode, you need to invert the type attribute.\n\nsenators &lt;- sdsm(cosponsor, alpha = 0.05, signed = FALSE)\nsenators\n\nIGRAPH 70e5489 UN-- 110 1591 -- \n+ attr: name (v/c), party (v/c)\n+ edges from 70e5489 (vertex names):\n [1] Enzi, Michael B.--Wicker, Roger F. Enzi, Michael B.--Alexander, Lamar\n [3] Enzi, Michael B.--Crapo, Mike      Enzi, Michael B.--Moran, Jerry    \n [5] Enzi, Michael B.--Scott, Tim       Enzi, Michael B.--Daines, Steve   \n [7] Enzi, Michael B.--Perdue, David    Enzi, Michael B.--Blunt, Roy      \n [9] Enzi, Michael B.--Inhofe, James M. Enzi, Michael B.--Barrasso, John  \n[11] Enzi, Michael B.--Fischer, Deb     Enzi, Michael B.--Ernst, Joni     \n[13] Enzi, Michael B.--Rounds, Mike     Enzi, Michael B.--Kennedy, John   \n[15] Enzi, Michael B.--Flake, Jeff      Enzi, Michael B.--Hoeven, John    \n+ ... omitted several edges\n\n\nFor signed = FALSE, a one-tailed test is performed for each edge with a non-zero weight. It yields a projection that preserves edges whose weights are significantly stronger than expected in the null model.\nWhen signed = TRUE, a two-tailed test is performed for every pair of nodes. It yields a backbone that contains positive edges for edges whose weights are significantly stronger, and negative edges for edges whose weights are significantly weaker, than expected in the chosen null model. The projections thus becomes a signed network (see Chapter 7).\nThe figure below shows the not so surprising result that Democrats and Republicans do not tend to significantly cosponsor the same bills.",
    "crumbs": [
      "Descriptive Network Analysis",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Two-Mode Networks</span>"
    ]
  },
  {
    "objectID": "descriptive/two-mode-networks.html#notable-packages",
    "href": "descriptive/two-mode-networks.html#notable-packages",
    "title": "6  Two-Mode Networks",
    "section": "6.5 Notable Packages",
    "text": "6.5 Notable Packages\n\nincidentally to create random two-mode networks with given structural features",
    "crumbs": [
      "Descriptive Network Analysis",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Two-Mode Networks</span>"
    ]
  },
  {
    "objectID": "descriptive/two-mode-networks.html#scientific-reading",
    "href": "descriptive/two-mode-networks.html#scientific-reading",
    "title": "6  Two-Mode Networks",
    "section": "6.6 Scientific Reading",
    "text": "6.6 Scientific Reading\nFaust, K. (1997). Centrality in affiliation networks. Social networks, 19(2), 157-191.\nEverett, M. G., & Borgatti, S. P. (2013). The dual-projection approach for two-mode networks. Social networks, 35(2), 204-210.\nOpsahl, T. (2013). Triadic closure in two-mode networks: Redefining the global and local clustering coefficients. Social networks, 35(2), 159-167.\nNeal, Z. P. (2014). The backbone of bipartite projections: Inferring relationships from co-authorship, co-sponsorship, co-attendance, and other co-behaviors. Social Networks, 39, 84-97.\nNeal, Z. P., Domagalski, R., and Sagan, B. (2021). Comparing Alternatives to the Fixed Degree Sequence Model for Extracting the Backbone of Bipartite Projections. Scientific Reports, 11, 23929.",
    "crumbs": [
      "Descriptive Network Analysis",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Two-Mode Networks</span>"
    ]
  },
  {
    "objectID": "descriptive/signed-networks.html",
    "href": "descriptive/signed-networks.html",
    "title": "7  Signed Networks",
    "section": "",
    "text": "7.1 Packages Needed for this Chapter\nTraditional SNA usually deals with relations among entities (e.g. people) that are positive, including “friendship”, “advice seeking”, etc. Most network analytic tools are devised under this premise, be that centrality indices, clustering tools and so forth. But of course not all occurring relations are positive. People can be friends but also foes.\nThis gives rise to signed networks. These networks are usually composed of both, positive and negative, ties measured among a set of entities. Traditional network analytic tools are not applicable to such networks without adapting for negative ties.\nThe signnet package brings together methods that have been developed to analyse signed networks. This includes\nlibrary(igraph)\nlibrary(signnet)\nlibrary(networkdata)",
    "crumbs": [
      "Descriptive Network Analysis",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Signed Networks</span>"
    ]
  },
  {
    "objectID": "descriptive/signed-networks.html#data-structures-for-signed-networks",
    "href": "descriptive/signed-networks.html#data-structures-for-signed-networks",
    "title": "7  Signed Networks",
    "section": "7.2 Data structures for signed networks",
    "text": "7.2 Data structures for signed networks\nThe foundation of signnet is provided by igraph. All functions in the package assume that an igraph object is a signed network if it has an edge attribute “sign” with values 1 (positive) or -1 (negative).\n\ng &lt;- graph.full(5, directed = FALSE, loops = FALSE)\n\nWarning: `graph.full()` was deprecated in igraph 2.1.0.\nℹ Please use `make_full_graph()` instead.\n\nE(g)$sign &lt;- 1\ng\n\nIGRAPH 31180b2 U--- 5 10 -- Full graph\n+ attr: name (g/c), loops (g/l), sign (e/n)\n+ edges from 31180b2:\n [1] 1--2 1--3 1--4 1--5 2--3 2--4 2--5 3--4 3--5 4--5\n\n\nAll methods throw an error if the sign attribute is missing or contains other values than -1 and 1.\nMatrices associated with a signed network follow the igraph naming scheme. The signed adjacency matrix can be obtained with as_adj_signed().\n\ndata(\"tribes\")\nas_adj_signed(tribes)[1:5,1:5]\n\n      Gavev Kotun Ove Alika Nagam\nGavev     0     1  -1    -1    -1\nKotun     1     0  -1     0    -1\nOve      -1    -1   0     1     0\nAlika    -1     0   1     0     0\nNagam    -1    -1   0     0     0\n\n\nThe signed Laplacian matrix is obtained by laplacian_matrix_signed().\n\nlaplacian_matrix_signed(tribes)[1:5,1:5]\n\n      Gavev Kotun Ove Alika Nagam\nGavev     8    -1   1     1     1\nKotun    -1     8   1     0     1\nOve       1     1   6    -1     0\nAlika     1     0  -1     3     0\nNagam     1     1   0     0     7\n\n\nThe signnet package includes two well known example datasets.\nThe “tribes” dataset is a signed social network of tribes of the Gahuku–Gama alliance structure of the Eastern Central Highlands of New Guinea. The network contains sixteen tribes connected by friendship (“rova”) and enmity (“hina”).\nThe “cowList” dataset contains a list of 52 signed networks of inter-state relations over time (1946-1999). Two countries are connected by a positive tie if they form an alliance or have a peace treaty. A negative tie exists between countries who are at war or in other kinds of conflicts. The dataset is derrived from the correlates of war.",
    "crumbs": [
      "Descriptive Network Analysis",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Signed Networks</span>"
    ]
  },
  {
    "objectID": "descriptive/signed-networks.html#structural-balance",
    "href": "descriptive/signed-networks.html#structural-balance",
    "title": "7  Signed Networks",
    "section": "7.3 Structural Balance",
    "text": "7.3 Structural Balance\nThe principles underlying structural balance are based on a theory in social psychology dating back to the work of Heider in the 1940s, which was generalized and extended to graphs by Cartwright and Harary in the 1950s. In its simplest form, it is defined via triangles. A triangle is balanced if all ties are positive (“the friend of a friend is a friend”) or only one tie is positive (“the enemy of my enemy is my friend”). The remaining configurations are said to be unbalanced.\n\nA network is balanced if i.a., it can be partitioned into two vertex subsets, such that intra-group edges are all positive and inter-group edges are all negative.\nA (random) balanced network can be obtained with the function sample_islands_signed() which is pretty much the same as sample_islands() from the igraph package.\n\ng &lt;- sample_islands_signed(islands.n = 2,islands.size = 10,\n                           islands.pin = 0.8,n.inter = 5)\n\n(The function ggsigned() can be used to visualize signed networks. Note that this requires the package ggraph to be installed.) Increasing islands.n leads to “clusterable” networks as defined by Davis.\nA balanced network only contains balanced triangles. This can be verified with count_signed_triangles().\n\ncount_signed_triangles(g)\n\n+++ ++- +-- --- \n 87   0   4   0 \n\n\nNote the absence of ++- and --- triangles.\nTo list all triangles use signed_triangles().\n\nhead(signed_triangles(g))\n\n     V1 V2 V3 P\n[1,]  5  1 10 3\n[2,]  5  1  7 3\n[3,]  5  3  4 3\n[4,]  5  3 10 3\n[5,]  5  3  7 3\n[6,]  5  3  8 3\n\n\nThe column P indicated the number of positive ties in the triangle. A value of 3 indicates that the triangle is “+++”.",
    "crumbs": [
      "Descriptive Network Analysis",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Signed Networks</span>"
    ]
  },
  {
    "objectID": "descriptive/signed-networks.html#balancedness",
    "href": "descriptive/signed-networks.html#balancedness",
    "title": "7  Signed Networks",
    "section": "7.4 Balancedness",
    "text": "7.4 Balancedness\nDetermining if a network is balanced or not is easy, but measuring a degree of balancedness (i.e. how close is a network to be balanced?) is not. The package, so far, implements three methods to calculate balance scores. All are defined such that a value of one indicates perfect balance and zero perfect unbalance. Though for intermediate networks, results may vary significantly. Check the paper by Samin Aref (and his other work) for more details.\n\nbalance_score(g, method = \"triangles\")\n\n[1] 1\n\nbalance_score(g, method = \"walk\")\n\n[1] 1\n\nbalance_score(g, method = \"frustration\")\n\n[1] 0.8648649\n\n\n“triangles” returns the fraction of balanced triangles.\n“walk” is based on eigenvalues of the signed and underlying unsigned network. Check the paper by Estrada for details.\n“frustration” assumes that the network can be partitioned into two groups, where intra group edges are positive and inter group edges are negative. The index is defined as the sum of intra group negative and inter group positive edges. Note that the problem is NP complete and only an upper bound is returned (based on simulated annealing). The function frustration_exact() implements an integer program to solve the exact optimization problem. More details can be found in the work of Aref.\nThere disagreement for non-balanced networks can be seen with the included “tribes” dataset.\n\ndata(\"tribes\")\nbalance_score(tribes, method = \"triangles\")\n\n[1] 0.8676471\n\nbalance_score(tribes, method = \"walk\")\n\n[1] 0.3575761\n\nbalance_score(tribes, method = \"frustration\")\n\n[1] 0.7586207",
    "crumbs": [
      "Descriptive Network Analysis",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Signed Networks</span>"
    ]
  },
  {
    "objectID": "descriptive/signed-networks.html#triangles",
    "href": "descriptive/signed-networks.html#triangles",
    "title": "7  Signed Networks",
    "section": "7.5 Triangles",
    "text": "7.5 Triangles\nThe function triad_census_signed() calculates the signed triad census of a directed signed network. While the unsigned triad census has only 16 possible outcomes, there are 138 non-isomorphic signed triads, shown below.\n\nThe naming scheme is “xxx-yyyyyy” where “xxx” corresponds to the name of the respective unsigned triad and “yyyyyy” is a string of “0”, “N”, “P”, describing the type of ties present. So “300-NNNNNN” is a triad with all ties present and all ties are negative.",
    "crumbs": [
      "Descriptive Network Analysis",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Signed Networks</span>"
    ]
  },
  {
    "objectID": "descriptive/signed-networks.html#traditional-blockmodeling",
    "href": "descriptive/signed-networks.html#traditional-blockmodeling",
    "title": "7  Signed Networks",
    "section": "7.6 Traditional Blockmodeling",
    "text": "7.6 Traditional Blockmodeling\nIn signed blockmodeling, the goal is to determine k blocks of nodes such that all intra-block edges are positive and inter-block edges are negative. In the example below, we construct a network with a perfect block structure with sample_islands_signed(). The network consists of 10 blocks with 10 vertices each, where each block has a density of 1 (of positive edges). The function signed_blockmodel() is used to construct the blockmodel. The parameter k is the number of desired blocks. alpha is a trade-off parameter. The function minimizes \\(P(C)=\\alpha N+(1-\\alpha)P\\), where \\(N\\) is the total number of negative ties within blocks and \\(P\\) be the total number of positive ties between blocks.\n\ng &lt;- sample_islands_signed(10,10,1,20)\nclu &lt;- signed_blockmodel(g,k = 10,alpha = 0.5)\ntable(clu$membership)\n\n\n 1  2  3  4  5  6  7  8  9 10 \n10 10 10 10 10 10 10 10 10 10 \n\nclu$criterion\n\n[1] 0\n\n\nThe function returns a list with two entries. The block membership of nodes and the value of \\(P(C)\\).\nThe function ggblock() can be used to plot the outcome of the blockmodel (ggplot2 is required).\n\nggblock(g,clu$membership,show_blocks = TRUE)\n\n\n\n\n\n\n\n\nIf the parameter annealing is set to TRUE, simulated annealing is used in the optimization step. This generally leads to better results but longer runtimes.\n\ndata(\"tribes\")\nset.seed(44) #for reproducibility\n\nsigned_blockmodel(tribes,k = 3,alpha=0.5,annealing = TRUE)\n\n$membership\n [1] 1 1 2 2 3 2 2 2 3 3 2 2 3 3 1 1\n\n$criterion\n[1] 2\n\nsigned_blockmodel(tribes,k = 3,alpha=0.5,annealing = FALSE)\n\n$membership\n [1] 1 1 2 2 3 2 2 2 3 3 2 2 3 3 1 1\n\n$criterion\n[1] 2",
    "crumbs": [
      "Descriptive Network Analysis",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Signed Networks</span>"
    ]
  },
  {
    "objectID": "descriptive/signed-networks.html#generalized-blockmodeling",
    "href": "descriptive/signed-networks.html#generalized-blockmodeling",
    "title": "7  Signed Networks",
    "section": "7.7 Generalized Blockmodeling",
    "text": "7.7 Generalized Blockmodeling\nThe function signed_blockmodel() is only able to provide a blockmodel where the diagonal blocks are positive and off-diagonal blocks are negative. The function signed_blockmodel_general() can be used to specify different block structures. In the below example, we construct a network that contains three blocks. Two have positive and one has negative intra-group ties. The inter-group edges are negative between group one and two, and one and three. Between group two and three, all edges are positive.\n\ng1 &lt;- g2 &lt;- g3 &lt;- graph.full(5)\n\nV(g1)$name &lt;- as.character(1:5)\nV(g2)$name &lt;- as.character(6:10)\nV(g3)$name &lt;- as.character(11:15)\n\ng &lt;- Reduce(\"%u%\",list(g1,g2,g3))\nE(g)$sign &lt;- 1\nE(g)$sign[1:10] &lt;- -1\ng &lt;- add.edges(g,c(rbind(1:5,6:10)),attr = list(sign=-1))\n\nWarning: `add.edges()` was deprecated in igraph 2.0.0.\nℹ Please use `add_edges()` instead.\n\ng &lt;- add.edges(g,c(rbind(1:5,11:15)),attr = list(sign=-1))\ng &lt;- add.edges(g,c(rbind(11:15,6:10)),attr = list(sign=1))\n\nThe parameter blockmat is used to specify the desired block structure.\n\nset.seed(424)\nblockmat &lt;- matrix(c(1,-1,-1,-1,1,1,-1,1,-1),3,3,byrow = TRUE)\nblockmat\n\n     [,1] [,2] [,3]\n[1,]    1   -1   -1\n[2,]   -1    1    1\n[3,]   -1    1   -1\n\ngeneral &lt;- signed_blockmodel_general(g,blockmat,alpha = 0.5)\ntraditional &lt;- signed_blockmodel(g,k = 3,alpha = 0.5,annealing = TRUE)\n\nc(general$criterion,traditional$criterion)\n\n[1] 0 6",
    "crumbs": [
      "Descriptive Network Analysis",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Signed Networks</span>"
    ]
  },
  {
    "objectID": "descriptive/signed-networks.html#centrality",
    "href": "descriptive/signed-networks.html#centrality",
    "title": "7  Signed Networks",
    "section": "7.8 Centrality",
    "text": "7.8 Centrality\nThere exist dozens of indices for networks with positive ties, but for signed networks they are rather scarce. The package implements three indices so far. Versions of degree and eigenvector centrality, and PN centrality by Everett & Borgatti.\nDegree centrality can be calculated in four different ways with degree_signed(), specified by the type parameter:\n\ntype=\"pos\" count only positive neighbors\ntype=\"neg\" count only negative neighbors\ntype=\"ratio\" positive neighbors/(positive neighbors+negative neighbors)\ntype=\"net\" positive neighbors-negative neighbors\n\nThe mode parameter can be used to get “in” and “out” versions for directed networks.\nThe PN index is very similar to Katz status and Hubbell’s measure for networks with only positive ties. The technical details can be found in the paper by Everett & Borgatti.\nThe below example illustrates all indices with a network where signed degree can not distinguish vertices.\n\nA &lt;- matrix(c(0,  1,  0,  1,  0,  0,  0, -1, -1,  0,  \n               1,  0,  1, -1,  1, -1, -1,  0,  0,  0,  \n               0,  1,  0,  1, -1,  0,  0,  0, -1,  0,  \n               1, -1,  1,  0,  1, -1, -1,  0,  0,  0,  \n               0,  1, -1,  1,  0,  1,  0, -1,  0, -1,  \n               0, -1,  0, -1,  1,  0,  1,  0,  1, -1,  \n               0, -1,  0, -1,  0,  1,  0,  1, -1,  1,  \n              -1,  0,  0,  0, -1,  0,  1,  0,  1,  0,  \n              -1,  0, -1,  0,  0,  1, -1,  1,  0,  1,  \n               0,  0,  0,  0, -1, -1,  1,  0,  1,  0),10,10)\n\ng &lt;- graph_from_adjacency_matrix(A,\"undirected\",weighted = \"sign\")\n\ndegree_signed(g,type = \"ratio\")\n\n [1] 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5\n\neigen_centrality_signed(g)\n\n [1] 0.62214960 1.00000000 0.74518850 1.00000000 0.89990041 0.64289592\n [7] 0.35828159 0.37471921 0.28087411 0.07834568\n\npn_index(g)\n\n [1] 0.9009747 0.8613482 0.9076997 0.8613482 0.8410658 0.8496558 0.8617321\n [8] 0.9015909 0.8509848 0.9072930\n\n\nNote that PN centrality and eigenvector centrality differ significantly for this network.\n\ncor(eigen_centrality_signed(g),pn_index(g),method = \"kendall\")\n\n[1] -0.2444444\n\n\n\n7.8.1 A note on eigenvector centrality\nThe adjacency matrix of a signed network may not have a dominant eigenvalue. This means it is not clear which eigenvector should be used. In addition it is possible for the adjacency matrix to have repeated eigenvalues and hence multiple linearly independent eigenvectors. In this case certain centralities can be arbitrarily assigned. The eigen_centrality_signed() function returns an error if this is the case.\n\nA &lt;- matrix(c( 0,  1,  1, -1,  0,  0, -1,  0,  0, \n               1,  0,  1,  0, -1,  0,  0, -1,  0, \n               1,  1,  0,  0,  0, -1,  0,  0, -1, \n              -1,  0,  0,  0,  1,  1, -1,  0,  0, \n               0, -1,  0,  1,  0,  1,  0, -1,  0, \n               0,  0, -1,  1,  1,  0,  0,  0, -1, \n              -1,  0,  0, -1,  0,  0,  0,  1,  1, \n               0, -1,  0,  0, -1,  0,  1,  0,  1, \n               0,  0, -1,  0,  0, -1,  1,  1, 0), 9, 9)\n\ng &lt;- igraph::graph_from_adjacency_matrix(A,\"undirected\",weighted = \"sign\")\neigen_centrality_signed(g)\n\nError in eigen_centrality_signed(g): no dominant eigenvalue exists",
    "crumbs": [
      "Descriptive Network Analysis",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Signed Networks</span>"
    ]
  },
  {
    "objectID": "descriptive/signed-networks.html#scientific-reading",
    "href": "descriptive/signed-networks.html#scientific-reading",
    "title": "7  Signed Networks",
    "section": "7.9 Scientific Reading",
    "text": "7.9 Scientific Reading\nEverett, Martin G., and Stephen P. Borgatti. 2014. “Networks Containing Negative Ties.” Social Networks 38: 111–20.\nBonacich, Phillip, and Paulette Lloyd. 2004. “Calculating Status with Negative Relations.” Social Networks 26 (4): 331–38.\nDoreian, Patrick, and Andrej Mrvar. 1996. “A Partitioning Approach to Structural Balance.” Social Networks 18 (2): 149–68.\nDoreian, Patrick, and Andrej Mrvar. 2009. “Partitioning Signed Social Networks.” Social Networks 31 (1): 1–11.\nDoreian, Patrick, and Andrej Mrvar. 2015. “Structural Balance and Signed International Relations.” Journal of Social Structure 16: 1.\nHeider, Fritz. 1946. “Attitudes and Cognitive Organization.” The Journal of Psychology 21 (1): 107–12.\nCartwright, Dorwin, and Frank Harary. 1956. “Structural Balance: A Generalization of Heider’s Theory.” Psychological Review 63 (5): 277.\nDavis, James A. 1967. “Clustering and Structural Balance in Graphs.” Human Relations 20 (2): 181–87.\nAref, Samin, and Mark C. Wilson. 2018. “Measuring Partial Balance in Signed Networks.” Journal of Complex Networks 6 (4): 566–95.\nEstrada, Ernesto. 2019. “Rethinking Structural Balance in Signed Social Networks.” Discrete Applied Mathematics",
    "crumbs": [
      "Descriptive Network Analysis",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Signed Networks</span>"
    ]
  },
  {
    "objectID": "descriptive/ego-networks.html",
    "href": "descriptive/ego-networks.html",
    "title": "8  Ego Networks",
    "section": "",
    "text": "8.1 Packages needed for this Chapter",
    "crumbs": [
      "Descriptive Network Analysis",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Ego Networks</span>"
    ]
  },
  {
    "objectID": "visualization/introduction.html",
    "href": "visualization/introduction.html",
    "title": "9  Introduction",
    "section": "",
    "text": "9.1 Required libraries\nMost network analytic tasks are fairly straightforward to do in R. But when it comes to visualizing networks, R may lack behind some standalone software tools. Not because it is not possible to produce nice figures, but rather because it requires some time to obtain pleasing results. Just take a look at the default output when plotting a network with the plot() function.\nIt is definitely possible to produce nice figures with the igraph package (Check out this wonderful tutorial), yet it may take some time to familiarize yourself with the syntax. Additionally, most of the layout algorithms of igraph are non-deterministic. This means that running the same plot call twice may produce different results.\nIn this part, you will learn the basics of ggraph, the “ggplot2 of networks”, together with the graphlayouts package, which introduces additional useful layout algorithms to R. Arguably, using ggraph is not really easier than igraph. But once the underlying principle of the grammar of graphics is understood, you’ll see that it is actually quite intuitive to work with.\nTo run all the code in this tutorial, you need to install and load several packages.\ninstall.packages(c(\"igraph\", \"graphlayouts\", \"ggraph\", \"ggforce\"))\ndevtools::install_github(\"schochastics/networkdata\")\nMake sure you have at least the version given below. Some of the examples may not be backward compatible.\npackageVersion(\"igraph\")\n\n[1] '2.1.4.9041'\n\npackageVersion(\"graphlayouts\")\n\n[1] '1.2.1'\n\npackageVersion(\"ggraph\")\n\n[1] '2.2.1'\n\npackageVersion(\"networkdata\")\n\n[1] '0.2.1'\n\npackageVersion(\"ggforce\")\n\n[1] '0.4.2'\nigraph is mostly used for its data structures and graphlayouts and ggraph for visualizations. The networkdata package contains a huge amount of example network data that always comes in handy for learning new visualization techniques.\nlibrary(igraph)\nlibrary(ggraph)\n\nLoading required package: ggplot2\n\nlibrary(graphlayouts)\nlibrary(ggforce)",
    "crumbs": [
      "Network Visualization",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "visualization/ggraph-basics.html",
    "href": "visualization/ggraph-basics.html",
    "title": "10  Basics of ggraph",
    "section": "",
    "text": "10.1 Packages Needed for this Chapter\nlibrary(igraph)\nlibrary(ggraph)\nlibrary(graphlayouts)\nlibrary(networkdata)\ndata(\"got\")\ngotS1 &lt;- got[[1]]\nWe add some more node attributes to the GoT network that can be used for visualization purposes.\n## define a custom color palette\ngot_palette &lt;- c(\n    \"#1A5878\", \"#C44237\", \"#AD8941\", \"#E99093\",\n    \"#50594B\", \"#8968CD\", \"#9ACD32\"\n)\n\n## compute a clustering for node colors\nV(gotS1)$clu &lt;- as.character(membership(cluster_louvain(gotS1)))\n\n## compute degree as node size\nV(gotS1)$size &lt;- degree(gotS1)\nTo champion ggraph you need to understand the basics of, or at least develop a feeling for, the grammar of graphics. Instead of explaining the grammar, let us directly jump into some code and work through it one line at a time.\nggraph(gotS1, layout = \"stress\") +\n  geom_edge_link0(aes(edge_linewidth = weight), edge_colour = \"grey66\") +\n  geom_node_point(aes(fill = clu, size = size), shape = 21) +\n  geom_node_text(aes(filter = size &gt;= 26, label = name), family = \"serif\") +\n  scale_fill_manual(values = got_palette) +\n  scale_edge_width(range = c(0.2, 3)) +\n  scale_size(range = c(1, 6)) +\n  theme_graph() +\n  theme(legend.position = \"none\")\nggraph works with layers. Each layer adds a new feature to the plot and thus builds the figure step-by-step. We will work through each of the layers separately in the following sections.",
    "crumbs": [
      "Network Visualization",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Basics of ggraph</span>"
    ]
  },
  {
    "objectID": "visualization/ggraph-basics.html#layout",
    "href": "visualization/ggraph-basics.html#layout",
    "title": "10  Basics of ggraph",
    "section": "10.2 Layout",
    "text": "10.2 Layout\nggraph(gotS1, layout = \"stress\")\nThe first step is to compute a layout. The layout parameter specifies the algorithm to use. The “stress” layout is part of the graphlayouts package and is always a safe choice since it is deterministic and produces nice layouts for almost any graph. I would recommend to use it as your default choice. Other algorithms for, e.g., concentric layouts and clustered networks are described further down in this tutorial. For the sake of completeness, here is a list of layout algorithms of igraph.\nc(\n  \"layout_with_dh\", \"layout_with_drl\", \"layout_with_fr\",\n  \"layout_with_gem\", \"layout_with_graphopt\", \"layout_with_kk\",\n  \"layout_with_lgl\", \"layout_with_mds\", \"layout_with_sugiyama\",\n  \"layout_as_bipartite\", \"layout_as_star\", \"layout_as_tree\"\n)\nTo use them, you just need the last part of the name.\nggraph(gotS1, layout = \"dh\") +\n  ...\nNote that there technically is no right or wrong choice. All layout algorithms are in a sense arbitrary since we can choose x and y coordinates freely (compare this to ordinary data!). It is all mostly about aesthetics.\nYou can also precompute the layout with the create_layout() function. This makes sense in cases where the calculation of the layout takes very long and you want to play around with other visual aspects.\ngotS1_layout &lt;- create_layout(gotS1 = \"stress\")\n\nggraph(gotS1_layout) +\n  ...",
    "crumbs": [
      "Network Visualization",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Basics of ggraph</span>"
    ]
  },
  {
    "objectID": "visualization/ggraph-basics.html#edges",
    "href": "visualization/ggraph-basics.html#edges",
    "title": "10  Basics of ggraph",
    "section": "10.3 Edges",
    "text": "10.3 Edges\ngeom_edge_link0(aes(width = weight), edge_colour = \"grey66\")\nThe second layer specifies how to draw the edges. Edges can be drawn in many different ways as the list below shows.\nc(\n  \"geom_edge_arc\", \"geom_edge_arc0\", \"geom_edge_arc2\", \"geom_edge_density\",\n  \"geom_edge_diagonal\", \"geom_edge_diagonal0\", \"geom_edge_diagonal2\",\n  \"geom_edge_elbow\", \"geom_edge_elbow0\", \"geom_edge_elbow2\", \"geom_edge_fan\",\n  \"geom_edge_fan0\", \"geom_edge_fan2\", \"geom_edge_hive\", \"geom_edge_hive0\",\n  \"geom_edge_hive2\", \"geom_edge_link\", \"geom_edge_link0\", \"geom_edge_link2\",\n  \"geom_edge_loop\", \"geom_edge_loop0\"\n)\nYou can do a lot of fancy things with these geoms but for a standard network plot, you should almost always stick with geom_edge_link0 since it simply draws a straight line between the endpoints. Some tools draw curved edges by default. While this may add some artistic value, it reduces readability. Always go with straight lines! If your network has multiple edges between two nodes, then you can switch to geom_edge_parallel().\nIn case you are wondering what the “0” stands for: The standard geom_edge_link() draws 100 dots on each edge compared to only two dots (the endpoints) in geom_edge_link0(). This is done to allow, e.g., gradients along the edge.\n\nggraph(gotS1, layout = \"stress\") +\n  geom_edge_link(aes(alpha = after_stat(index)), edge_colour = \"black\") +\n  geom_node_point(aes(fill = clu, size = size), shape = 21) +\n  scale_fill_manual(values = got_palette) +\n  scale_edge_width_continuous(range = c(0.2, 3)) +\n  scale_size_continuous(range = c(1, 6)) +\n  theme_graph() +\n  theme(legend.position = \"none\")\n\n\n\n\n\n\n\n\nThe drawback of using geom_edge_link() is that the time to render the plot increases and so does the size of the file if you export the plot (example) Typically, you do not need gradients along an edge. Hence, geom_edge_link0() should be your default choice to draw edges.\nWithin geom_edge_link0, you can specify the appearance of the edge, either by mapping edge attributes to aesthetics or setting them globally for the graph. Mapping attributes to aesthetics is done within aes(). In the example, we map the edge width to the edge attribute “weight”. ggraph then automatically scales the edge width according to the attribute. The colour of all edges is globally set to “grey66”.\nThe following aesthetics can be used within geom_edge_link0 either within aes() or globally:\n\nedge_colour (colour of the edge)\nedge_linewidth (width of the edge)\nedge_linetype (linetype of the edge, defaults to “solid”)\nedge_alpha (opacity; a value between 0 and 1)\n\nggraph does not automatically draw arrows if your graph is directed. You need to do this manually using the arrow parameter.\ngeom_edge_link0(aes(...), ...,\n  arrow = arrow(\n    angle = 30, length = unit(0.15, \"inches\"),\n    ends = \"last\", type = \"closed\"\n  )\n)\nThe default arrowhead type is “open”, yet “closed” usually has a nicer appearance.",
    "crumbs": [
      "Network Visualization",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Basics of ggraph</span>"
    ]
  },
  {
    "objectID": "visualization/ggraph-basics.html#nodes",
    "href": "visualization/ggraph-basics.html#nodes",
    "title": "10  Basics of ggraph",
    "section": "10.4 Nodes",
    "text": "10.4 Nodes\ngeom_node_point(aes(fill = clu, size = size), shape = 21) +\n  geom_node_text(aes(filter = size &gt;= 26, label = name), family = \"serif\")\nOn top of the edge layer, we draw the node layer. Always draw the node layer above the edge layer. Otherwise, edges will be visible on top of nodes. There are slightly less geoms available for nodes.\nc(\n  \"geom_node_arc_bar\", \"geom_node_circle\", \"geom_node_label\",\n  \"geom_node_point\", \"geom_node_text\", \"geom_node_tile\", \"geom_node_treemap\"\n)\nThe most important ones here are geom_node_point() to draw nodes as simple geometric objects (circles, squares,…) and geom_node_text() to add node labels. You can also use geom_node_label(), but this draws labels within a box.\nThe mapping of node attributes to aesthetics is similar to edge attributes. In the example code, we map the fill attribute of the node shape to the “clu” attribute, which holds the result of a clustering, and the size of the nodes to the attribute “size”. The shape of the node is globally set to 21.\nThe figure below shows all possible shapes that can be used for the nodes.\n\nPersonally, I prefer “21” since it draws a border around the nodes. If you prefer another shape, say “19”, you have to be aware of several things. To change the color of shapes 1-20, you need to use the colour parameter. For shapes 21-25 you need to use fill. The colour parameter only controls the border for these cases.\nThe following aesthetics can be used within geom_node_point() either within aes() or globally:\n\nalpha (opacity; a value between 0 and 1)\ncolour (colour of shapes 0-20 and border colour for 21-25)\nfill (fill colour for shape 21-25)\nshape (node shape; a value between 0 and 25)\nsize (size of node)\nstroke (size of node border)\n\nFor geom_node_text(), there are a lot more options available, but the most important once are:\n\nlabel (attribute to be displayed as node label)\ncolour (text colour)\nfamily (font to be used)\nsize (font size)\n\nNote that we also used a filter within aes() of geom_node_text(). The filter parameter allows you to specify a rule for when to apply the aesthetic mappings. The most frequent use case is for node labels (but can also be used for edges or nodes). In the example, we only display the node label if the size attribute is larger than 26.",
    "crumbs": [
      "Network Visualization",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Basics of ggraph</span>"
    ]
  },
  {
    "objectID": "visualization/ggraph-basics.html#scales",
    "href": "visualization/ggraph-basics.html#scales",
    "title": "10  Basics of ggraph",
    "section": "10.5 Scales",
    "text": "10.5 Scales\nscale_fill_manual(values = got_palette) +\n  scale_edge_width_continuous(range = c(0.2, 3)) +\n  scale_size_continuous(range = c(1, 6))\nThe scale_* functions are used to control aesthetics that are mapped within aes(). You do not necessarily need to set them, since ggraph can take care of it automatically.\n\nggraph(gotS1, layout = \"stress\") +\n  geom_edge_link0(aes(edge_linewidth = weight), edge_colour = \"grey66\") +\n  geom_node_point(aes(fill = clu, size = size), shape = 21) +\n  geom_node_text(aes(filter = size &gt;= 26, label = name), family = \"serif\") +\n  theme_graph() +\n  theme(legend.position = \"none\")\n\nWarning: The `trans` argument of `continuous_scale()` is deprecated as of ggplot2 3.5.0.\nℹ Please use the `transform` argument instead.\n\n\n\n\n\n\n\n\n\nWhile the node fill and size seem reasonable, the edges are a little too thick. In general, it is always a good idea to add a scale_* for each aesthetic within aes().\nWhat kind of scale_* function you need depends on the aesthetic and on the type of attribute you are mapping. Generally, scale functions are structured like this:\nscale_&lt;aes&gt;_&lt;variable type&gt;().\nThe “aes” part is easy. Just us the type you specified within aes(). For edges, however, you have to prepend edge_. The “variable type” part depends on which scale the attribute is on. Before we continue, it may be a good idea to briefly discuss what aesthetics make sense for which variable type.\n\n\n\n\n\n\n\n\naesthetic\nvariable type\nnotes\n\n\n\n\nnode size\ncontinuous\n\n\n\nedge width\ncontinuous\n\n\n\nnode colour/fill\ncategorical/continuous\nuse a gradient for continuous variables\n\n\nedge colour\ncontinuous\ncategorical only if there are different types of edges\n\n\nnode shape\ncategorical\nonly if there are a few categories (1-5). Colour should be the preferred choice\n\n\nedge linetype\ncategorical\nonly if there are a few categories (1-5). Colour should be the preferred choice\n\n\nnode/edge alpha\ncontinuous\n\n\n\n\nThe easiest to use scales are those for continuous variables mapped to edge width and node size (also the alpha value, which is not used here). While there are several parameters within scale_edge_width_continuous() and scale_size_continuous(), the most important one is “range” which fixes the minimum and maximum width/size. It usually suffices to adjust this parameter.\nFor continuous variables that are mapped to node/edge colour, you can use scale_colour_gradient() scale_colour_gradient2() or scale_colour_gradientn() (add edge_ before colour for edge colours). The difference between these functions is in how the gradient is constructed. gradient creates a two colour gradient (low-high). Simply specify the the two colours to be used (e.g. low = “blue”, high = “red”). gradient2 creates a diverging colour gradient (low-mid-high) (e.g. low = “blue”, mid = “white”, high = “red”) and gradientn a gradient consisting of more than three colours (specified with the colours parameter).\nFor categorical variables that are mapped to node colours (or fill in our example), you can use scale_fill_manual(). This forces you to choose a color for each category yourself. Simply create a vector of colors (see the got_palette) and pass it to the function with the parameter values.\nggraph then assigns the colors in the order of the unique values of the categorical variable. This are either the factor levels (if the variable is a factor) or the result of sorting the unique values (if the variable is a character).\n\nsort(unique(V(gotS1)$clu))\n\n[1] \"1\" \"2\" \"3\" \"4\" \"5\" \"6\" \"7\"\n\n\nIf you want more control over which value is mapped to which colour, you can pass the vector of colours as a named vector.\n\ngot_palette2 &lt;- c(\n    \"5\" = \"#1A5878\", \"3\" = \"#C44237\", \"2\" = \"#AD8941\",\n    \"1\" = \"#E99093\", \"4\" = \"#50594B\", \"7\" = \"#8968CD\", \"6\" = \"#9ACD32\"\n)\n\n\n\n\n\n\n\n\n\n\nUsing your own colour palette gives your network a unique touch. If you can’t be bothered with choosing colours, you may want to consider scale_fill_brewer() and scale_colour_brewer(). The function offers all palettes available at colorbrewer2.org.\n\nggraph(gotS1, layout = \"stress\") +\n  geom_edge_link0(aes(edge_linewidth = weight), edge_colour = \"grey66\") +\n  geom_node_point(aes(fill = clu, size = size), shape = 21) +\n  geom_node_text(aes(filter = size &gt;= 26, label = name), family = \"serif\") +\n  scale_fill_brewer(palette = \"Dark2\") +\n  scale_edge_width_continuous(range = c(0.2, 3)) +\n  scale_size_continuous(range = c(1, 6)) +\n  theme_graph() +\n  theme(legend.position = \"none\")\n\n\n\n\n\n\n\n\n(Check out this github repo from Emil Hvitfeldt for a comprehensive list of color palettes available in R)",
    "crumbs": [
      "Network Visualization",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Basics of ggraph</span>"
    ]
  },
  {
    "objectID": "visualization/ggraph-basics.html#themes",
    "href": "visualization/ggraph-basics.html#themes",
    "title": "10  Basics of ggraph",
    "section": "10.6 Themes",
    "text": "10.6 Themes\ntheme_graph() +\n  theme(legend.position = \"none\")\nthemes control the overall look of the plot. There are a lot of options within the theme() function of ggplot2. Luckily, we really don’t need any of those. theme_graph() is used to erase all of the default ggplot theme (e.g. axis, background, grids, etc.) since they are irrelevant for networks. The only option worthwhile in theme() is legend.position, which we set to “none”, i.e. don’t show the legend.\nThe code below gives an example for a plot with a legend.\n\nggraph(gotS1, layout = \"stress\") +\n  geom_edge_link0(aes(edge_linewidth = weight), edge_colour = \"grey66\") +\n  geom_node_point(aes(fill = clu, size = size), shape = 21) +\n  geom_node_text(aes(filter = size &gt;= 26, label = name), family = \"serif\") +\n  scale_fill_manual(values = got_palette) +\n  scale_edge_width_continuous(range = c(0.2, 3)) +\n  scale_size_continuous(range = c(1, 6)) +\n  theme_graph() +\n  theme(legend.position = \"bottom\")\n\n\n\n\n\n\n\n\nThis covers all the necessary steps to produce a standard network plot with ggraph. More advanced techniques will be covered in the next sections. We conclude the introductory part by recreating a quite famous network visualization",
    "crumbs": [
      "Network Visualization",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Basics of ggraph</span>"
    ]
  },
  {
    "objectID": "visualization/ggraph-basics.html#extended-example",
    "href": "visualization/ggraph-basics.html#extended-example",
    "title": "10  Basics of ggraph",
    "section": "10.7 Extended Example",
    "text": "10.7 Extended Example\nIn this section, we do a little code through to recreate the figure shown below.\n The network shows the linking between political blogs during the 2004 election in the US. Red nodes are conservative leaning blogs and blue ones liberal.\nThe dataset is included in the networkdata package.\n\ndata(\"polblogs\")\n\n## add a vertex attribute for the indegree\nV(polblogs)$deg &lt;- degree(polblogs, mode = \"in\")\n\nLet us start with a simple plot without any styling.\n\nlay &lt;- create_layout(polblogs, \"stress\")\n\nggraph(lay) +\n    geom_edge_link0(\n        edge_linewidth = 0.2, edge_colour = \"grey66\",\n        arrow = arrow(\n            angle = 15, length = unit(0.15, \"inches\"),\n            ends = \"last\", type = \"closed\"\n        )\n    ) +\n    geom_node_point()\n\n\n\n\n\n\n\n\nThere is obviously a lot missing. First, we delete all isolates and plot again.\n\npolblogs &lt;- delete.vertices(polblogs, which(degree(polblogs) == 0))\n\nWarning: `delete.vertices()` was deprecated in igraph 2.0.0.\nℹ Please use `delete_vertices()` instead.\n\nlay &lt;- create_layout(polblogs, \"stress\")\n\nggraph(lay) +\n    geom_edge_link0(\n        edge_linewidth = 0.2, edge_colour = \"grey66\",\n        arrow = arrow(\n            angle = 15, length = unit(0.1, \"inches\"),\n            ends = \"last\", type = \"closed\"\n        )\n    ) +\n    geom_node_point()\n\n\n\n\n\n\n\n\nThe original does feature a small disconnected component, but we remove this here.\n\ncomps &lt;- components(polblogs)\npolblogs &lt;- delete.vertices(polblogs, which(comps$membership == which.min(comps$csize)))\n\nlay &lt;- create_layout(polblogs, \"stress\")\nggraph(lay) +\n    geom_edge_link0(\n        edge_linewidth = 0.2, edge_colour = \"grey66\",\n        arrow = arrow(\n            angle = 15, length = unit(0.15, \"inches\"),\n            ends = \"last\", type = \"closed\"\n        )\n    ) +\n    geom_node_point()\n\n\n\n\n\n\n\n\nBetter, let’s start with some styling of the nodes.\n\nggraph(lay) +\n    geom_edge_link0(\n        edge_linewidth = 0.2, edge_colour = \"grey66\",\n        arrow = arrow(\n            angle = 15, length = unit(0.15, \"inches\"),\n            ends = \"last\", type = \"closed\"\n        )\n    ) +\n    geom_node_point(shape = 21, aes(fill = pol))\n\n\n\n\n\n\n\n\nThe colors are obviously wrong, so we fix this with a scale_fill_manual(). Additionally, we map the degree to node size.\n\nggraph(lay) +\n    geom_edge_link0(\n        edge_linewidth = 0.2, edge_colour = \"grey66\",\n        arrow = arrow(\n            angle = 15, length = unit(0.15, \"inches\"),\n            ends = \"last\", type = \"closed\"\n        )\n    ) +\n    geom_node_point(shape = 21, aes(fill = pol, size = deg), show.legend = FALSE) +\n    scale_fill_manual(values = c(\"left\" = \"#104E8B\", \"right\" = \"firebrick3\"))\n\n\n\n\n\n\n\n\nThe node sizes are also not that satisfactory, so we fix the range with scale_size().\n\nggraph(lay) +\n    geom_edge_link0(\n        edge_linewidth = 0.2, edge_colour = \"grey66\",\n        arrow = arrow(\n            angle = 10, length = unit(0.1, \"inches\"),\n            ends = \"last\", type = \"closed\"\n        )\n    ) +\n    geom_node_point(shape = 21, aes(fill = pol, size = deg), show.legend = FALSE) +\n    scale_fill_manual(values = c(\"left\" = \"#104E8B\", \"right\" = \"firebrick3\")) +\n    scale_size(range = c(0.5, 7))\n\n\n\n\n\n\n\n\nNow we move on to the edges. This is a bit more complicated since we have to create an edge variable first which indicates if an edge is within or between political orientations. This new variable is mapped to the edge color.\n\nel &lt;- get.edgelist(polblogs, names = FALSE)\n\nWarning: `get.edgelist()` was deprecated in igraph 2.0.0.\nℹ Please use `as_edgelist()` instead.\n\nel_pol &lt;- cbind(V(polblogs)$pol[el[, 1]], V(polblogs)$pol[el[, 2]])\nE(polblogs)$col &lt;- ifelse(el_pol[, 1] == el_pol[, 2], el_pol[, 1], \"mixed\")\n\n\nlay &lt;- create_layout(polblogs, \"stress\")\nggraph(lay) +\n    geom_edge_link0(\n        edge_linewidth = 0.2, aes(edge_colour = col),\n        arrow = arrow(\n            angle = 10, length = unit(0.1, \"inches\"),\n            ends = \"last\", type = \"closed\"\n        )\n    ) +\n    geom_node_point(shape = 21, aes(fill = pol, size = deg), show.legend = FALSE) +\n    scale_fill_manual(values = c(\"left\" = \"#104E8B\", \"right\" = \"firebrick3\")) +\n    scale_size(range = c(0.5, 7))\n\n\n\n\n\n\n\n\nSimilar to the node colors, we add a scale_edge_colour_manual() to adjust the edge colors.\n\nggraph(lay) +\n    geom_edge_link0(\n        edge_linewidth = 0.2, aes(edge_colour = col),\n        arrow = arrow(\n            angle = 10, length = unit(0.1, \"inches\"),\n            ends = \"last\", type = \"closed\"\n        ), show.legend = FALSE\n    ) +\n    geom_node_point(shape = 21, aes(fill = pol, size = deg), show.legend = FALSE) +\n    scale_fill_manual(values = c(\"left\" = \"#104E8B\", \"right\" = \"firebrick3\")) +\n    scale_edge_colour_manual(values = c(\"left\" = \"#104E8B\", \"mixed\" = \"goldenrod\", \"right\" = \"firebrick3\")) +\n    scale_size(range = c(0.5, 7))\n\n\n\n\n\n\n\n\nAlmost, but it seems there are a lot of yellow edges which run over blue edges. It looks as if these should run below according to the original viz. To achieve this, we use a filter trick. We add two geom_edge_link0() layers: First, for the mixed edges and then for the remaining edges. In that way, the mixed edges are getting plotted below.\n\nggraph(lay) +\n    geom_edge_link0(\n        edge_linewidth = 0.2, aes(filter = (col == \"mixed\"), edge_colour = col),\n        arrow = arrow(\n            angle = 10, length = unit(0.1, \"inches\"),\n            ends = \"last\", type = \"closed\"\n        ), show.legend = FALSE\n    ) +\n    geom_edge_link0(\n        edge_linewidth = 0.2, aes(filter = (col != \"mixed\"), edge_colour = col),\n        arrow = arrow(\n            angle = 10, length = unit(0.1, \"inches\"),\n            ends = \"last\", type = \"closed\"\n        ), show.legend = FALSE\n    ) +\n    geom_node_point(shape = 21, aes(fill = pol, size = deg), show.legend = FALSE) +\n    scale_fill_manual(values = c(\"left\" = \"#104E8B\", \"right\" = \"firebrick3\")) +\n    scale_edge_colour_manual(values = c(\"left\" = \"#104E8B\", \"mixed\" = \"goldenrod\", \"right\" = \"firebrick3\")) +\n    scale_size(range = c(0.5, 7))\n\n\n\n\n\n\n\n\nNow lets just add the theme_graph().\n\nggraph(lay) +\n    geom_edge_link0(\n        edge_linewidth = 0.2, aes(filter = (col == \"mixed\"), edge_colour = col),\n        arrow = arrow(\n            angle = 10, length = unit(0.1, \"inches\"),\n            ends = \"last\", type = \"closed\"\n        ), show.legend = FALSE\n    ) +\n    geom_edge_link0(\n        edge_linewidth = 0.2, aes(filter = (col != \"mixed\"), edge_colour = col),\n        arrow = arrow(\n            angle = 10, length = unit(0.1, \"inches\"),\n            ends = \"last\", type = \"closed\"\n        ), show.legend = FALSE\n    ) +\n    geom_node_point(shape = 21, aes(fill = pol, size = deg), show.legend = FALSE) +\n    scale_fill_manual(values = c(\"left\" = \"#104E8B\", \"right\" = \"firebrick3\")) +\n    scale_edge_colour_manual(values = c(\"left\" = \"#104E8B\", \"mixed\" = \"goldenrod\", \"right\" = \"firebrick3\")) +\n    scale_size(range = c(0.5, 7)) +\n    theme_graph()",
    "crumbs": [
      "Network Visualization",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Basics of ggraph</span>"
    ]
  },
  {
    "objectID": "visualization/ggraph-advanced.html",
    "href": "visualization/ggraph-advanced.html",
    "title": "11  Advanced Layouts",
    "section": "",
    "text": "11.1 Packages Needed for this Chapter\nlibrary(igraph)\nlibrary(ggraph)\nlibrary(graphlayouts)\nlibrary(networkdata)\nlibrary(ggforce)\ndata(\"got\")\n\ngotS1 &lt;- got[[1]]\n\ngot_palette &lt;- c(\n  \"#1A5878\",\n  \"#C44237\",\n  \"#AD8941\",\n  \"#E99093\",\n  \"#50594B\",\n  \"#8968CD\",\n  \"#9ACD32\"\n)\n\n## compute a clustering for node colors\nV(gotS1)$clu &lt;- as.character(membership(cluster_louvain(gotS1)))\n\n## compute degree as node size\nV(gotS1)$size &lt;- degree(gotS1)\nWhile “stress” is the key layout algorithm in graphlayouts, there are other, more specialized layouts that can be used for different purposes. In this part, we work through some examples with concentric layouts and learn how to disentangle extreme “hairball” networks.",
    "crumbs": [
      "Network Visualization",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Advanced Layouts</span>"
    ]
  },
  {
    "objectID": "visualization/ggraph-advanced.html#large-networks",
    "href": "visualization/ggraph-advanced.html#large-networks",
    "title": "11  Advanced Layouts",
    "section": "11.2 Large Networks",
    "text": "11.2 Large Networks\nThe stress layout also works well with medium to large graphs.\n\nThe network shows the biggest componentn of the co-authorship network of R package developers on CRAN (~12k nodes)\nIf you want to go beyond ~20k nodes, then you may want to switch to layout_with_pmds() or layout_with_sparse_stress() which are optimized to work with large graphs.\nThese are capable to deal with networks with several 100,000 nodes.",
    "crumbs": [
      "Network Visualization",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Advanced Layouts</span>"
    ]
  },
  {
    "objectID": "visualization/ggraph-advanced.html#concentric-layouts",
    "href": "visualization/ggraph-advanced.html#concentric-layouts",
    "title": "11  Advanced Layouts",
    "section": "11.3 Concentric Layouts",
    "text": "11.3 Concentric Layouts\nCircular layouts are generally not advisable. Concentric circles, on the other hand, help to emphasize the position of certain nodes in the network. The graphlayouts package has two function to create concentric layouts, layout_with_focus() and layout_with_centrality().\nThe first one allows to focus the network on a specific node and arrange all other nodes in concentric circles (depending on the geodesic distance) around it. Below we focus on the character Ned Stark.\n\nggraph(gotS1, layout = \"focus\", focus = 1) +\n  geom_edge_link0(aes(edge_linewidth = weight), edge_colour = \"grey66\") +\n  geom_node_point(aes(fill = clu, size = size), shape = 21) +\n  geom_node_text(\n    aes(filter = (name == \"Ned\"), size = size, label = name),\n    family = \"serif\"\n  ) +\n  scale_edge_width_continuous(range = c(0.2, 1.2)) +\n  scale_size_continuous(range = c(1, 5)) +\n  scale_fill_manual(values = got_palette) +\n  coord_fixed() +\n  theme_graph() +\n  theme(legend.position = \"none\")\n\n\n\n\n\n\n\n\nThe parameter focus in the first line is used to choose the node id of the focal node. The function coord_fixed() is used to always keep the aspect ratio at one (i.e. the circles are always displayed as a circle and not an ellipse).\nThe function draw_circle() can be used to add the circles explicitly.\n\nggraph(gotS1, layout = \"focus\", focus = 1) +\n  draw_circle(col = \"#00BFFF\", use = \"focus\", max.circle = 3) +\n  geom_edge_link0(aes(width = weight), edge_colour = \"grey66\") +\n  geom_node_point(aes(fill = clu, size = size), shape = 21) +\n  geom_node_text(\n    aes(filter = (name == \"Ned\"), size = size, label = name),\n    family = \"serif\"\n  ) +\n  scale_edge_width_continuous(range = c(0.2, 1.2)) +\n  scale_size_continuous(range = c(1, 5)) +\n  scale_fill_manual(values = got_palette) +\n  coord_fixed() +\n  theme_graph() +\n  theme(legend.position = \"none\")\n\n\n\n\n\n\n\n\nlayout_with_centrality() works in a similar way. You can specify any centrality index (or any numeric vector for that matter), and create a concentric layout where the most central nodes are put in the center and the most peripheral nodes in the biggest circle. The numeric attribute used for the layout is specified with the cent parameter. Here, we use the weighted degree of the characters.\n\nggraph(gotS1, layout = \"centrality\", cent = graph.strength(gotS1)) +\n  geom_edge_link0(aes(edge_linewidth = weight), edge_colour = \"grey66\") +\n  geom_node_point(aes(fill = clu, size = size), shape = 21) +\n  geom_node_text(aes(size = size, label = name), family = \"serif\") +\n  scale_edge_width_continuous(range = c(0.2, 0.9)) +\n  scale_size_continuous(range = c(1, 8)) +\n  scale_fill_manual(values = got_palette) +\n  coord_fixed() +\n  theme_graph() +\n  theme(legend.position = \"none\")\n\nWarning: `graph.strength()` was deprecated in igraph 2.0.0.\nℹ Please use `strength()` instead.\n\n\n\n\n\n\n\n\n\n(Concentric layouts are not only helpful to focus on specific nodes, but also make for a good tool to visualize ego networks.)",
    "crumbs": [
      "Network Visualization",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Advanced Layouts</span>"
    ]
  },
  {
    "objectID": "visualization/ggraph-advanced.html#backbone-layout",
    "href": "visualization/ggraph-advanced.html#backbone-layout",
    "title": "11  Advanced Layouts",
    "section": "11.4 Backbone Layout",
    "text": "11.4 Backbone Layout\nlayout_as_backbone() is a layout algorithm that can help emphasize hidden group structures. To illustrate the performance of the algorithm, we create an artificial network with a subtle group structure using sample_islands() from igraph.\n\ng &lt;- sample_islands(9, 40, 0.4, 15)\ng &lt;- simplify(g)\nV(g)$grp &lt;- as.character(rep(1:9, each = 40))\n\nThe network consists of 9 groups with 40 vertices each. The density within each group is 0.4 and there are 15 edges running between each pair of groups. Let us try to visualize the network with what we have learned so far.\n\nggraph(g, layout = \"stress\") +\n  geom_edge_link0(\n    edge_colour = \"black\",\n    edge_linewidth = 0.1,\n    edge_alpha = 0.5\n  ) +\n  geom_node_point(aes(fill = grp), shape = 21) +\n  scale_fill_brewer(palette = \"Set1\") +\n  theme_graph() +\n  theme(legend.position = \"none\")\n\n\n\n\n\n\n\n\nAs you can see, the graph seems to be a proper “hairball” without any special structural features standing out. In this case, though, we know that there should be 9 groups of vertices that are internally more densely connected than externally. To uncover this group structure, we turn to the “backbone layout”.\n\nbb &lt;- layout_as_backbone(g, keep = 0.4)\nE(g)$col &lt;- FALSE\nE(g)$col[bb$backbone] &lt;- TRUE\n\nThe idea of the algorithm is as follows. For each edge, an embeddedness score is calculated which serves as an edge weight attribute. These weights are then ordered and only the edges with the highest score are kept. The number of edges to keep is controlled with the keep parameter. In our example, we keep the top 40%. The parameter usually requires some experimenting to find out what works best. Since this may result in an unconnected network, we add all edges of the union of all maximum spanning trees. The resulting network is the “backbone” of the original network and the “stress” layout algorithm is applied to this network. Once the layout is calculated, all edges are added back to the network.\nThe output of the function are the x and y coordinates for nodes and a vector that gives the ids of the edges in the backbone network. In the code above, we use this vector to create a binary edge attribute that indicates if an edge is part of the backbone or not.\n\nggraph(g, layout = \"backbone\", keep = 0.4) +\n  geom_edge_link0(aes(edge_colour = backbone), edge_linewidth = 0.1) +\n  geom_node_point(aes(fill = grp), shape = 21) +\n  scale_fill_brewer(palette = \"Set1\") +\n  scale_edge_color_manual(values = c(rgb(0, 0, 0, 0.3), rgb(0, 0, 0, 1))) +\n  theme_graph() +\n  theme(legend.position = \"none\")\n\n\n\n\n\n\n\n\nThe groups are now clearly visible! Of course the network used in the example is specifically tailored to illustrate the power of the algorithm. Using the backbone layout in real world networks may not always result in such a clear division of groups. It should thus not be seen as a universal remedy for drawing hairball networks. Keep in mind: It can only emphasize a hidden group structure if it exists.\nThe plot below shows an empirical example where the algorithm was able to uncover a hidden group structure. The network shows facebook friendships of a university in the US. Node colour corresponds to dormitory of students. Left is the ordinary stress layout and right the backbone layout.",
    "crumbs": [
      "Network Visualization",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Advanced Layouts</span>"
    ]
  },
  {
    "objectID": "visualization/ggraph-advanced.html#longitudinal-networks",
    "href": "visualization/ggraph-advanced.html#longitudinal-networks",
    "title": "11  Advanced Layouts",
    "section": "11.5 Longitudinal Networks",
    "text": "11.5 Longitudinal Networks\nLongitudinal network data usually comes in the form of panel data, gathered at different points in time. We thus have a series of snapshots that need to be visualized in a way that individual nodes are easy to trace without the layout becoming to awkward.\nFor this part of the tutorial, you will need two additional packages.\n\nlibrary(gganimate)\n\nNo renderer backend detected. gganimate will default to writing frames to separate files\nConsider installing:\n- the `gifski` package for gif output\n- the `av` package for video output\nand restarting the R session\n\nlibrary(ggplot2)\nlibrary(patchwork)\n\nWe will be using the 50 actor excerpt from the Teenage Friends and Lifestyle Study from the RSiena data repository as an example. The data is part of the networkdata package.\n\ndata(\"s50\")\n\nThe dataset consists of three networks with 50 actors each and a vertex attribute for the smoking behavior of students. The function layout_as_dynamic() from graphlayouts can be used to visualize the three networks. The implemented algorithm calculates a reference layout which is a layout of the union of all networks and individual layouts based on stress minimization and combines those in a linear combination which is controlled by the alpha parameter. For alpha=1, only the reference layout is used and all graphs have the same layout. For alpha=0, the stress layout of each individual graph is used. Values in-between interpolate between the two layouts.\n\nxy &lt;- layout_as_dynamic(s50, alpha = 0.2)\n\nNow you could use ggraph in conjunction with patchwork to produce a static plot with all networks side-by-side.\n\npList &lt;- vector(\"list\", length(s50))\n\nfor (i in 1:length(s50)) {\n  pList[[i]] &lt;- ggraph(\n    s50[[i]],\n    layout = \"manual\",\n    x = xy[[i]][, 1],\n    y = xy[[i]][, 2]\n  ) +\n    geom_edge_link0(edge_linewidth = 0.6, edge_colour = \"grey66\") +\n    geom_node_point(shape = 21, aes(fill = as.factor(smoke)), size = 6) +\n    geom_node_text(label = 1:50, repel = FALSE, color = \"white\", size = 4) +\n    scale_fill_manual(\n      values = c(\"forestgreen\", \"grey25\", \"firebrick\"),\n      guide = ifelse(i != 2, \"none\", \"legend\"),\n      name = \"smoking\",\n      labels = c(\"never\", \"occasionally\", \"regularly\")\n    ) +\n    theme_graph() +\n    theme(legend.position = \"bottom\") +\n    labs(title = paste0(\"Wave \", i))\n}\n\nwrap_plots(pList)\n\n\n\n\n\n\n\n\nThis is nice but of course we want to animate the changes. This is where we have to get inventive, because ggraph does not (yet) work with gganimate out of the box. For the time being, the function below provides a hacky workaround to produce a data structure that can be passed to gganimate.\n\naninet_data &lt;- function(gList, alpha = 0.2, nodes = NULL) {\n  # check for absent nodes and add them\n  if (is.null(nodes)) {\n    all_nodes &lt;- unique(unlist(sapply(\n      gList,\n      function(x) vertex_attr(x, \"name\")\n    )))\n    for (i in 1:length(gList)) {\n      idx &lt;- which(!all_nodes %in% V(gList[[i]])$name)\n      if (length(idx) &gt; 0) {\n        gList[[i]] &lt;- add_vertices(\n          gList[[i]],\n          length(idx),\n          attr = list(name = all_nodes[idx])\n        )\n      }\n    }\n  } else if (nodes &gt;= 0) {\n    all_nodes &lt;- unlist(sapply(gList, function(x) vertex_attr(x, \"name\")))\n    all_nodes &lt;- names(which(table(all_nodes) &gt;= nodes * length(gList)))\n\n    for (i in 1:length(gList)) {\n      idx &lt;- which(!all_nodes %in% V(gList[[i]])$name)\n      if (length(idx) &gt; 0) {\n        gList[[i]] &lt;- add_vertices(\n          gList[[i]],\n          length(idx),\n          attr = list(name = all_nodes[idx])\n        )\n      }\n    }\n\n    for (i in 1:length(gList)) {\n      idx &lt;- which(!V(gList[[i]])$name %in% all_nodes)\n      if (length(idx) &gt; 0) {\n        gList[[i]] &lt;- delete_vertices(gList[[i]], idx)\n      }\n    }\n  }\n\n  xy &lt;- graphlayouts::layout_as_dynamic(gList, alpha = alpha)\n  nodes_lst &lt;- lapply(1:length(gList), function(i) {\n    cbind(\n      igraph::as_data_frame(gList[[i]], \"vertices\"),\n      x = xy[[i]][, 1],\n      y = xy[[i]][, 2],\n      frame = i\n    )\n  })\n\n  edges_lst &lt;- lapply(\n    1:length(gList),\n    function(i) cbind(igraph::as_data_frame(gList[[i]], \"edges\"), frame = i)\n  )\n\n  edges_lst &lt;- lapply(1:length(gList), function(i) {\n    edges_lst[[i]]$x &lt;- nodes_lst[[i]]$x[match(\n      edges_lst[[i]]$from,\n      nodes_lst[[i]]$name\n    )]\n    edges_lst[[i]]$y &lt;- nodes_lst[[i]]$y[match(\n      edges_lst[[i]]$from,\n      nodes_lst[[i]]$name\n    )]\n    edges_lst[[i]]$xend &lt;- nodes_lst[[i]]$x[match(\n      edges_lst[[i]]$to,\n      nodes_lst[[i]]$name\n    )]\n    edges_lst[[i]]$yend &lt;- nodes_lst[[i]]$y[match(\n      edges_lst[[i]]$to,\n      nodes_lst[[i]]$name\n    )]\n    edges_lst[[i]]$id &lt;- paste0(edges_lst[[i]]$from, \"-\", edges_lst[[i]]$to)\n    edges_lst[[i]]$status &lt;- TRUE\n    edges_lst[[i]]\n  })\n\n  all_edges &lt;- do.call(\"rbind\", lapply(gList, get.edgelist))\n  all_edges &lt;- all_edges[!duplicated(all_edges), ]\n  all_edges &lt;- cbind(all_edges, paste0(all_edges[, 1], \"-\", all_edges[, 2]))\n\n  edges_lst &lt;- lapply(1:length(gList), function(i) {\n    idx &lt;- which(!all_edges[, 3] %in% edges_lst[[i]]$id)\n    if (length(idx != 0)) {\n      tmp &lt;- data.frame(\n        from = all_edges[idx, 1],\n        to = all_edges[idx, 2],\n        id = all_edges[idx, 3]\n      )\n      tmp$x &lt;- nodes_lst[[i]]$x[match(tmp$from, nodes_lst[[i]]$name)]\n      tmp$y &lt;- nodes_lst[[i]]$y[match(tmp$from, nodes_lst[[i]]$name)]\n      tmp$xend &lt;- nodes_lst[[i]]$x[match(tmp$to, nodes_lst[[i]]$name)]\n      tmp$yend &lt;- nodes_lst[[i]]$y[match(tmp$to, nodes_lst[[i]]$name)]\n      tmp$frame &lt;- i\n      tmp$status &lt;- FALSE\n      idy &lt;- which(!names(edges_lst[[i]]) %in% names(tmp))\n      if (length(idy) &gt; 0) {\n        tmp[names(edges_lst[[i]])[idy]] &lt;- NA\n      }\n      edges_lst[[i]] &lt;- rbind(edges_lst[[i]], tmp)\n    }\n    edges_lst[[i]]\n  })\n\n  edges_df &lt;- do.call(\"rbind\", edges_lst)\n  nodes_df &lt;- do.call(\"rbind\", nodes_lst)\n  list(nodes = nodes_df, edges = edges_df)\n}\n\n#| label: build\nedges_df &lt;- do.call(\"rbind\", edges_lst)\nnodes_df &lt;- do.call(\"rbind\", nodes_lst)\n\ndat &lt;- aninet(s50)\nAnd that’s it in terms of data wrangling. All that is left is to plot/animate the data.\nggplot() +\n  geom_segment(\n    data = dat$edges_df,\n    aes(x = x, xend = xend, y = y, yend = yend, group = id, alpha = status),\n    show.legend = FALSE\n  ) +\n  geom_point(\n    data = dat$nodes_df, aes(x, y, group = name, fill = as.factor(smoke)),\n    shape = 21, size = 4, show.legend = FALSE\n  ) +\n  scale_fill_manual(values = c(\"forestgreen\", \"grey25\", \"firebrick\")) +\n  scale_alpha_manual(values = c(0, 1)) +\n  ease_aes(\"quadratic-in-out\") +\n  transition_states(frame, state_length = 0.5, wrap = FALSE) +\n  labs(title = \"Wave {closest_state}\") +\n  theme_void()",
    "crumbs": [
      "Network Visualization",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Advanced Layouts</span>"
    ]
  },
  {
    "objectID": "visualization/ggraph-advanced.html#multilevel-networks",
    "href": "visualization/ggraph-advanced.html#multilevel-networks",
    "title": "11  Advanced Layouts",
    "section": "11.6 Multilevel networks",
    "text": "11.6 Multilevel networks\nIn this section, you will get to know layout_as_multilevel(), a layout algorithm in the raphlayouts package which can be use to visualize multilevel networks.\nA multilevel network consists of two (or more) levels with different node sets and intra-level ties. For instance, one level could be scientists and their collaborative ties and the second level are labs and ties among them, and inter-level edges are the affiliations of scientists and labs.\nThe graphlayouts package contains an artificial multilevel network which will be used to illustrate the algorithm.\n\ndata(\"multilvl_ex\")\n\nThe package assumes that a multilevel network has a vertex attribute called lvl which holds the level information (1 or 2).\nThe underlying algorithm of layout_as_multilevel() has three different versions, which can be used to emphasize different structural features of a multilevel network.\nIndependent of which option is chosen, the algorithm internally produces a 3D layout, where each level is positioned on a different y-plane. The 3D layout is then mapped to 2D with an isometric projection. The parameters alpha and beta control the perspective of the projection. The default values seem to work for many instances, but may not always be optimal. As a rough guideline: beta rotates the plot around the y axis (in 3D) and alpha moves the POV up or down.\n\n11.6.1 Complete layout\nA layout for the complete network can be computed via layout_as_multilevel() setting type = \"all\". Internally, the algorithm produces a constrained 3D stress layout (each level on a different y plane) which is then projected to 2D. This layout ignores potential differences in each level and optimizes only the overall layout.\n\nxy &lt;- layout_as_multilevel(multilvl_ex, type = \"all\", alpha = 25, beta = 45)\n\nTo visualize the network with ggraph, you may want to draw the edges for each level (and inter level edges) with a different edge geom. This gives you more flexibility to control aesthetics and can easily be achieved with a filter.\n\nggraph(multilvl_ex, \"manual\", x = xy[, 1], y = xy[, 2]) +\n  geom_edge_link0(\n    aes(filter = (node1.lvl == 1 & node2.lvl == 1)),\n    edge_colour = \"firebrick3\",\n    alpha = 0.5,\n    edge_linewidth = 0.3\n  ) +\n  geom_edge_link0(\n    aes(filter = (node1.lvl != node2.lvl)),\n    alpha = 0.3,\n    edge_linewidth = 0.1,\n    edge_colour = \"black\"\n  ) +\n  geom_edge_link0(\n    aes(\n      filter = (node1.lvl == 2 &\n        node2.lvl == 2)\n    ),\n    edge_colour = \"goldenrod3\",\n    edge_linewidth = 0.3,\n    alpha = 0.5\n  ) +\n  geom_node_point(aes(shape = as.factor(lvl)), fill = \"grey25\", size = 3) +\n  scale_shape_manual(values = c(21, 22)) +\n  theme_graph() +\n  coord_cartesian(clip = \"off\", expand = TRUE) +\n  theme(legend.position = \"none\")\n\n\n\n\n\n\n\n\n\n\n11.6.2 Separate layouts for both levels\nIn many instances, there may be different structural properties inherent to the levels of the network. In that case, two layout functions can be passed to layout_as_multilevel() to deal with these differences. In our artificial network, level 1 has a hidden group structure and level 2 has a core-periphery structure.\nTo use this layout option, set type = \"separate\" and specify two layout functions with FUN1 and FUN2. You can change internal parameters of these layout functions with named lists in the params1 and params2 argument. Note that this version optimizes inter-level edges only minimally. The emphasis is on the intra-level structures.\n\nxy &lt;- layout_as_multilevel(\n  multilvl_ex,\n  type = \"separate\",\n  FUN1 = layout_as_backbone,\n  FUN2 = layout_with_stress,\n  alpha = 25,\n  beta = 45\n)\n\nAgain, try to include an edge geom for each level.\n\ncols2 &lt;- c(\n  \"#3A5FCD\",\n  \"#CD00CD\",\n  \"#EE30A7\",\n  \"#EE6363\",\n  \"#CD2626\",\n  \"#458B00\",\n  \"#EEB422\",\n  \"#EE7600\"\n)\n\nggraph(multilvl_ex, \"manual\", x = xy[, 1], y = xy[, 2]) +\n  geom_edge_link0(\n    aes(\n      filter = (node1.lvl == 1 & node2.lvl == 1),\n      edge_colour = col\n    ),\n    alpha = 0.5,\n    edge_linewidth = 0.3\n  ) +\n  geom_edge_link0(\n    aes(filter = (node1.lvl != node2.lvl)),\n    alpha = 0.3,\n    edge_linewidth = 0.1,\n    edge_colour = \"black\"\n  ) +\n  geom_edge_link0(\n    aes(\n      filter = (node1.lvl == 2 & node2.lvl == 2),\n      edge_colour = col\n    ),\n    edge_linewidth = 0.3,\n    alpha = 0.5\n  ) +\n  geom_node_point(aes(\n    fill = as.factor(grp),\n    shape = as.factor(lvl),\n    size = nsize\n  )) +\n  scale_shape_manual(values = c(21, 22)) +\n  scale_size_continuous(range = c(1.5, 4.5)) +\n  scale_fill_manual(values = cols2) +\n  scale_edge_color_manual(values = cols2, na.value = \"grey12\") +\n  scale_edge_alpha_manual(values = c(0.1, 0.7)) +\n  theme_graph() +\n  coord_cartesian(clip = \"off\", expand = TRUE) +\n  theme(legend.position = \"none\")\n\n\n\n\n\n\n\n\n\n\n11.6.3 Fix only one level\nThis layout can be used to emphasize one intra-level structure. The layout of the second level is calculated in a way that optimizes inter-level edge placement. Set type = \"fix1\" and specify FUN1 and possibly params1 to fix level 1 or set type = \"fix2\" and specify FUN2 and possibly params2 to fix level 2.\n\nxy &lt;- layout_as_multilevel(\n  multilvl_ex,\n  type = \"fix2\",\n  FUN2 = layout_with_stress,\n  alpha = 25,\n  beta = 45\n)\n\nggraph(multilvl_ex, \"manual\", x = xy[, 1], y = xy[, 2]) +\n  geom_edge_link0(\n    aes(\n      filter = (node1.lvl == 1 & node2.lvl == 1),\n      edge_colour = col\n    ),\n    alpha = 0.5,\n    edge_linewidth = 0.3\n  ) +\n  geom_edge_link0(\n    aes(filter = (node1.lvl != node2.lvl)),\n    alpha = 0.3,\n    edge_linewidth = 0.1,\n    edge_colour = \"black\"\n  ) +\n  geom_edge_link0(\n    aes(\n      filter = (node1.lvl == 2 & node2.lvl == 2),\n      edge_colour = col\n    ),\n    edge_linewidth = 0.3,\n    alpha = 0.5\n  ) +\n  geom_node_point(aes(\n    fill = as.factor(grp),\n    shape = as.factor(lvl),\n    size = nsize\n  )) +\n  scale_shape_manual(values = c(21, 22)) +\n  scale_size_continuous(range = c(1.5, 4.5)) +\n  scale_fill_manual(values = cols2) +\n  scale_edge_color_manual(values = cols2, na.value = \"grey12\") +\n  scale_edge_alpha_manual(values = c(0.1, 0.7)) +\n  theme_graph() +\n  coord_cartesian(clip = \"off\", expand = TRUE) +\n  theme(legend.position = \"none\")\n\n\n\n\n\n\n\n\n\n\n11.6.4 3D with threejs\nInstead of the default 2D projection, layout_as_multilevel() can also return the 3D layout by setting project2d = FALSE. The 3D layout can then be used with e.g. threejs to produce an interactive 3D visualization.\n\nlibrary(threejs)\nxyz &lt;- layout_as_multilevel(\n  multilvl_ex,\n  type = \"separate\",\n  FUN1 = layout_as_backbone,\n  FUN2 = layout_with_stress,\n  project2D = FALSE\n)\nmultilvl_ex$layout &lt;- xyz\nV(multilvl_ex)$color &lt;- c(\"#00BFFF\", \"#FF69B4\")[V(multilvl_ex)$lvl]\nV(multilvl_ex)$vertex.label &lt;- V(multilvl_ex)$name\n\ngraphjs(multilvl_ex, bg = \"black\", vertex.shape = \"sphere\")",
    "crumbs": [
      "Network Visualization",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Advanced Layouts</span>"
    ]
  },
  {
    "objectID": "visualization/enhance-viz.html",
    "href": "visualization/enhance-viz.html",
    "title": "12  Enhancing Visualizations",
    "section": "",
    "text": "12.1 Packages Needed for this Chapter\nlibrary(igraph)\nlibrary(ggraph)\nlibrary(graphlayouts)\nlibrary(ggforce)\nlibrary(networkdata)\ndata(\"got\")\n\ngotS1 &lt;- got[[1]]\n\ngot_palette &lt;- c(\n  \"#1A5878\",\n  \"#C44237\",\n  \"#AD8941\",\n  \"#E99093\",\n  \"#50594B\",\n  \"#8968CD\",\n  \"#9ACD32\"\n)\n\n## compute a clustering for node colors\nV(gotS1)$clu &lt;- as.character(membership(cluster_louvain(gotS1)))\n\n## compute degree as node size\nV(gotS1)$size &lt;- degree(gotS1)\nEverything that was covered so far should be enough to produce nice network visualizations, especially for scientific publications. However, ggraph has a lot more advanced functions/parameter settings to further enhance your visualization. If you are looking for something specific, it is always a good idea to read the documentation of the geoms.",
    "crumbs": [
      "Network Visualization",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Enhancing Visualizations</span>"
    ]
  },
  {
    "objectID": "visualization/enhance-viz.html#use-the-ggforce",
    "href": "visualization/enhance-viz.html#use-the-ggforce",
    "title": "12  Enhancing Visualizations",
    "section": "12.2 use the ggforce",
    "text": "12.2 use the ggforce\nThe ggforce package works pretty nicely with ggraph. You can, for instance, use the geom_mark_*() functions to highlight clusters.\n\nset.seed(665)\n\n## create network with a group structure\ng &lt;- sample_islands(9, 40, 0.4, 15)\ng &lt;- igraph::simplify(g)\nV(g)$grp &lt;- as.character(rep(1:9, each = 40))\n\nbb &lt;- layout_as_backbone(g, keep = 0.4)\nE(g)$col &lt;- F\nE(g)$col[bb$backbone] &lt;- T\n\n\nggraph(\n  g, #|\n  layout = \"manual\",\n  x = bb$xy[, 1],\n  y = bb$xy[, 2]\n) +\n  geom_edge_link0(aes(col = col), width = 0.2) +\n  geom_node_point(aes(fill = grp), shape = 21, size = 3) +\n  geom_mark_hull(\n    aes(x, y, group = grp, fill = grp),\n    concavity = 4,\n    expand = unit(2, \"mm\"),\n    alpha = 0.25\n  ) +\n  scale_color_brewer(palette = \"Set1\") +\n  scale_fill_brewer(palette = \"Set1\") +\n  scale_edge_color_manual(values = c(rgb(0, 0, 0, 0.3), rgb(0, 0, 0, 1))) +\n  theme_graph() +\n  theme(legend.position = \"none\")\n\n\n\n\n\n\n\n\nOf course you can also add a label to your clusters.\n\nggraph(\n  g, #|\n  layout = \"manual\",\n  x = bb$xy[, 1],\n  y = bb$xy[, 2]\n) +\n  geom_edge_link0(aes(col = col), width = 0.2) +\n  geom_node_point(aes(fill = grp), shape = 21, size = 3) +\n  geom_mark_hull(\n    aes(x, y, group = grp, fill = grp, label = grp),\n    concavity = 4,\n    expand = unit(2, \"mm\"),\n    alpha = 0.25\n  ) +\n  scale_color_brewer(palette = \"Set1\") +\n  scale_fill_brewer(palette = \"Set1\") +\n  scale_edge_color_manual(values = c(rgb(0, 0, 0, 0.3), rgb(0, 0, 0, 1))) +\n  theme_graph() +\n  theme(legend.position = \"none\")\n\n\n\n\n\n\n\n\nIf you want to avoid node overlaps, you can use geom_node_voronoi(). So this is actually already implemented in ggraph, but originates from geom_voronoi_tile().\n\nggraph(g, layout = \"manual\", x = bb$xy[, 1], y = bb$xy[, 2]) +\n  geom_edge_link0(aes(filter = !col, col = col), width = 0.2) +\n  geom_node_voronoi(\n    aes(x, y, fill = grp),\n    max.radius = 0.4,\n    expand = unit(-0.5, \"mm\"),\n    colour = \"black\"\n  ) +\n  scale_color_brewer(palette = \"Set1\") +\n  scale_fill_brewer(palette = \"Set1\") +\n  scale_edge_color_manual(values = c(rgb(0, 0, 0, 0.3), rgb(0, 0, 0, 1))) +\n  theme(\n    legend.position = \"none\",\n    panel.grid = element_blank(),\n    axis.ticks = element_blank(),\n    axis.text = element_blank()\n  ) +\n  theme_graph() +\n  theme(legend.position = \"none\")",
    "crumbs": [
      "Network Visualization",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Enhancing Visualizations</span>"
    ]
  },
  {
    "objectID": "visualization/enhance-viz.html#small-tricks-for-common-problems",
    "href": "visualization/enhance-viz.html#small-tricks-for-common-problems",
    "title": "12  Enhancing Visualizations",
    "section": "12.3 Small tricks for common problems",
    "text": "12.3 Small tricks for common problems\n\n“How can I achieve that my directed edges stop at the node border, independent from the node size?”\n\nThis one has given me headaches for the longest time. No matter what I tried, I always ended up with something like the below plot.\n\n## create a random network\nset.seed(1071)\ng &lt;- sample_pa(30, 1)\nV(g)$degree &lt;- degree(g, mode = \"in\")\n\nggraph(g, \"stress\") +\n  geom_edge_link(\n    aes(end_cap = circle(node2.degree + 2, \"pt\")),\n    edge_colour = \"black\",\n    arrow = arrow(\n      angle = 10,\n      length = unit(0.15, \"inches\"),\n      ends = \"last\",\n      type = \"closed\"\n    )\n  ) +\n  geom_node_point(aes(size = degree), col = \"grey66\", show.legend = FALSE) +\n  scale_size(range = c(3, 11)) +\n  theme_graph()\n\n\n\n\n\n\n\n\nThe overlap can be avoided by using the I() function from base R, which treats the entries of a vector “as is”. So we know that if a node has degree 5, it will be mapped to a circle with radius (or diameter?) “5pt”. Since this means, that you have no control over the scaling, you need to do that beforehand.\n\n## this function is borrowed from the ambient package\nnormalise &lt;- function(x, from = range(x), to = c(0, 1)) {\n  x &lt;- (x - from[1]) / (from[2] - from[1])\n  if (!identical(to, c(0, 1))) {\n    x &lt;- x * (to[2] - to[1]) + to[1]\n  }\n  x\n}\n\n## map to the range you want\nV(g)$degree &lt;- normalise(V(g)$degree, to = c(3, 11))\n\nggraph(g, \"stress\") +\n  geom_edge_link(\n    aes(end_cap = circle(node2.degree + 2, \"pt\")),\n    edge_colour = \"grey25\",\n    arrow = arrow(\n      angle = 10,\n      length = unit(0.15, \"inches\"),\n      ends = \"last\",\n      type = \"closed\"\n    )\n  ) +\n  geom_node_point(aes(size = I(degree)), col = \"grey66\") +\n  theme_graph()\n\n\n\n\n\n\n\n\nI would not be surprised though if there is an even easier fix for this problem.\n\n“How can I lower the opacity of nodes without making edges visible underneath?”\n\nOne of the rules I try to follow is that edges should not be visible on top of nodes. Usually that is easy to achieve by drawing the edges before the nodes. But if you want to lower the opacity of nodes, they do become visible again.\n\ng &lt;- sample_gnp(20, 0.5)\nV(g)$degree &lt;- degree(g)\n\nggraph(g, \"stress\") +\n  geom_edge_link(edge_colour = \"grey66\") +\n  geom_node_point(\n    size = 8,\n    aes(alpha = degree),\n    col = \"red\",\n    show.legend = FALSE\n  ) +\n  theme_graph()\n\n\n\n\n\n\n\n\nThe solution is rather simple. Just add a node layer with the same aesthetics below with alpha=1 (default) and color=\"white\" (or the background color of the plot).\n\nggraph(g, \"stress\") +\n  geom_edge_link(edge_colour = \"grey66\") +\n  geom_node_point(size = 8, col = \"white\") +\n  geom_node_point(\n    aes(alpha = degree),\n    size = 8,\n    col = \"red\",\n    show.legend = FALSE\n  ) +\n  theme_graph()\n\n\n\n\n\n\n\n\nOf course you could also use start_cap and end_cap here, but you may have to fiddle again as in the last example.\n\n“How can I enhance readability of node labels in hairball graphs?”\n\nSometimes it is really hard to make labels readable when the network is very cluttered\n\ng &lt;- sample_gnp(50, 0.7)\nV(g)$name &lt;- sapply(1:50, function(x) paste0(sample(LETTERS, 4), collapse = \"\"))\nE(g)$weight &lt;- runif(ecount(g))\n\nggraph(g) +\n  geom_edge_link0(\n    aes(edge_color = weight, edge_linewidth = weight),\n    show.legend = FALSE\n  ) +\n  geom_node_point(size = 8, color = \"#44a6c6\") +\n  geom_node_text(aes(label = name), fontface = \"bold\") +\n  scale_edge_color_continuous(low = \"grey66\", high = \"black\") +\n  scale_edge_width(range = c(0.1, 0.5)) +\n  theme_graph() +\n  coord_fixed()\n\nUsing \"stress\" as default layout\n\n\n\n\n\n\n\n\n\nHere you can make use of the fact that the layout of the nodes are stored in a “hidden” data frame when a ggraph object is constructed. That means you can use other geoms from other packages. In this case, the shadowtext package as shown below.\n\nggraph(g, \"stress\") +\n  geom_edge_link0(\n    aes(edge_color = weight, edge_linewidth = weight),\n    show.legend = FALSE\n  ) +\n  geom_node_point(size = 8, color = \"#44a6c6\") +\n  shadowtext::geom_shadowtext(\n    aes(x, y, label = name),\n    color = \"black\",\n    size = 4,\n    bg.colour = \"white\"\n  ) +\n  scale_edge_color_continuous(low = \"grey66\", high = \"black\") +\n  scale_edge_width(range = c(0.1, 0.5)) +\n  theme_graph() +\n  coord_fixed()",
    "crumbs": [
      "Network Visualization",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Enhancing Visualizations</span>"
    ]
  },
  {
    "objectID": "visualization/enhance-viz.html#edge-bundling",
    "href": "visualization/enhance-viz.html#edge-bundling",
    "title": "12  Enhancing Visualizations",
    "section": "12.4 Edge bundling",
    "text": "12.4 Edge bundling\n\ngr &lt;- edgebundle::us_flights\nstates &lt;- map_data(\"state\")\n\nggraph(gr, x = longitude, y = latitude) +\n  geom_polygon(\n    aes(long, lat, group = group),\n    states,\n    color = \"white\",\n    linewidth = 0.2\n  ) +\n  coord_sf(crs = \"NAD83\", default_crs = sf::st_crs(4326)) +\n  geom_edge_bundle_force(color = \"white\", width = 0.05)\n\n\n\n\n\n\n\nFigure 12.1\n\n\n\n\n\n\nggraph(gr, x = longitude, y = latitude) +\n  geom_polygon(\n    aes(long, lat, group = group),\n    states,\n    color = \"white\",\n    linewidth = 0.2\n  ) +\n  coord_sf(crs = \"NAD83\", default_crs = sf::st_crs(4326)) +\n  geom_edge_bundle_path(color = \"white\", width = 0.05)\n\n\n\n\n\n\n\nFigure 12.2",
    "crumbs": [
      "Network Visualization",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Enhancing Visualizations</span>"
    ]
  },
  {
    "objectID": "visualization/enhance-viz.html#snahelper",
    "href": "visualization/enhance-viz.html#snahelper",
    "title": "12  Enhancing Visualizations",
    "section": "12.5 snahelper",
    "text": "12.5 snahelper\nEven with a lot of experience, it may still be a painful process to produce nice looking figures by writing ggraph code. Enter the snahelper.\ninstall.packages(\"snahelper\")\nThe snahelper is an RStudio addin which provides you with a GUI to plot networks. Instead of writing code, you simply use drop-down menus to assign attributes to aesthetics or change appearances globally. One great feature of the addin is that you can adjust the position of nodes individually if you are not satisfied with their location. Once you are done, you can either directly export the figure to png or automatically insert the code to produce the figure into your script. That way, you can review the code and hopefully learn something from it. Below if a demo that shows its functionality.\n\nTo use the addin, simply highlight the variable name of your network within an R script and choose the SNAhelper from the Addins drop-down menu within RStudio. You can find more about the Addin on its dedicated pkgdown page",
    "crumbs": [
      "Network Visualization",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Enhancing Visualizations</span>"
    ]
  },
  {
    "objectID": "visualization/enhance-viz.html#misc",
    "href": "visualization/enhance-viz.html#misc",
    "title": "12  Enhancing Visualizations",
    "section": "12.6 Misc",
    "text": "12.6 Misc\nSome things that I frequently use are the following:\n\nchange the end_cap in geom_edge_link() to end edges before reaching the node. This is helpful for directed edges to not make the arrows disappear.\nlegend.position in theme() controls all legends at once. If you don’t want to show a specific legend, use guide = \"none\" in the respective scale_* function.\nuse scale_color_viridis_c() and scale_color_viridis_d(). The viridis colour palette makes plots easier to read by those with colorblindness and print well in grey scale.",
    "crumbs": [
      "Network Visualization",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Enhancing Visualizations</span>"
    ]
  },
  {
    "objectID": "inferential/introduction.html",
    "href": "inferential/introduction.html",
    "title": "14  Introduction",
    "section": "",
    "text": "14.1 The Challenges of Network Modeling\nUsing inferential network analysis, we can determine whether observed patterns — such as transitivity or homophily — are statistically significant or simply the result of random processes. This allows us to differentiate between patterns that are meaningful and those that arise by chance.\nStatistical network models bridge the gap between theory and empirical reality, allowing us to test hypotheses, uncover hidden mechanisms, and build predictive models that inform policy and intervention across diverse domains. When we look at a network, we don’t just want to describe what we see. We want to understand why the connections exist and how they shape the bigger picture. Thus to move beyond description and into prediction and explanation, statistical network modeling becomes essential. They allow us to the follwing questions questions: What would happen if we measured the network at a different point in time, among a different set of actors, or under different environmental conditions? Statistical models help estimate expected outcomes along with their variability, providing deeper insights into the nature of social structures.\nMost traditional data analysis treats the world like a collection of independent units; people, companies, events, each with their own neatly packaged characteristics. This monadic approach assumes that what happens to one entity has no impact on another, much like strangers sitting silently in a waiting room. This assumption, known as independent and identically distributed (i.i.d.) data, underlies most statistical models.\nBut network data laughs in the face of independence. Here, the unit of observation isn’t the individual, but the relationships between them; whether it’s friendships, collaboration or trade agreements. These relationships (or ties) don’t exist in isolation; they overlap, influence each other, and evolve in complex ways. When walking down the street, you do not let the flip of a coin determine whether you will befriend a by-passer or not.\nA defining feature of network data is that one tie can change the likelihood of another forming. Think of social circles: if Alice and Bob are both friends with Carol, odds are Alice and Bob will become friends too. This phenomenon (triadic closure), explains everything from friend groups to why your LinkedIn keeps suggesting you connect with your ex’s new colleague.\nUnlike spatial or temporal dependencies, where values near each other in space or time are correlated, network dependencies aren’t just noise to correct for; they’re the main event. Instead of being a nuisance, these dependencies are precisely what we study to understand how networks form and evolve.",
    "crumbs": [
      "Inferential Network Analysis",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "inferential/introduction.html#why-statistical-network-modeling",
    "href": "inferential/introduction.html#why-statistical-network-modeling",
    "title": "14  Introduction",
    "section": "14.2 Why Statistical Network Modeling?",
    "text": "14.2 Why Statistical Network Modeling?\nNetworks do not emerge randomly; rather, they are shaped by underlying social processes that drive the formation of ties and structures. These processes operate at both local and global levels, leading to self-organizing patterns that define social interaction.\nAt the core of social networks is the idea that structural patterns emerge locally. Individual relationships form through reciprocity, trust, and shared interests, and these small-scale interactions gradually create larger, more complex structures. Network ties do not form in isolation; they are dependent on existing connections, meaning that the presence of one tie often leads to another.\nNetwork theory provides a framework for understanding how these mechanisms shape the growth, stability, and dynamics of social networks. Moreover, inferential statistics allow us to test hypotheses based on these theories with respect to an observed network. Below, a few examples of theories that are directly connected to the social rules governing network formation are given:\n\nSocial Exchange & Reciprocity – Ties are often formed based on resource exchange, reinforcing reciprocity in network interactions (Gouldner 1960).\n\nStructural Balance & Triadic Closure – Networks tend toward stable triads, following the principle of balance theory (Heider 1946; Cartwright and Harary 1956)\nStructural Holes & Brokerage – Some individuals act as bridges between disconnected groups, gaining influence by controlling information flow. This process is linked to social influence, as brokers shape the diffusion of knowledge and innovation (Burt 1992).\nHomophily: Selection & Influence (“Birds of a feather flock together”) — People tend to form relationships with those who share similar attributes, attitude and behavior (McPherson, Smith-Lovin, and Cook 2001).\nThe Matthew Effect & Preferential Attachment – Individuals with many connections tend to accumulate even more—a process known in network theory as preferential attachment. This dynamic follows the rich-get-richer principle, reinforcing homopholy (or assortativity) as well-connected individuals attract even more ties (Merton 1968; Barabási and Albert 1999).\n\nWhen combined with inferential methods, network theory offers more than just structural description—it enables deep insight into how social networks emerge, stabilize, and adapt. This is vital for understanding social influence, the spread of information or disease, and the resilience of communities in the face of disruption.\nStatistical models of network data can serve several major purposes. First, they explain social relations and behaviors by identifying the underlying rules and processes that govern the formation and evolution of networks. Understanding these mechanisms helps us reveal why certain network structures arise, such as the tendency for individuals to form triads or clusters.\nSecond, statistical models are used for predicting social relations and behaviors. By learning from observed data, models can forecast future interactions or changes in network structure, making them invaluable for applications such as organizational planning, public health interventions, and online recommendation systems.\nThird, they enable the random generation of networks that resemble observed data. This is essential in fields such as algorithm engineering, where realistic network structures are needed to test algorithm performance. Moreover, simulated networks allow researchers to study processes such as information diffusion or the spread of diseases under controlled conditions.\nAt the heart of these purposes lies the concept of specifying realistic probability distributions for social networks. By formalizing hypothetical dependencies between ties—such as the likelihood of reciprocity or preferential attachment—researchers can sample graphs at random from these distributions. A good model will produce sampled networks that closely resemble the observed network with respect to key features of interest. This resemblance indicates that the modeled structural effects plausibly explain the emergence of the network.\nTo determine whether a model is a good fit for the data, we rely on parametric or non-parametric methods, depending on the assumptions we are willing (or able) to make about the data.",
    "crumbs": [
      "Inferential Network Analysis",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "inferential/introduction.html#parametric-and-non-parametric-methods",
    "href": "inferential/introduction.html#parametric-and-non-parametric-methods",
    "title": "14  Introduction",
    "section": "14.3 Parametric and Non-Parametric Methods",
    "text": "14.3 Parametric and Non-Parametric Methods\nParametric methods in network analysis are based on the assumption that the data conform to a specific theoretical probability distribution. This foundation enables formal statistical inference: researchers can evaluate network summary statistics—such as centrality, density, or clustering coefficients—within the framework of known distributions.\nA big advantage of parametric approaches is their ability to incorporate dependencies among ties. In social networks, the presence of one tie often influences the likelihood of others forming. Parametric models, such as Exponential Random Graph Models (ERGMs) and Stochastic Actor-Oriented Models (SAOMs), are explicitly designed to account for these interdependencies. They can model complex phenomena like reciprocity (the tendency for ties to be mutual), transitivity (the tendency for friends of friends to become friends), and homophily (the preference for connecting with similar others). When the assumptions underlying parametric methods hold, these models provide powerful explanations and predictions. They allow researchers to simulate alternative network scenarios, test competing hypotheses about network formation, and predict how networks might evolve under different conditions.\nIn contrast, non-parametric approaches avoid making strong assumptions about the underlying distribution of the data. These methods are especially valuable when working with real-world networks that are too complex, noisy, or irregular to fit neatly into a parametric mold. Rather than relying on predefined distributions, non-parametric approaches use data-driven techniques to generate reference distributions, often through permutation or resampling methods.\nNon-parametric methods are particularly useful in exploratory analyses or when assessing the robustness of findings derived from parametric models. They provide a flexible framework for evaluating whether observed patterns—such as high clustering or assortative mixing—are statistically meaningful or likely to have arisen by chance.\nThe choice between parametric and non-parametric methods should be guided by the research objectives, the nature and structure of the data, and the strength of theoretical assumptions. In the following chapters, we will explore both approaches in greater depth, focusing on their foundational principles, how they align with different data structures, and their practical use in modeling social networks.\n\n\n\n\n\n\nNoteNote on data structure and object assignments\n\n\n\nIn the inferential section of this book, we will be working with matrix, network and graph objects, interchangeably. It is important that you can understand and pay attention to these since some package functions only work with graph objects, and others with network/array objects. We try to keep it clear here by using suffix g, net and mat to clarify object assignment.",
    "crumbs": [
      "Inferential Network Analysis",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "inferential/introduction.html#references",
    "href": "inferential/introduction.html#references",
    "title": "14  Introduction",
    "section": "References",
    "text": "References\n\n\n\n\nBarabási, Albert-László, and Réka Albert. 1999. “Emergence of Scaling in Random Networks.” Science 286 (5439): 509–12.\n\n\nBurt, Ronald. 1992. Structural Holes: The Social Structure of Competition. Harvard University Press.\n\n\nCartwright, Dorwin, and Frank Harary. 1956. “Structural Balance: A Generalization of Heider’s Theory.” Psychological Review 63 (5): 277.\n\n\nGouldner, Alvin W. 1960. “The Norm of Reciprocity: A Preliminary Statement.” American Sociological Review, 161–78.\n\n\nHeider, Fritz. 1946. “Attitudes and Cognitive Organization.” The Journal of Psychology 21 (1): 107–12.\n\n\nMcPherson, Miller, Lynn Smith-Lovin, and James M Cook. 2001. “Birds of a Feather: Homophily in Social Networks.” Annual Review of Sociology 27 (1): 415–44.\n\n\nMerton, Robert K. 1968. “The Matthew Effect in Science.” Science 159 (3810): 56–63.",
    "crumbs": [
      "Inferential Network Analysis",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "inferential/non-parametric.html",
    "href": "inferential/non-parametric.html",
    "title": "15  Non-Parametric Methods",
    "section": "",
    "text": "15.1 Packages Needed for this Chapter\nA common non-parametric technique is permutation testing, where the observed network is systematically altered—typically by shuffling ties while preserving certain properties, such as the number of connections each node has. This process creates a reference distribution under the null hypothesis, allowing researchers to assess whether observed patterns (like high clustering or assortativity) are statistically significant or could have arisen by chance.\nImportantly, in non-parametric frameworks, \\(p\\)-values retain their traditional interpretation. They represent the probability of observing network features as extreme as those found in the data if the null hypothesis were true. This familiar statistical grounding makes non-parametric tests both intuitive and flexible.\nNon-parametric methods are particularly valuable during the exploratory phase of research, for validating parametric models, and in contexts where theoretical models are either undeveloped or poorly understood.\nlibrary(statnet)\nlibrary(igraph)\nlibrary(ggraph)\nlibrary(graphlayouts)\nlibrary(patchwork)\nlibrary(networkdata)\nlibrary(tidyverse)",
    "crumbs": [
      "Inferential Network Analysis",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Non-Parametric Methods</span>"
    ]
  },
  {
    "objectID": "inferential/non-parametric.html#conditional-uniform-graph-distributions-cugs",
    "href": "inferential/non-parametric.html#conditional-uniform-graph-distributions-cugs",
    "title": "15  Non-Parametric Methods",
    "section": "15.2 Conditional Uniform Graph Distributions (CUGs)",
    "text": "15.2 Conditional Uniform Graph Distributions (CUGs)\nOne widely used non-parametric approach is the Conditional Uniform Graph Distribution (CUG). CUGs define a reference distribution of graphs generated uniformly at random, but conditioned on one or more fixed characteristics of the observed network, such as the number of nodes, total number of edges, or degree distribution. By preserving these basic structural features, CUGs enable researchers to assess whether a more complex pattern (e.g., high reciprocity, homophily, or transitivity) is likely to have arisen by chance given the observed constraints.\nWhy do we do this? Well, in network analysis, statistical inference relies on some form of randomness; we assume that if a structure emerges more often than we would expect under random conditions, it likely reflects an underlying social mechanism. A CUG distribution formalizes this by defining a uniform distribution over a set of graphs, where each graph satisfies a fixed constraint, such as the same number of edges, node degrees, or dyadic relationships.\nFor example, if a network displays unusually high clustering, a CUG test can evaluate whether this is statistically exceptional or simply a byproduct of the network’s size and density. By comparing the observed statistic to the distribution of values from randomly generated but structurally constrained networks, researchers can determine the significance of the observed pattern. CUGs are especially useful when a full parametric model is unavailable, difficult to specify, or when the goal is to control for known network features while testing specific structural hypotheses.\nSeveral types of CUGs can be defined:\n\n𝒰 |E(L): All graphs with the same density as the observed graph.\n𝒰 |L: All graphs with the same number of edges as the observed graph.\n𝒰 |d: All graphs with the same degree distribution \\(d = (d_1, d_2, ..., d_n)\\).\n𝒰 |MAN: All graphs with the same dyad census (i.e., counts of mutual, asymmetric, and null ties).\n\nFor example, in a CUG with fixed density (number of edges), each graph that meets the constraint is equally probable. Graphs that do not satisfy the constraint (e.g., different number of ties) are assigned a probability of zero. This results in a null model that is tightly defined and suitable for hypothesis testing.",
    "crumbs": [
      "Inferential Network Analysis",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Non-Parametric Methods</span>"
    ]
  },
  {
    "objectID": "inferential/non-parametric.html#hypothesis-testing-procedure",
    "href": "inferential/non-parametric.html#hypothesis-testing-procedure",
    "title": "15  Non-Parametric Methods",
    "section": "15.3 Hypothesis Testing Procedure",
    "text": "15.3 Hypothesis Testing Procedure\nThe process follows classical null hypothesis testing logic:\n\nNull Hypothesis (\\(H_{0}\\)): The observed network is drawn from a CUG model that preserves a given constraint (e.g., edge count).\nAlternative Hypothesis (\\(H_{1}\\)): The observed network structure is not typical under this model and thus suggests a non-random social mechanism.\n\nTo test \\(H_0\\) we follow the below steps:\n\nDefine a summary statistic (e.g., degree centralization, reciprocity or transitivity).\nGenerate a large number of networks from the CUG distribution.\nCompute the statistic for each simulated network.\nCompare the observed statistic to the simulated distribution for that statistic.\n\nIf the observed value lies in the extreme tails (e.g., below the 2.5th percentile or above the 97.5th), we reject \\(H_{0}\\) at the corresponding 5% significance level, concluding that the observed structure likely reflects a genuine social mechanism rather than random chance. Since the tests are actually non-parametric (Monte Carlo) tests, we can estimate the \\(p\\)-value as the proportion of simulated test statistics that are as extreme or more extreme than the observed value. This provides a data-driven way to assess whether the observed statistic is likely under the null model, allowing for hypothesis testing without strong distributional assumptions.\nThe process is visualized in Figure 15.1 where the left panel shows the observed network, while the right panel displays the null distribution of the statistic based on 500 randomly generated graphs that preserve the same number of nodes and edges. The dashed red line indicates the observed value. If this value lies in the tail of the distribution, it suggests the observed structure is unlikely to have arisen by chance under the null model.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 15.1: Conditional Uniform Graph (CUG) test of a network statistic of interest.",
    "crumbs": [
      "Inferential Network Analysis",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Non-Parametric Methods</span>"
    ]
  },
  {
    "objectID": "inferential/non-parametric.html#running-example",
    "href": "inferential/non-parametric.html#running-example",
    "title": "15  Non-Parametric Methods",
    "section": "15.4 Running Example",
    "text": "15.4 Running Example\nWe will use a running example, namely Coleman (Coleman 1964) which is available as graph object in the networkdata package and as an array in statnet (or more specifically sna). We will use these interchangeably depending on the function and package used for our analysis. The data consists of self-reported friendship ties among 73 boys in a small high school in Illinois over the 1957-1958 academic year. Networks of reported ties for all 73 informants are provided for two time points (fall and spring). We will only focus on the fall network here, which we load and visualize:\n\n# Load data as graph object from networkdata and extract the fall network\ncoleman_g &lt;- networkdata::coleman[[1]]\n\n# plot using gggraph\nggraph(coleman_g, layout = \"stress\") + \n          geom_edge_link(edge_colour = \"#666060\", end_cap = circle(9,\"pt\"), \n                         n = 2, edge_width = 0.4, edge_alpha = 0.7, \n                         arrow = arrow(angle = 15, \n                         length = unit(0.1, \"inches\"), \n                         ends = \"last\", type = \"closed\"))  +\n          geom_node_point(fill = \"#525240\",color = \"#FFFFFF\", \n                           size = 5, stroke = 1.1, shape = 21) + \n          theme_graph() + \n          ggtitle(\"fall friendship network\") +\n          theme(legend.position = \"none\")",
    "crumbs": [
      "Inferential Network Analysis",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Non-Parametric Methods</span>"
    ]
  },
  {
    "objectID": "inferential/non-parametric.html#uniform-graph-distribution-given-expected-density",
    "href": "inferential/non-parametric.html#uniform-graph-distribution-given-expected-density",
    "title": "15  Non-Parametric Methods",
    "section": "15.5 Uniform Graph Distribution Given Expected Density",
    "text": "15.5 Uniform Graph Distribution Given Expected Density\nHere, we simulate graoh distributions while keeping the expected frequency from the observed network fixed. We aim to answer the follwoing quesiton: do we observed significantly many more reciporcal ties in our observed than what is expected by pure chance given random networks of the same expected density? So the statistic of interested here is number of reciprocal ties which we obtain from dyad census:\n\ndyad_census(coleman_g)\n\n$mut\n[1] 62\n\n$asym\n[1] 119\n\n$null\n[1] 2447\n\n# alterantiver one can also use the below\nsum(which_mutual(coleman_g)) / 2 \n\n[1] 62\n\n\nThe number of reciprocal ties is equal to 62. In order to compute this density, we can use the igraph function edge_density().\n\ndensity_obs &lt;- edge_density(coleman_g)\n\nFurther we save number of nodes and edges of the observed network:\n\nn_nodes &lt;- vcount(coleman_g)\n\nTo simulate the null distribution we use the function igraph from the sample_gnp package. We simulate 1000 random networks with the parameters specified above.\n\n# Simulate 1000 random graphs with same density\nset.seed(123)\nsim_g_dens &lt;- replicate(1000, {\n  sample_gnp(n = n_nodes, p = density_obs, directed = TRUE, loops = FALSE)\n}, simplify = FALSE)\n\nNote that the output consists of 1000 randomly generated networks as graph objects. While each individual simulated network may not contain exactly the same number of edges as the observed network, they are stochastically equivalent in terms of overall density.\nWe then compute the number of reciprocal (mutual) ties in each simulated network and visualize the resulting distribution under the null model of random tie allocation in Figure 15.2. A red dashed line is included in the plot to indicate the number of mutual ties observed in the actual network (62), highlighting how it compares to the distribution expected by chance.\n\n\n\n\n# Define mutual tie counter \ncount_mutual &lt;- function(g) {\n  sum(which_mutual(g)) / 2\n}\n\n# Apply to all simulated igraph objects\nmutual_counts &lt;- sapply(sim_g_dens, count_mutual)\n\n# Create data frame\nmutual_df &lt;- data.frame(mutual_ties = mutual_counts)\n\n# Plot\nggplot(mutual_df, aes(x = mutual_ties)) +\n  geom_histogram(binwidth = 1, fill = \"skyblue\", color = \"white\") +\n  geom_vline(xintercept = dyad_census(coleman_g)$mut, color = \"firebrick3\", linetype = \"dashed\") +\n  annotate(\"text\", x = dyad_census(coleman_g)$mut, y = Inf, label = \"Observed\", \n           vjust = -0.5, hjust = 1.1, color = \"firebrick3\", angle = 90) +\n  labs(title = \" \",\n       x = \"Number of Mutual Ties\",\n       y = \"Frequency\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\nFigure 15.2: Empirical distribution of reciprocal ties obtained from 1,000 random directed networks generated under the Conditional Uniform Graph (CUG) null model, holding constant the number of nodes and density (or expected number of edges). The vertical dashed line shows the number of mutual ties observed in the actual network, highlighting its deviation from the null model.\n\n\n\nAs shown in the plot, the observed number of mutual ties falls far in the right tail of the distribution. This indicates a substantial deviation from what would be expected under random tie formation. This can be interpreted as follows:\n\nIf ties in the network were allocated completely at random—while preserving the overall density—it would be highly unlikely to observe as many reciprocal ties as we do in the actual network.\n\nStated more formally, this analysis serves as a test of the following null hypothesis:\n\n\\(H_0\\): The number of mutual ties in the observed network does not differ from what would be expected under random tie formation, given the observed network density.\n\n\n\\(H_1\\): The observed number of mutual ties is significantly greater than expected by chance, indicating a tendency toward reciprocity beyond what density alone would predict.\n\nSince the observed number of mutual ties falls far in the right tail of the simulated distribution, we see a clear and substantial deviation from the null model. Therefore, we reject the null hypothesis, concluding that the observed network exhibits significantly more reciprocity than would be expected by random chance alone. This indicates a strong tendency for mutual connections in the network that is not explained by density alone.\nIn order to compute the \\(p\\)-value, we compare the observed value to the distribution from the simulated null model:\n\n# Empirical p-value (proportion of simulated mutual counts &gt;= observed)\np_value &lt;- mean(mutual_counts &gt;= dyad_census(coleman_g)$mut)\np_value\n\n[1] 0\n\n\nUnsurprisingly, this value is equal to zero, as we do not have any part of the distribution ranging over the observed count. However, \\(p\\)-values from Monte Carlo tests are never exactly 0, but rather “less than 1 / number of simulations”, so a more correct statement would be that\n\n\\(p\\) &lt; 0.001 (based on 1000 simulations) providing very strong evidence against the null hypothesis.\n\n\n\n\n\n\n\nNoteNote\n\n\n\n\n\nThe rgraph() function from the sna (or statnet) package can also be used to simulate random networks with a specified expected density. However, note that its output consists of adjacency matrices (as a 3D array or list), rather than igraph graph objects. This requires additional conversion before applying igraph-based analyses.",
    "crumbs": [
      "Inferential Network Analysis",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Non-Parametric Methods</span>"
    ]
  },
  {
    "objectID": "inferential/non-parametric.html#uniform-graph-distribution-given-number-of-edges",
    "href": "inferential/non-parametric.html#uniform-graph-distribution-given-number-of-edges",
    "title": "15  Non-Parametric Methods",
    "section": "15.6 Uniform Graph Distribution Given Number of Edges",
    "text": "15.6 Uniform Graph Distribution Given Number of Edges\nIn this section, we perform the same test but condition the random networks generated on the exact number of edges. So out null world is now stated as\n\n\\(H_0\\): The number of mutual ties in the observed network is consistent with what would be expected under random edge assignment, given a fixed number of nodes and edges.\n\nTo simulate this null distribution under this new constraint, we generate 1000 random networks using the sample_gnm() function from the igraph package, which samples graphs with a fixed number of edges.\n\nset.seed(123)  \n# Parameters from the observed network\nn_nodes &lt;- vcount(coleman_g)\nn_edges &lt;- ecount(coleman_g)\n\n# Simulate 1000 random directed graphs with fixed number of edges\nsim_g_edges &lt;- replicate(1000, {\n  g &lt;- sample_gnm(n = n_nodes, m = n_edges, directed = TRUE, loops = FALSE)\n}, simplify = FALSE)\n\nNote that each simulated network has exactly the same number of edges as the observed network, ensuring that any difference in mutual ties arises from the pattern of connections, not their quantity. We then compute the number of mutual ties in each simulated network and visualize the distribution, shown in Figure 15.3:\n\n\n\n\n# Define mutual tie counting function\ncount_mutual &lt;- function(g) {\n  sum(which_mutual(g)) / 2\n}\n\n# Apply to all simulated graphs\nmutual_counts_edges &lt;- sapply(sim_g_edges, count_mutual)\n\n# Plot the distribution\nmutual_df_edges &lt;- data.frame(mutual_ties = mutual_counts_edges)\n\nggplot(mutual_df_edges, aes(x = mutual_ties)) +\n  geom_histogram(binwidth = 1, fill = \"skyblue\", color = \"white\") +\n  geom_vline(xintercept = dyad_census(coleman_g)$mut, \n             color = \"firebrick3\", linetype = \"dashed\") +\n  annotate(\"text\", x = dyad_census(coleman_g)$mut, y = Inf, label = \"Observed\", \n           vjust = -0.5, hjust = 1.1, color = \"firebrick3\", angle = 90) +\n  labs(title = \" \",\n       x = \"Number of Mutual Ties\",\n       y = \"Frequency\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\nFigure 15.3: Empirical distribution of reciprocal ties obtained from 1000 random directed networks generated under the Conditional Uniform Graph (CUG) null model, holding constant the number of nodes and edges. The vertical dashed line shows the number of mutual ties observed in the actual network, highlighting its deviation from the null model.\n\n\n\nThe results are very similar to earlier; the observed number of mutual ties lies in the extreme right tail of the simulated distribution. This suggests a strong deviation from what would be expected under the null model of randomly assigned edges with no structural bias toward reciprocity.\n\n\n\n\n\n\nNoteNote\n\n\n\n\n\nThe rgnm() function from the sna (or statnet) package can also be used to simulate random networks with a specified expected density. However, note again that its output consists of adjacency matrices (as a 3D array or list).",
    "crumbs": [
      "Inferential Network Analysis",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Non-Parametric Methods</span>"
    ]
  },
  {
    "objectID": "inferential/non-parametric.html#uniform-graph-distribution-given-dyad-census",
    "href": "inferential/non-parametric.html#uniform-graph-distribution-given-dyad-census",
    "title": "15  Non-Parametric Methods",
    "section": "15.7 Uniform Graph Distribution Given Dyad Census",
    "text": "15.7 Uniform Graph Distribution Given Dyad Census\nGiven the results above where we tested for reciprocity using CUG null models that preserved either network density or the total number of edges—we now move to a more constrained and informative baseline: null models conditioned on dyadic structure. Specifically, we consider CUG tests based on the dyad census, which preserve the number of mutual, asymmetric, and null dyads observed in the original network. This means that the basic dyadic processes, such as the overall tendency toward reciprocity, are held constant in the simulated networks.\nBecause mutual ties are fixed across all networks under this null model, the number of reciprocal ties can no longer be used as a test statistic (it will be identical in every simulation). As a result, we shift our focus to higher-order structural properties that emerge from patterns of connected dyads. One such property is transitivity (or triadic closure), which captures the tendency for actors who share a common partner to also be directly connected.\nBy conditioning on the dyad census and evaluating statistics like transitivity, we can assess whether the observed network exhibits more complex structural organization than would be expected by chance, even after accounting for baseline dyadic tendencies.\nOur null hypothesis is now defined as:\n\n\\(H_0\\): The number of complete triangles (transitive triads) observed in the network does not differ from what would be expected under random tie allocation, conditional on the observed dyadic processes (i.e., mutual, asymmetric, and null dyad frequencies).\n\nCan we then say that there are more complete triangles than we expect by chance? Just how likely or unlikely is it to observe this many triangles? In order to answer this we again need to produce the world of hypothetical networks by simulation.\n\n\n\n\n\n\nNoteNote\n\n\n\n\n\nSince igraph does not currently provide a function to generate random graphs with a fixed dyad census, we make use of rguman() from the sna package, and then convert them to graph objects, and use igraph::triad_census() to compute the triad-level statistics.\n\n\n\nIn directed netowrks, there are two triad types cprresponding to closed triads MAN-030 and MAN-300.\n\nset.seed(123) \n# Generate 1000 graphs with fixed dyad census using rguman (from sna)\nsim_nets_man &lt;- rguman(n = 1000, nv = n_nodes, \n                        mut =  dyad_census(coleman_g)$mut, \n                        asym = dyad_census(coleman_g)$asym, \n                        null = dyad_census(coleman_g)$null, \n                        method = \"exact\")\n\n# Convert to igraph objects\nsim_g_man &lt;- lapply(1:dim(sim_nets_man)[1], function(i) {\n  graph_from_adjacency_matrix(sim_nets_man[i,,], mode = \"directed\")\n})\n\n# Get observed triad counts by index\nobs_triad &lt;- triad_census(coleman_g)\nobs_030T &lt;- obs_triad[9]   # position of 030T\nobs_300  &lt;- obs_triad[16]  # position of 300\n\n# sim_graphs: list of igraph directed networks\nsim_g_tc &lt;- t(sapply(sim_g_man, triad_census))\nsim_g_tc_df &lt;- as.data.frame(sim_g_tc)\n\n# Extract simulated triad counts \ntriad_df &lt;- sim_g_tc_df %&gt;%\n  select(9, 16) %&gt;%\n  rename(`Triad 030T` = 1, `Triad 300` = 2) %&gt;%\n  pivot_longer(cols = everything(), names_to = \"Triad_Type\", values_to = \"Count\")\n\n# Create observed values for vertical lines\nobs_df &lt;- data.frame(\n  Triad_Type = c(\"Triad 030T\", \"Triad 300\"),\n  Count = c(obs_030T, obs_300)\n)\n\nNow we have everything set to plot out the results as before to test out hypothesis. The results are shown in Figure 15.4.\n\n\n\n\nggplot(triad_df, aes(x = Count)) +\n  geom_histogram(binwidth = 1, fill = \"skyblue\", color = \"white\") +\n  geom_vline(data = obs_df, aes(xintercept = Count), color = \"firebrick3\", linetype = \"dashed\") +\n  geom_text(data = obs_df, aes(x = Count, y = Inf, label = \"Observed\"),\n            angle = 90, vjust = -0.5, hjust = 1.1, color = \"firebrick3\") +\n  facet_wrap(~Triad_Type, scales = \"free\") +\n  labs(title = \" \",\n       x = \"Triad Counts\",\n       y = \"Frequency\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\nFigure 15.4: Distributions of two types of closed triads, 030T (one mutual and two asymmetric ties) and 300 (three mutual ties), across 1000 randomly generated directed networks with a fixed dyad census. The red dashed lines indicate the counts of each triad type observed in the actual network. These plots assess whether the observed levels of triadic closure (23 and 22) deviate significantly from what would be expected under a null model that preserves mutual, asymmetric, and null dyads.",
    "crumbs": [
      "Inferential Network Analysis",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Non-Parametric Methods</span>"
    ]
  },
  {
    "objectID": "inferential/non-parametric.html#uniform-graph-distribution-given-fixed-degree",
    "href": "inferential/non-parametric.html#uniform-graph-distribution-given-fixed-degree",
    "title": "15  Non-Parametric Methods",
    "section": "15.8 Uniform Graph Distribution Given Fixed Degree",
    "text": "15.8 Uniform Graph Distribution Given Fixed Degree\nIs the average geodesic distance in the observed network significantly shorter (or longer) than would be expected in random networks with the same in- and out-degree sequence? To answer this we turn to uniform graph distribution given fixed degree. The null world correspond now to the following:\n\n\\(H_0\\): The average geodesic distance observed in the network is consistent with what would be expected under random graphs that preserve the in- and out-degree sequence.\n\nThis CUG test here evaluates whether the observed network is more (or less) efficiently connected than expected under degree-preserving randomization. The results are run and visualized below in Figure 15.5.\n\n\n\n\n# Extract in-degree and out-degree from observed network\nin_deg &lt;- igraph::degree(coleman_g, mode = \"in\")\nout_deg &lt;- igraph::degree(coleman_g, mode = \"out\")\n\n# Compute observed average geodesic distance (exclude disconnected pairs)\nobs_dist &lt;- mean_distance(coleman_g, directed = TRUE, unconnected = TRUE)\n\n# Simulate 1000 graphs preserving degree sequence\nset.seed(123)\nsim_deg_graphs &lt;- replicate(1000, sample_degseq(out.deg = out_deg, \n                                                in.deg = in_deg,\n                                                method = \"fast.heur.simple\"), \n                                                simplify = FALSE)  \n\n# Compute average geodesic distance for each simulation\nsim_geodist &lt;- sapply(sim_deg_graphs, function(g) {\n  mean_distance(g, directed = TRUE, unconnected = TRUE)\n})\n\n# Create data frame\ngeodist_df &lt;- data.frame(avg_geodist = sim_geodist)\n\n# Plot\nggplot(geodist_df, aes(x = avg_geodist)) +\n  geom_histogram(binwidth = 0.05, fill = \"skyblue\", color = \"white\") +\n  geom_vline(xintercept = obs_dist, color = \"firebrick3\", linetype = \"dashed\") +\n  annotate(\"text\", x = obs_dist, y = Inf, label = \"Observed\",\n           angle = 90, vjust = -0.5, hjust = 1.1, color = \"firebrick3\") +\n  labs(title = \" \",\n       x = \"Average Geodesic Distance\",\n       y = \"Frequency\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\nFigure 15.5: Distribution of average geodesic distance across 1000 random directed graphs preserving the in- and out-degree sequences of the observed network. The red dashed line indicates the observed value.\n\n\n\nThis indicates that the observed network is significantly more efficiently connected (in terms of shortest paths) than what would be expected under random edge arrangement, given the same degree distribution. In other words, the network’s structure enables actors to reach one another more quickly than random networks with the same node-level connectivity.\nWhat’s the \\(p\\)-value? We compare the observed value to the distribution from the simulated null model:\n\n# Empirical p-value (proportion of simulated mutual counts &gt;= observed)\np_value &lt;- mean(sim_geodist &lt;= obs_dist)\np_value\n\n[1] 0.002\n\n\nThus, we would reject the null. This example also highlights that you need to pay attention to which tail the observed value is located when computing the \\(p\\)-value (here left).",
    "crumbs": [
      "Inferential Network Analysis",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Non-Parametric Methods</span>"
    ]
  },
  {
    "objectID": "inferential/non-parametric.html#analysing-homophily-using-non-parametric-null-distribution",
    "href": "inferential/non-parametric.html#analysing-homophily-using-non-parametric-null-distribution",
    "title": "15  Non-Parametric Methods",
    "section": "15.9 Analysing Homophily Using Non-Parametric Null Distribution",
    "text": "15.9 Analysing Homophily Using Non-Parametric Null Distribution\nTo illustrate how we can test for homophily using a non-paramtric approach, we turn to another data set with available node attributes, namely the one by Lazega (2001). This data set comes from a network study of corporate law partnership that was carried out in a Northeastern US corporate law firm, referred to as SG&R, 1988-1991 in New England. It includes (among others) measurements of networks among the 71 attorneys (partners and associates) of this firm, i.e. their coworker network, advice network, friendship network, and indirect control networks. Various members’ attributes are also part of the dataset, including seniority, formal status, office in which they work, gender, lawschool attended.\nTwo tests will be performed:\n\nTest 1: Friendship based on gender\nTest 2: Cowork among partners based on law practice\n\nThese two networks that are used in the following are visualized in Figure 15.6.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 15.6: Visualization of two relational networks among lawyers in the Lazega dataset. Top: The friendship network, with nodes colored by gender. Bottom: The symmetrized coworking network among the 36 law firm partners, with nodes colored by practice area.\n\n\n\n\nTest 1: Friendship based on gender\nFirst, we examine gender-based homophily in a friendship network among corporate lawyers from the Lazega dataset. We load the data as a graph object from the package networkdata and vizualize it, with node color representing gender:\n\nlaw_friends_g &lt;-  networkdata::law_friends\n\nWe are here testing the following\n\n\\(H_0\\): The number of same-gender ties in the network is consistent with a random distribution of ties (given network size and density).\n\n\n\\(H_1\\): The observed network has significantly more (or fewer) same-gender ties than expected by chance.\n\nTo answer this, we use a non-parametric test based on randomly generated networks that match the observed network in size and density. This is effectively a CUG test under the null model 𝒰 |L, where ties are randomly distributed.\nWe start by converting the friendship network to an adjacency matrix and extracting the gender attribute from the nodes:\n\nlaw_mat &lt;- as.matrix(as_adjacency_matrix(law_friends_g, sparse = FALSE))\nlaw_nodes &lt;- vcount(law_friends_g)\nlaw_edges &lt;- sum(law_mat)\nlaw_gender &lt;- V(law_friends_g)$gender # gender extracted\n\nThen, we compute the number of observed homophilous ties (same-gender friendships) in the observed network:\n\n# 1 = male, 2 = female\nhomoph_obs &lt;- sum(law_mat[law_gender == 1, law_gender == 1]) +\n              sum(law_mat[law_gender == 2, law_gender == 2])\n\nTo generate networks from the null world, we simulate 1000 random directed graphs with the same number of nodes and edges as the observed network and then compute the number of homophilous ties in each simulated network (assuming the same ordering of gender assignments to nodes in each simulated network):\n\nset.seed(123)\nlaw_sim_g &lt;- replicate(1000, {\n  g &lt;- sample_gnm(n = law_nodes, m = law_edges, directed = TRUE, loops = FALSE)\n  as_adjacency_matrix(g, sparse = FALSE)\n}, simplify = FALSE)\n\nhomoph_sim &lt;- sapply(law_sim_g, function(mat) {\n  sum(mat[law_gender == 1, law_gender == 1]) +\n  sum(mat[law_gender == 2, law_gender == 2])\n})\n\nAs before, we can create a histogram of the simulated homophilous tie counts and mark the observed value, as shown in Figure 15.7:\n\n\n\n\nhomoph_df &lt;- data.frame(homophilous_ties = homoph_sim)\n\nggplot(homoph_df, aes(x = homophilous_ties)) +\n  geom_histogram(binwidth = 5, fill = \"skyblue\", color = \"white\") +\n  geom_vline(xintercept = homoph_obs, color = \"firebrick3\", linetype = \"dashed\", size = 1) +\n  annotate(\"text\", x = homoph_obs, y = Inf, label = \"Observed\",\n           vjust = -0.5, hjust = 1.1, angle = 90, color = \"firebrick3\") +\n  labs(title = \" \",\n       x = \"Number of Homophilous Ties\",\n       y = \"Frequency\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\nFigure 15.7: Distribution of same-gender friendship ties in 1000 random directed graphs preserving the number of nodes and total number of ties. The red dashed line marks the number of same-gender ties in the observed network.\n\n\n\nThe empirical \\(p\\)-value is computed as\n\nmean(homoph_sim &gt;= homoph_obs)\n\n[1] 0\n\n\nwhich implies that we reject the null hypothesis, concluding that gender-based homophily exists in the friendship network.\n\n\nTest 2: Cowork among partners based on law practice\nOur second test is whether partners in the law firm are more likely to collaborate (cowork) with others who share the same practice area (litigation or corporate) than we would expect by chance. This is a test of homophily based on professional specialization.\nWe begin by loading the law_cowork network from the networkdata package:\n\nlaw_cw_g &lt;- networkdata::law_cowork\n\nWe restrict the analysis to the first 36 lawyers, corresponding to the partners of the firm (as indicated by their “status” attribute).\nThe follwing hypotheses can be stated:\n\n\\(H_0\\): The number of same-practice coworking ties is consistent with a random distribution of ties, given number of ties and edges of the network.\n\n\n\\(H_1\\): The observed number of same-practice ties is significantly greater than expected under random conditions; suggesting homophily based on professional specialization.\n\nSince coworking is a reciprocal relationship, we treat the network as undirected by symmetrizing the adjacency matrix:\n\n# Create an adjacency matrix\nlaw_mat_cwdir &lt;- as.matrix(as_adjacency_matrix(law_cw_g, sparse = FALSE))\nlaw_mat_cwdir &lt;- law_mat_cwdir[1:36, 1:36]\n\n# Symmetrize to create undirected matrix (co-ties must be mutual)\nlaw_mat_cw &lt;- (law_mat_cwdir == 1 & t(law_mat_cwdir) == 1) * 1\n\nlaw_nodes_cw &lt;- nrow(law_mat_cw)\nlaw_ties_cw &lt;- sum(law_mat_cw) / 2  # undirected: each tie counted twice\n\nWe now extract the binary attribute practice for each partner and store it as a vector:\n\nlaw_attr_pract &lt;- V(law_cw_g)$practice[1:36]\n\nWe define homophilous ties as coworking ties between two lawyers of the same practice area. We count both litigation–litigation and corporate–corporate ties:\n\nhomoph_obs_cw &lt;- sum(\n  law_mat_cw[law_attr_pract == 1, law_attr_pract == 1]) / 2 +\n  sum(law_mat_cw[law_attr_pract == 2, law_attr_pract == 2]) / 2\n\nTo construct the null model, we generate 1000 random undirected graphs with the same number of nodes and total number of ties:\n\nset.seed(77)\nlaw_sim_cw &lt;- replicate(1000, {\n  g &lt;- sample_gnm(n = law_nodes_cw, m = law_ties_cw, directed = FALSE, loops = FALSE)\n  as_adjacency_matrix(g, sparse = FALSE)\n}, simplify = FALSE)\n\nWe calculate the number of same-practice ties in each simulated graph:\n\nhomoph_sim_cw &lt;- sapply(law_sim_cw, function(mat) {\n  sum(mat[law_attr_pract == 1, law_attr_pract == 1]) / 2 +\n  sum(mat[law_attr_pract == 2, law_attr_pract == 2]) / 2\n})\n\nand finally, plot the null distribution and compare with observed value:\n\n\n\n\nhomoph_sim_df &lt;- data.frame(homophilous_ties = homoph_sim_cw)\n\nggplot(homoph_sim_df, aes(x = homophilous_ties)) +\n  geom_histogram(binwidth = 1, fill = \"skyblue\", color = \"white\") +\n  geom_vline(xintercept = homoph_obs_cw, color = \"firebrick3\", linetype = \"dashed\") +\n  annotate(\"text\", x = homoph_obs_cw, y = Inf, label = \"Observed\",\n           vjust = -0.5, hjust = 1.1, angle = 90, color = \"firebrick3\") +\n  labs(title = \"\",\n       x = \"Number of Homophilous Ties\",\n       y = \"Frequency\") +\n  coord_cartesian(ylim = c(0, 100)) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\nFigure 15.8: Distribution of same-practice coworking ties in 1000 random undirected graphs preserving the number of nodes and ties. The red dashed line indicates the observed number of homophilous ties among law firm partners. This test evaluates whether the level of professional homophily (litigation or corporate) is different than what is expected under random tie allocation.\n\n\n\nAs seen in Figure 15.8, we are in the right tail, indicating we arfe observing more homophilous ties than expected by chance. To formnally test the hypothesis above we can compute the \\(p\\)-value:\n\nmean(homoph_sim_cw &gt;= homoph_obs_cw)\n\n[1] 0.002\n\n\nThus, there is statistically significant evidence of practice-based homophily among the firm’s partners.",
    "crumbs": [
      "Inferential Network Analysis",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Non-Parametric Methods</span>"
    ]
  },
  {
    "objectID": "inferential/non-parametric.html#references",
    "href": "inferential/non-parametric.html#references",
    "title": "15  Non-Parametric Methods",
    "section": "References",
    "text": "References\n\n\n\n\nColeman, James Samuel. 1964. Introduction to Mathematical Sociology.\n\n\nLazega, Emmanuel. 2001. The Collegial Phenomenon: The Social Mechanisms of Cooperation Among Peers in a Corporate Law Partnership. Oxford University Press, USA.",
    "crumbs": [
      "Inferential Network Analysis",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Non-Parametric Methods</span>"
    ]
  },
  {
    "objectID": "inferential/rgm.html",
    "href": "inferential/rgm.html",
    "title": "16  Random Graph Models",
    "section": "",
    "text": "16.1 Packages Needed for this Chapter\nA random graph model is a probability distribution over a set of possible networks, defined by a set of parameters that control how likely different graph structures are to occur. Instead of producing a single deterministic network, the model generates an ensemble of networks, each drawn according to specific rules or constraints. These models allow us to explore what kinds of network structures arise by chance and to test whether observed features, such as clustering or degree patterns, are more or less likely than expected under a given set of assumptions.\nThis chapter introduces a range of models used to represent and understand the structure of social and relational networks. We begin with the classic \\(G(n,p)\\) random graph model, in which each pair of nodes is connected independently with a fixed probability. While analytically tractable and conceptually simple, \\(G(n,p)\\) falls short in capturing the structural complexity of real-world networks; it fails to account for common features such as clustering, skewed degree distributions, or short average path lengths.\nTo address these shortcomings, we explore several alternative models that incorporate more realistic structural constraints. The small-world model captures the coexistence of high local clustering and global reachability observed in many social systems. The configuration model enables the generation of random graphs with a fixed degree sequence, offering more control over node-level connectivity patterns. For each model, we discuss its definition, how to simulate or estimate it, and its relevance for modeling real network data.\nWhile these basic random graph models can serve as a simple yet informative baseline for understanding connectivity patterns, they only allow us to control only a single feature (e.g. edge probability and degree sequence) at a time, limiting its usefulness for modeling richer network patterns. This leads the way to out next chapter where we introduce exponential random graph models (ERGMs), which is a powerful and flexible framework that allows for the specification of multiple dependencies between ties simultaneously; such as reciprocity, triadic closure, and attribute-based mixing.\nlibrary(igraph)\nlibrary(ggraph)\nlibrary(graphlayouts)\nlibrary(networkdata)\nlibrary(tidyverse)",
    "crumbs": [
      "Inferential Network Analysis",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Random Graph Models</span>"
    ]
  },
  {
    "objectID": "inferential/rgm.html#the-erdősrényi-model-gn-p",
    "href": "inferential/rgm.html#the-erdősrényi-model-gn-p",
    "title": "16  Random Graph Models",
    "section": "16.2 The Erdős–Rényi Model: \\(G(n, p)\\)",
    "text": "16.2 The Erdős–Rényi Model: \\(G(n, p)\\)\nThe Erdős–Rényi model (Erdos and Rényi 1959), commonly denoted as \\(G(n, p)\\), is one of the earliest and most fundamental models in the study of random graphs. In this model, a graph is constructed by taking a set of \\(n\\) nodes and forming edges between each pair of nodes independently with fixed probability \\(p\\), where \\(0 &lt; p &lt; 1\\). This means that every possible dyad in the network has the same chance of being connected, and the existence of one edge has no influence on any other. The result is a fully dyad-independent structure with homogeneous tie probabilities.\nOne of the key implications of the \\(G(n, p)\\) model is that it defines a probability distribution over the entire space of possible graphs with \\(n\\) nodes. Each graph in this space is assigned a probability based on how many edges it contains and the value of \\(p\\). The expected number of edges in a graph generated from \\(G(n, p)\\) is \\(\\binom{n}{2} \\cdot p\\), and the degree of each node follows a binomial distribution with parameters \\((n - 1, p)\\). For sufficiently large \\(n\\), this degree distribution approaches a normal distribution due to the central limit theorem. The expected degree of a node is approximately \\((n - 1) \\cdot p\\), and the overall density of the graph is close to \\(p\\).\nIf we observe an empirical network and wish to fit a \\(G(n, p)\\) model to it, the most natural estimate for \\(p\\) is the observed edge density. This can be computed as the number of edges divided by the total number of possible dyads, giving the maximum likelihood estimate \\(\\hat{p} = \\frac{m}{\\binom{n}{2}}\\), where \\(m\\) is the number of observed edges.\nWhile the \\(G(n, p)\\) model is mathematically elegant and provides a useful baseline for comparison, it fails to capture many features commonly seen in real-world networks. Most notably, it cannot account for clustering (triadic closure), skewed degree distributions, or structural dependencies such as reciprocity or community structure. Moreover, the model allows control over only one parameter—tie probability—making it impossible to simultaneously model multiple features of network structure.\nBecause of these limitations, the \\(G(n, p)\\) model is best used as a null model for hypothesis testing or as a starting point for understanding more complex generative processes. It helps clarify which features of an observed network deviate from randomness and motivates the need for more structured models such as the configuration model, the small-world model, or exponential random graph models, which allow for greater flexibility in representing real-world relational data.\n\nExample: \\(G(n,p)\\)\nTo see why the \\(G(n, p)\\) model is often an inadequate representation of real-world networks, we can compare its properties to those of an actual empirical network. A typical social or informational network displays three features that are not captured well by \\(G(n, p)\\): a right-skewed degree distribution (with hubs), high clustering or triadic closure, and short average path lengths. While \\(G(n, p)\\) can match the density of a network, it assumes a binomial (or normal) degree distribution, minimal clustering, and does not account for structural heterogeneity.\nThe example below uses the igraph package in R and a real network dataset of moderate-to-large size to illustrate these differences. We load a real-world network; a network of co-appearances of characters in Victor Hugo’s novel “Les Miserables” which can be loaded from the networkdata package. The network is visulaize din Figure 16.1.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 16.1: Visualization of the Les Misérables co-appearance network. Nodes represent characters, and edges indicate co-occurrence in the same chapter. Node size reflects degree (number of connections), and labels are shown only for nodes with degree greater than 15. The network exhibits strong heterogeneity, with a few highly connected hubs and many low-degree nodes—a feature not captured by simple random graph models like \\(G(n, p)\\).\n\n\n\nWe compute this its key structural properties, then generate a random graph with the same number of nodes and expected density using sample_gnp(). We then compare the two in terms of degree distribution, transitivity (clustering), and average geodesic distance.\nThe code below yields Table 16.1 which summarizes key structural properties of the observed Les Misérables network and the corresponding \\(G(n, p)\\) random graph. These include the global clustering coefficient (measuring the tendency of nodes to form closed triads), the average geodesic distance (a measure of path efficiency), and the maximum degree (the highest number of connections any single node has).\nThe observed network shows substantially higher clustering, a slightly shorter average path length, and a much larger maximum degree. These results highlight that the empirical network is both more locally cohesive and more hierarchically structured than its random counterpart. The presence of hubs and local clusters—common in real-world networks—is not reproduced by the \\(G(n, p)\\) model, which assumes uniform and independent edge probabilities.\nTogether, these differences support the conclusion that random tie formation alone cannot explain the structure of this network.\n\n\n\nTable 16.1: Comparison of structural features: Observed vs G(n, p)\n\n\n\nlibrary(knitr)\n# Load the Les Misérables network from networkdata\ndata(\"miserables\")\ng_obs &lt;- miserables\n\n# Basic stats of the observed network\nn &lt;- vcount(g_obs)\nm &lt;- ecount(g_obs)\ndensity_obs &lt;- edge_density(g_obs)\ndeg_obs &lt;- degree(g_obs)\nclustering_obs &lt;- transitivity(g_obs, type = \"global\")\ndist_obs &lt;- mean_distance(g_obs, directed = FALSE, unconnected = TRUE)\n\n# Generate a G(n, p) graph with the same density\nset.seed(123)\ng_gnp &lt;- sample_gnp(n = n, p = density_obs, directed = FALSE)\n\ndeg_gnp &lt;- degree(g_gnp)\nclustering_gnp &lt;- transitivity(g_gnp, type = \"global\")\ndist_gnp &lt;- mean_distance(g_gnp, directed = FALSE, unconnected = TRUE)\n\n# Combine comparison into a data frame\ncomparison &lt;- data.frame(\n  Model = c(\"Observed\", \"G(n, p)\"),\n  Clustering = c(clustering_obs, clustering_gnp),\n  AvgPathLength = c(dist_obs, dist_gnp),\n  MaxDegree = c(max(deg_obs), max(deg_gnp))\n)\n\n# Format with kable\nkable(comparison, caption = \"Comparison of structural features: Observed vs G(n, p)\",\n      digits = 3, align = \"c\")\n\n\n\n\nModel\nClustering\nAvgPathLength\nMaxDegree\n\n\n\n\nObserved\n0.499\n4.861\n36\n\n\nG(n, p)\n0.086\n2.636\n12\n\n\n\n\n\n\n\n\nThe comparison confirms that the \\(G(n, p)\\) model falls short in capturing key structural features of real-world networks. The observed network displays a right-skewed degree distribution, with several highly connected nodes, while the random graph shows a tight distribution around the mean. The clustering coefficient in the observed network is substantially higher, indicating a tendency for character triads to form—an effect not captured by dyad-independent models. Although both networks have relatively short path lengths, this is largely coincidental and does not reflect the underlying efficiency created by hubs or clustered substructures.\nTo further illustrate the limitations of the \\(G(n, p)\\) model, we also examine the degree distributions of the observed network and the simulated random graph. Real-world networks often exhibit right-skewed degree distributions, with many nodes having few connections and a small number of hubs with very high degree. In contrast, the \\(G(n, p)\\) model produces a binomial (and approximately normal) degree distribution, where most nodes have degrees clustered around the mean. By comparing these two distributions side by side, we can observe how poorly the random model captures the heterogeneity present in the empirical network.\n\n\n\n\n# Plot the degree distributions\ndf_deg &lt;- data.frame(\n  Degree = c(deg_obs, deg_gnp),\n  Type = rep(c(\"Observed\", \"G(n, p)\"), times = c(length(deg_obs), length(deg_gnp)))\n)\n\nggplot(df_deg, aes(x = Degree, fill = Type)) +\n  geom_histogram(position = \"identity\", bins = 20, alpha = 0.6, color = \"white\") +\n  facet_wrap(~Type, scales = \"free_y\") +\n  labs(x = \"Node Degree\", y = \"Frequency\") +\n  theme_minimal() +\n  scale_fill_manual(values = c(\"skyblue\", \"tomato\")) +\n  theme(legend.title = element_blank())\n\n\n\n\n\n\n\n\n\n\nFigure 16.2: Comparison of node degree distributions in the observed Les Misérables network and a random graph generated using the \\(G(n, p)\\) model with matching density. The observed network shows a right-skewed distribution with several high-degree hubs, while the \\(G(n, p)\\) graph has a more symmetric distribution centered around the mean. This contrast illustrates the inability of the random model to replicate the degree heterogeneity typical of real-world networks.\n\n\n\nThis example underscores the need for more realistic network models that can capture multiple structural properties simultaneously. While the \\(G(n, p)\\) model offers a useful theoretical baseline, its assumptions of uniform edge probability and independent tie formation lead to networks with unrealistic degree distributions. In particular, it fails to capture the heterogeneity observed in many real-world systems, where some nodes act as hubs while others have very few connections. To address this limitation, we turn to the configuration model, which allows us to fix the degree sequence of the network and thereby preserve node-level connectivity patterns. This model represents a natural next step toward our second random graph model.\n\n\n\n\n\n\nNoteNote: \\(G(n, p)\\) and CUG Given Density\n\n\n\nThe \\(G(n, p)\\) model is mathematically equivalent to a Conditional Uniform Graph (CUG) test given density. In both cases, edges are formed between node pairs independently with fixed probability \\(p\\), and the overall network density is preserved on average across simulations.\nHowever, there are key differences in interpretation and usage:\n\nThe \\(G(n, p)\\) model is a generative model used to define a probability distribution over the space of graphs with \\(n\\) nodes and tie probability \\(p\\). It is often used in theoretical network science as a baseline or null model.\nA CUG test given density is a hypothesis testing framework. It conditions on the observed number of nodes and the expected density, and tests whether an observed network statistic (e.g., mutual ties, clustering) deviates significantly from what would be expected by chance.\n\nIn practice, simulating random graphs under the \\(G(n, p)\\) model is functionally identical to conducting a CUG test with fixed density. The distinction lies in whether the model is used for generative modeling or for evaluating the statistical significance of observed network features.",
    "crumbs": [
      "Inferential Network Analysis",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Random Graph Models</span>"
    ]
  },
  {
    "objectID": "inferential/rgm.html#the-configuration-model",
    "href": "inferential/rgm.html#the-configuration-model",
    "title": "16  Random Graph Models",
    "section": "16.3 The Configuration Model",
    "text": "16.3 The Configuration Model\nAs discussed in the previous section, the \\(G(n, p)\\) model assumes that all ties are equally likely and independent, which leads to unrealistic degree distributions. In many real-world networks, however, some nodes are consistently more connected than others. To capture this degree heterogeneity, we turn to the configuration model (Bender and Canfield 1978), which generates random graphs while preserving the exact degree sequence of the observed network.\nIntuitively, the configuration model works by first decomposing the network into “stubs”—half-edges representing the degree of each node. These stubs are then randomly paired to form complete edges, resulting in a network that maintains the original node degrees but otherwise randomizes the connections.\nFor example, if a node has degree 3, it is assigned 3 stubs. The model then shuffles all stubs and connects them randomly in pairs. This procedure generates a new graph where each node has the same degree as in the original network, but where the overall topology (i.e., who connects to whom) is random.\nWhile powerful, the configuration model comes with some caveats. Because stubs are paired at random, the resulting graphs may contain self-loops (nodes connected to themselves) or multi-edges (multiple edges between the same pair of nodes). Additionally, the sum of the degree sequence must be even for the pairing to be valid.\n\nExample: Configuration Model\nIn this example, we continue with the Les Misérables co-appearance network and compare it to a random network generated from the configuration model. The goal is to assess how well the configuration model replicates key structural features of the observed network when it exactly preserves the degree sequence but randomizes the specific tie configuration.\nWe use the igraph package to compute network properties and simulate the configuration model using sample_degseq(). The configuration model guarantees that each node retains its observed degree. We create a comparison table and visualize the degree distribution as before in Table 16.2 and Figure 16.3.\n\n\n\nTable 16.2: Comparison of structural features: Observed vs Configuration Model\n\n\n\n# Get observed degree sequence\ndeg_seq &lt;- degree(g_obs)\n\n# Compute observed properties\nclustering_obs &lt;- transitivity(g_obs, type = \"global\")\ndist_obs &lt;- mean_distance(g_obs, directed = FALSE, unconnected = TRUE)\n\n# Simulate configuration model with the same degree sequence\nset.seed(123)\ng_conf &lt;- sample_degseq(deg_seq, method = \"fast.heur.simple\")\n\n# Compute simulated properties\nclustering_conf &lt;- transitivity(g_conf, type = \"global\")\ndist_conf &lt;- mean_distance(g_conf, directed = FALSE, unconnected = TRUE)\n\n# Degree distribution comparison\ndeg_conf &lt;- degree(g_conf)\ndeg_df &lt;- data.frame(\n  Degree = c(deg_seq, deg_conf),\n  Type = rep(c(\"Observed\", \"Configuration Model\"), times = c(length(deg_seq), length(deg_conf)))\n)\n\n# Summary table\nconf_comparison &lt;- data.frame(\n  Model = c(\"Observed\", \"Configuration Model\"),\n  Clustering = c(clustering_obs, clustering_conf),\n  AvgPathLength = c(dist_obs, dist_conf),\n  MaxDegree = c(max(deg_seq), max(deg_conf))\n)\n\n# Display comparison table with kable\nkable(conf_comparison, caption = \"Comparison of structural features: Observed vs Configuration Model\",\n      digits = 3, align = \"c\")\n\n\n\n\nModel\nClustering\nAvgPathLength\nMaxDegree\n\n\n\n\nObserved\n0.499\n4.861\n36\n\n\nConfiguration Model\n0.239\n2.503\n36\n\n\n\n\n\n\n\n\n\n\n\n\nggplot(deg_df, aes(x = Degree, fill = Type)) +\n  geom_histogram(position = \"identity\", bins = 20, alpha = 0.6, color = \"white\") +\n  facet_wrap(~Type, scales = \"free_y\") +\n  labs(x = \"Node Degree\", y = \"Frequency\") +\n  scale_fill_manual(values = c(\"skyblue\", \"tomato\")) +\n  theme_minimal() +\n  theme(legend.title = element_blank())\n\n\n\n\n\n\n\n\n\n\nFigure 16.3: Comparison of degree distributions for the observed Les Misérables network and a network generated from the configuration model. The configuration model preserves the exact degree sequence of the observed network, resulting in an identical distribution.\n\n\n\nAs expected, the degree distribution of the simulated network matches that of the original exactly. However, when we examine higher-order properties, such as the global clustering coefficient and average path length, we find notable differences. The observed network has significantly more clustering, suggesting the presence of structured triadic closure that is not reproduced by the configuration model’s randomized pairing process. The average path length may also differ, although it often remains in the same general range.\nThese results highlight an important distinction: while the configuration model controls for degree-based features, it does not account for clustering, community structure, or other forms of structural dependency. As such, it is useful as a baseline or null model for testing whether observed patterns can be explained by degree alone.\n\n\n\n\n\n\nNoteNote: Configuration Model vs. CUG Given Degree\n\n\n\nThe configuration model and a Conditional Uniform Graph (CUG) test given degree both generate random networks that preserve the observed degree sequence. In this sense, they are conceptually aligned: both assume that node-level connectivity (i.e., degrees) is fixed and use this constraint to explore how other structural features might arise by chance.\nThe key distinction lies in how each is used. The configuration model is a generative model; it produces random graphs that exactly match a specified degree sequence, often for theoretical or simulation purposes. A CUG test given degree, on the other hand, is a hypothesis testing framework. It evaluates whether a particular network statistic (such as clustering or transitivity) in the observed network is unusually high or low compared to what would be expected under random tie arrangement, given the same degree sequence.\n\n\nThe configuration model preserves the observed node degrees exactly, resulting in a degree distribution identical to that of the real network. This stands in contrast to the \\(G(n, p)\\) model, which smooths the degree distribution into a binomial (or normal) form. However, because tie formation is otherwise random, the configuration model still fails to replicate higher-order structures such as clustering. The comparison table shows that while the configuration model matches degree statistics perfectly, its clustering coefficient and average path length still differ from those of the observed network.\nThis highlights both the strength and the limits of the configuration model. It improves on \\(G(n, p)\\) by controlling for degree heterogeneity but remains structurally random beyond that. In practice, this makes it a useful null model when testing for effects that go beyond degree constraints—especially in the context of CUG tests given degree.",
    "crumbs": [
      "Inferential Network Analysis",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Random Graph Models</span>"
    ]
  },
  {
    "objectID": "inferential/rgm.html#the-small-world-model",
    "href": "inferential/rgm.html#the-small-world-model",
    "title": "16  Random Graph Models",
    "section": "16.4 The Small-World Model",
    "text": "16.4 The Small-World Model\nMany real-world networks exhibit the so-called small-world phenomenon: they have relatively short average path lengths, yet remain highly clustered. This means that although nodes tend to form local groups or neighborhoods, information or influence can still spread efficiently through the network. The small-world model (Watts and Strogatz 1998), was designed to capture this combination of local clustering and global reachability.\nThe model begins with a ring lattice, where each node is connected to a fixed number of its nearest neighbors on either side. This initial structure is highly regular and highly clustered, but has long average path lengths. To introduce randomness and reduce path length, the model rewires each edge with a fixed probability \\(p\\). When \\(p = 0\\), the network remains completely regular. When \\(p = 1\\), all ties are rewired randomly, resulting in a Bernoulli random graph. For intermediate values of \\(p\\) (e.g., \\(0 &lt; p &lt; 0.5\\)), the resulting networks retain much of their local clustering while dramatically lowering the average path length; a hallmark of small-world structure.\nThis simple rewiring mechanism leads to networks that exhibit both high clustering (like regular lattices) and short path lengths (like random graphs), capturing the structural characteristics observed in many social, biological, and technological networks.\nHowever, the model also comes with some limitations. In its basic form, all nodes have the same degree, which may be unrealistic for empirical networks that exhibit degree heterogeneity. Additionally, for \\(p\\) close to 1, the model becomes essentially equivalent to a Bernoulli random graph, losing the small-world structure.\nDespite these limitations, the small-world model remains a useful generative model for studying how global efficiency and local cohesion can coexist in networks. It also serves as a bridge between purely regular and purely random topologies.\n\nExample: The Small World Model\nTo evaluate how well the small-world model captures structural features of a real network, we simulate a small-world graph using the same number of nodes and approximate average degree as the Les Misérables co-appearance network. We then compare the simulated graph to the observed one in terms of degree distribution, clustering, and average path length.\nThe simulation uses igraph::sample_smallworld(), which generates a Watts–Strogatz small-world graph by starting from a regular ring lattice and randomly rewiring edges with a given probability \\(p\\). We set \\(p = 0.05\\) to introduce moderate randomness while maintaining local structure (we discuss the choice of \\(p\\) in more detail below).\n\n\n\nTable 16.3: Comparison of structural features: Observed vs Small-World Model\n\n\n\navg_deg_obs &lt;- mean(deg_obs)\nk &lt;- round(avg_deg_obs / 2)  # average degree per side for ring lattice\n\n# Simulate small-world graph\nset.seed(123)\ng_sw &lt;- sample_smallworld(dim = 1, size = n, nei = k, p = 0.05)\n\n# Compute properties\nclustering_obs &lt;- transitivity(g_obs, type = \"global\")\nclustering_sw &lt;- transitivity(g_sw, type = \"global\")\n\ndist_obs &lt;- mean_distance(g_obs, directed = FALSE, unconnected = TRUE)\ndist_sw &lt;- mean_distance(g_sw, directed = FALSE, unconnected = TRUE)\n\ndeg_sw &lt;- degree(g_sw)\n\n# Comparison table\nsw_comparison &lt;- data.frame(\n  Model = c(\"Observed\", \"Small-World\"),\n  Clustering = c(clustering_obs, clustering_sw),\n  AvgPathLength = c(dist_obs, dist_sw),\n  MaxDegree = c(max(deg_obs), max(deg_sw))\n)\n\n# Print formatted table\nkable(sw_comparison, caption = \"Comparison of structural features: Observed vs Small-World Model\",\n      digits = 3, align = \"c\")\n\n\n\n\nModel\nClustering\nAvgPathLength\nMaxDegree\n\n\n\n\nObserved\n0.499\n4.861\n36\n\n\nSmall-World\n0.498\n3.778\n7\n\n\n\n\n\n\n\n\n\n\n\n\n# Degree distribution\ndeg_df &lt;- data.frame(\n  Degree = c(deg_sw, deg_obs),\n  Type = rep(c(\"Small-World\",\"Observed\"), times = c(length(deg_obs), length(deg_sw)))\n)\n # Reverse the factor levels\ndeg_df$Type &lt;- factor(deg_df$Type, levels = c(\"Small-World\", \"Observed\"))\n\nggplot(deg_df, aes(x = Degree, fill = Type)) +\n  geom_histogram(position = \"identity\", bins = 20, alpha = 0.6, color = \"white\") +\n  facet_wrap(~Type, scales = \"free_y\") +\n  labs(x = \"Node Degree\", y = \"Frequency\") +\n  scale_fill_manual(values = c(\"skyblue\",\"tomato\")) +\n  theme_minimal() +\n  theme(legend.title = element_blank())\n\n\n\n\n\n\n\n\n\n\nFigure 16.4: Degree distributions for the observed Les Misérables network and the simulated small-world model. The observed network displays a more skewed distribution, while the small-world model produces a narrow, symmetric range centered around the average degree.\n\n\n\nThe simulation summarized in Table 16.3 demonstrates how the small-world model approximates certain properties of the observed network. As shown in the table, the simulated network achieves a relatively short average path length, similar to that of the Les Misérables network, due to the introduction of random long-range ties. The clustering coefficient remains substantial, reflecting the model’s ability to preserve local neighborhood structure.\nHowever, the degree distribution in the small-world model shown in Figure 16.4 remains relatively narrow, with most nodes having degrees close to the average. This limitation highlights that while the small-world model captures some global and local properties, it does not account for degree heterogeneity. The results show that the small-world model offers a useful structural middle ground between regular and fully random graphs but still lacks the full complexity observed in empirical networks.\nNote that the choice of the rewiring probability \\(p\\) in the small-world model is crucial, as it balances regularity and randomness. Small values of \\(p\\) (e.g., between 0.01 and 0.2) are typically chosen to introduce enough randomness to significantly reduce path lengths, while still preserving high clustering. If \\(p\\) is too low, the network remains overly regular; if \\(p\\) is too high, the network behaves like a random graph and loses its local structure. In practice, \\(p\\) is often selected empirically to achieve small-world characteristics (high clustering and short average path length) relative to the number of nodes and degree.\nTo illustrate how the small-world model transitions between regular and random structure, we simulate multiple networks with the same number of nodes as Les Misérables network with varying values of the rewiring probability \\(p\\) and track how two key properties (clustering and average path length) change. This helps identify a “sweet spot” for \\(p\\) where the network retains high clustering but achieves short global paths, capturing the essence of small-world structure. The results are shown in Figure 16.5.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 16.5: Clustering coefficient and average path length as functions of the rewiring probability \\(p\\) in the Watts–Strogatz small-world model. Solid lines show values from simulated networks of the same size and average degree as the Les Misérables network. Dashed horizontal lines indicate the observed values from the actual Les Misérables co-appearance network.\n\n\n\nThe plot in Figure 16.5 shows a sharp transition in network structure as \\(p\\) increases. At \\(p = 0\\), the network is a regular lattice: clustering is high, but average path length is long. As \\(p\\) increases slightly (e.g., \\(p \\approx 0.1\\)), the average path length drops rapidly due to the introduction of long-range shortcuts, while clustering remains relatively high. This intermediate range is where small-world characteristics emerge.\nAs \\(p\\) approaches 1, the network becomes increasingly random (think \\(G(n,p)\\)): clustering drops off, and path length stabilizes at a low level. This demonstrates the trade-off between local cohesion and global efficiency controlled by the rewiring parameter \\(p\\).",
    "crumbs": [
      "Inferential Network Analysis",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Random Graph Models</span>"
    ]
  },
  {
    "objectID": "inferential/rgm.html#beyond-the-basic-random-graph-models",
    "href": "inferential/rgm.html#beyond-the-basic-random-graph-models",
    "title": "16  Random Graph Models",
    "section": "16.5 Beyond the Basic Random Graph Models",
    "text": "16.5 Beyond the Basic Random Graph Models\nThe models we’ve considered so far; \\(G(n, p)\\), the configuration model, and the small-world model, each represent increasingly realistic ways to model network structure. However, even these models have important limitations. Most notably, they assume that ties form independently of one another, or they control for only a narrow set of structural features (such as degree sequence or rewiring probability).\nThese limitations motivate the need for a more flexible and expressive modeling framework, one that allows for dependencies between ties and the simultaneous inclusion of multiple structural features. Exponential random graph models (ERGMs) address this by explicitly modeling the probability of a network as a function of observed local configurations, such as mutual ties, stars, and triangles. Rather than assuming independent tie formation, ERGMs can incorporate structural dependencies like reciprocity, transitivity, or popularity effects, offering a way to formalize and test competing social mechanisms. In doing so, ERGMs extend beyond purely generative models and provide a principled statistical approach for evaluating whether specific network features occur more or less frequently than expected under competing structural assumptions.",
    "crumbs": [
      "Inferential Network Analysis",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Random Graph Models</span>"
    ]
  },
  {
    "objectID": "inferential/rgm.html#references",
    "href": "inferential/rgm.html#references",
    "title": "16  Random Graph Models",
    "section": "References",
    "text": "References\n\n\n\n\nBender, Edward A, and E Rodney Canfield. 1978. “The Asymptotic Number of Labeled Graphs with Given Degree Sequences.” Journal of Combinatorial Theory, Series A 24 (3): 296–307.\n\n\nErdos, Paul, and Alfréd Rényi. 1959. “On Random Graphs i.” Publ. Math. Debrecen 6 (290-297): 18.\n\n\nWatts, Duncan J, and Steven H Strogatz. 1998. “Collective Dynamics of ‘Small-World’networks.” Nature 393 (6684): 440–42.",
    "crumbs": [
      "Inferential Network Analysis",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Random Graph Models</span>"
    ]
  },
  {
    "objectID": "inferential/ergm.html",
    "href": "inferential/ergm.html",
    "title": "17  Exponential Random Graph Models (ERGMs)",
    "section": "",
    "text": "17.1 Packages Needed for this Chapter\nSo far, the models we’ve considered have treated tie formation as mostly random or locally constrained: ties are formed with fixed probabilities (\\(G(n, p)\\)), rewired at random (small-world), or arranged to match node degrees (configuration model). However, real-world networks are shaped by richer and more structured processes. People don’t form ties independently or at random, they’re influenced by patterns like mutual friendships, popularity, shared group membership, and social norms like reciprocity and transitivity. For example, two people who share a common friend may be more likely to form a tie themselves (triadic closure), or individuals may be more likely to reciprocate a connection that is initiated by someone else. These kinds of dependencies are central to many social theories, and they give rise to complex patterns of self-organization in networks.\nTo capture these processes, we need a more flexible modeling framework—one that allows for:\nThis leads us to models that are built around network configurations, i.e. small, interpretable patterns of ties (such as mutual ties, stars, and triangles) that collectively describe the structure of the network. These configurations are often nested, meaning they build on one another hierarchically, and they can represent competing explanations for the observed structure. For instance, both reciprocity and popularity could explain why a node has many incoming tie, but these explanations imply different underlying processes.\nExponential Random Graph Models (ERGMs) provide a flexible way to model these kinds of patterns. Instead of assuming each edge forms independently, ERGMs allow the probability of a tie to depend on what else is happening in the network. For example, the likelihood of a tie forming between two nodes may increase if they share mutual connections (triadic closure), or decrease if one node is already connected to many others (crowding out).\nBy explicitly modeling such configurations, we gain not only better empirical fit but also the ability to test theoretical hypotheses about the generative processes that shape real-world networks.\nlibrary(statnet)\nlibrary(intergraph)\nlibrary(igraph)\nlibrary(ggraph)\nlibrary(graphlayouts)\nlibrary(networkdata)\nlibrary(tidyverse)",
    "crumbs": [
      "Inferential Network Analysis",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Exponential Random Graph Models (ERGMs)</span>"
    ]
  },
  {
    "objectID": "inferential/ergm.html#ergm-modeling-outline",
    "href": "inferential/ergm.html#ergm-modeling-outline",
    "title": "17  Exponential Random Graph Models (ERGMs)",
    "section": "17.2 ERGM Modeling Outline",
    "text": "17.2 ERGM Modeling Outline\nExponential Random Graph Models (ERGMs) are a powerful family of statistical models for analyzing and simulating social networks. Unlike simpler random graph models, ERGMs are designed to capture the complex dependencies that often characterize real-world networks, such as reciprocity, transitivity, or clustering—by explicitly modeling how the presence of one tie can affect the likelihood of others.\nThe core idea is as follows. ERGMs specify a probability distribution over the set of all possible networks with a given number of nodes. The probability of observing a particular graph \\(G\\) is defined as:\n\\[\nP(G) = \\frac{1}{\\kappa} \\exp\\left( \\sum_{i=1}^{p} \\theta_i \\cdot s_i(G) \\right)\n\\] where\n\n\\(s_i(G)\\) is a network statistic that counts a specific configuration (e.g., number of edges, mutual ties, triangles).\n\\(\\theta_i\\) is the parameter associated with statistic \\(s_i(G)\\); it tells us how strongly that configuration influences tie formation.\n\\(\\kappa\\) is a normalizing constant ensuring that all possible graphs sum to a probability of 1: \\[\n\\kappa = \\sum_{G'} \\exp\\left( \\sum_{i=1}^{p} \\theta_i \\cdot s_i(G') \\right)\n\\]\n\nThis formulation makes clear that networks with more of the structures positively weighted by \\(\\theta\\) are more probable. For instance, a large positive \\(\\theta_{\\text{mutual}}\\) implies that the observed network contains more reciprocated ties than expected by chance.\nIn general, we interpret the parameters as follows:\n\nA positive \\(\\theta_i\\) means that the corresponding configuration (e.g., triangles) is overrepresented in the observed network than what is expected by pure chance.\nA negative \\(\\theta_i\\) indicates that the configuration is underrepresented than what is expected by pure chance.\nA zero value implies that the configuration occurs at chance levels.\n\nThe statistics \\(s_i\\) are network configurations. As mentioned, ERGMs rely on small, interpretable network configurations to explain structure. These include:\n\nEdges: baseline tendency for tie formation (controls overall density)\nMutual: reciprocity in directed networks\nTriangles: clustering or transitivity (friends of friends)\nStars: centralization or popularity (many ties to a node)\nHomophily: ties between nodes with similar attributes\n\nEach of these configurations reflects a hypothesized mechanism that may drive the evolution of network structure. We formalize some of the standard statistics in the following.\nIn the following section, we will move through four generations of dependence assumptions yielding different ERGM specifications with statistics (network configurations) that are included in the model.\n\n\n\n\n\n\nTipSpecifying an ERGM in R\n\n\n\nAn ERGM is specified using a formula that defines the network structure and any covariates to be included. The ergm() function takes a network object on the left-hand side and a set of model terms on the right-hand side.\nHere is an example model including edge density, a node-level covariate, homophily on an attribute, and a structural dependency term (gwesp):\n\nergm(network_object ~ model_terms)\n\nwhere model_terms represent structural configurations or covariate effects, e.g., edges, triangles, mutual, etc. For a full list of available statistics in the ergm package you can check:\n\n?ergmTerm\n\n\n\n\n17.2.1 Model Specification\n\n1. Dyadic Independence: Edges and Homophily\nTo begin working with exponential random graph models (ERGMs), we first need to understand how models are specified and what kinds of network structures they describe. In this section, we introduce the simplest possible ERGM, one that assumes Bernoulli dependence, and show how it connects to the familiar \\(G(n, p)\\) model. This first step lays the foundation for more complex ERGMs that incorporate structural dependencies.\nThe Bernoulli ERGM assumes that all possible edges form independently of one another. In other words, the probability that a tie exists between nodes \\(u\\) and \\(v\\) is unrelated to whether other ties are present elsewhere in the network. This is a very strong assumption—and one that is often unrealistic in practice—but it is useful as a baseline.\nLet \\(Y_{uv}\\) be a binary random variable indicating the presence (\\(1\\)) or absence (\\(0\\)) of a tie between nodes \\(u\\) and \\(v\\). The full graph \\(G\\) can then be represented by its adjacency matrix \\(Y\\), where \\(Y_{uv}\\) encodes the tie between \\(u\\) and \\(v\\).\nIn the general formulation of an exponential random graph model, one could imagine assigning a different parameter \\(\\theta_{uv}\\) to every possible tie \\((u, v)\\) in the network. This would allow the model to express highly specific tendencies for each dyad. The probability of observing a network \\(G\\) under this formulation would be:\n\\[\nP(G) = \\frac{1}{\\kappa} \\exp\\left( \\sum_{u&lt;v} \\theta_{uv} y_{uv} \\right)\n\\]\nHowever, this model quickly becomes infeasible because it would require estimating a separate parameter for every possible edge—a number that grows quadratically with the number of nodes.\nTo make the model estimable and interpretable, ERGMs rely on the homogeneity assumption: the effect of a given configuration (such as an edge, triangle, or mutual tie) is the same wherever it appears in the network. That is, all instances of the same configuration share a single global parameter. For the Bernoulli ERGM, this means:\n\\[\n\\theta_{uv} = \\theta \\quad \\text{for all } (u, v)\n\\]\nApplying this assumption simplifies the model considerably:\n\\[\nP(G) = \\frac{1}{\\kappa} \\exp\\left( \\theta \\sum_{u&lt;v} y_{uv} \\right) = \\frac{1}{\\kappa} \\exp(\\theta \\cdot L)\n\\]\nHere, \\(L\\) is the total number of edges in the network, \\(\\theta\\) is a single parameter that governs the overall probability of tie formation, and \\(\\kappa\\) is the normalizing constant ensuring all probabilities sum to 1.\nThis structure implies that each potential tie forms independently with the same probability, much like in a \\(G(n, p)\\) random graph. Here, the parameter \\(\\theta\\) governs the log-odds of a tie between any two nodes. More formally, the relationship between \\(\\theta\\) and the tie probability \\(p\\) (i.e., the density of the network) is:\n\\[\n\\theta = \\log\\left( \\frac{p}{1 - p} \\right) \\quad \\Leftrightarrow \\quad p = \\frac{e^\\theta}{1 + e^\\theta}\n\\]\nThis means that:\n\nWhen \\(\\theta = 0\\), the expected density is \\(p\\) = 0.5\nWhen \\(\\theta &lt; 0\\), the expected density is less than 0.5 (a sparse network)\nWhen \\(\\theta &gt; 0\\), the expected density is greater than 0.5\n\nSo, the edge parameter directly controls the overall density of the network. In practice, real-world social networks tend to be sparse, so \\(\\theta\\) is typically negative. This relationship also means that if you estimate a Bernoulli ERGM and obtain an edge coefficient \\(\\hat{\\theta}\\), you can compute the implied expected density using:\n\\[\n\\hat{p} = \\frac{e^{\\hat{\\theta}}}{1 + e^{\\hat{\\theta}}}\n\\]\nThis makes the edge parameter easy to interpret: it is just the logit-transformed density.\n\n\n\n\n\n\nNoteNote: \\(G(n, p)\\) as a Special Case of ERGM\n\n\n\nThe Bernoulli ERGM is structurally identical to the \\(G(n, p)\\) model, with the relationship:\n\n\\(\\theta = 0\\) → expected density \\(p = 0.5\\)\n\\(\\theta &gt; 0\\) → expected density \\(p &gt; 0.5\\)\n\\(\\theta &lt; 0\\) → expected density \\(p &lt; 0.5\\)\n\nHowever, this does not hold in general (if the ERGM contains other statistics).\nPut differently, the \\(G(n, p)\\) random graph model is actually a special case of an ERGM. It includes only one term: the number of edges.\n\n\nIn ERGMs, a parameter estimate is considered statistically significant (by convention) if the absolute value of the estimate is greater than approximately twice its standard error (i.e., \\(|\\hat{\\theta}| &gt; 2 \\cdot \\text{SE}\\)). This rule of thumb suggests that the corresponding network feature (such as homophily or reciprocity) is unlikely to have arisen by chance, and is therefore meaningfully associated with the observed pattern of tie formation.\n\n\nExample: Coleman Friendship\nTo demonstrate the Bernoulli ERGM, we fit the model to the Coleman high school friendship network used in Chapter 15. However, here we convert ther graph object into an adjacency matrix since the ergm response argument needs to be a network object or a matrix that can be coerced to a network object.\n\n# Load data as graph object from networkdata and extract the fall network\ncoleman_g &lt;- networkdata::coleman[[1]]\n\n# Convert to adjacency matrix\ncoleman_mat &lt;- as_adjacency_matrix(coleman_g, sparse = FALSE)\n\n# Fit Bernoulli ERGM with only the edge term\nmodel_bern &lt;- ergm(coleman_mat ~ edges)\nsummary(model_bern)\n\nCall:\nergm(formula = coleman_mat ~ edges)\n\nMaximum Likelihood Results:\n\n      Estimate Std. Error MCMC % z value Pr(&gt;|z|)    \nedges -3.02673    0.06569      0  -46.08   &lt;1e-04 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n     Null Deviance: 7286  on 5256  degrees of freedom\n Residual Deviance: 1969  on 5255  degrees of freedom\n \nAIC: 1971  BIC: 1977  (Smaller is better. MC Std. Err. = 0)\n\n\nThe output provides the estimated \\(\\theta\\) parameter for the edge term. A strongly negative value suggests that the probability of a tie between any two students is low, which is typical of real-world social networks that are sparse.\nThe estimated edge parameter from the Bernoulli ERGM is: \\[\n\\hat{\\theta}_{\\text{edges}} = -3.02673\n\\]\nThis value reflects the log-odds of a tie forming between any two nodes in the network. To convert this into an expected tie probability (i.e., network density), we apply the inverse logit function:\n\\[\np = \\frac{e^{\\hat{\\theta}}}{1 + e^{\\hat{\\theta}}}\n\\]\nSubstituting the estimated value:\n\\[\np = \\frac{e^{-3.02673}}{1 + e^{-3.02673}} \\approx 0.046\n\\]\nThis result implies that the expected density of the network under this model is approximately 4.6%. This is in fact the density of the observed network which you can verify:\n\nedge_density(coleman_g)\n\n[1] 0.04623288\n\n\nAnother class of network statistics in ERGMs captures homophily: the tendency of individuals to form ties with others who are similar to themselves in a particular attribute, such as gender, age, race, or occupation.\nLet each node in the network have a categorical attribute, denoted:\n\\[\na : V \\rightarrow \\{1, \\dots, c\\}\n\\]\nThis maps each actor to one of \\(c\\) categories. To model homophily, we define a statistic that counts the number of ties between actors who share the same attribute value:\n\\[\nm_a(G) = \\left| \\left\\{ \\{u, v\\} \\in E : a(u) = a(v) \\right\\} \\right|\n\\]\nThat is, \\(m_a(G)\\) is the number of edges where both endpoints belong to the same category. Adding this statistic to the ERGM, the model becomes:\n\\[\nP(G) = \\frac{1}{\\kappa} \\exp\\left( \\theta_1 \\cdot L + \\theta_2 \\cdot m_a(G) \\right)\n\\]\nWe interpret the homophily parameter as follows:\n\n\\(\\theta_2 &gt; 0\\): Ties between similar nodes are more likely (homophily)\n\\(\\theta_2 &lt; 0\\): Ties between dissimilar nodes are more likely (heterophily)\n\\(\\theta_2 = 0\\): No effect of similarity\n\nThis formulation assumes that the tendency for similarity-based tie formation is uniform across all dyads, consistent with the homogeneity assumption discussed earlier. Moreover, since the statistic depends only on the attributes of the two actors in a dyad and not on any other ties in the network, it also maintains dyadic independence. That is, the presence or absence of one tie does not influence the probability of others, unless additional structural terms (like triangles or mutual ties) are included in the model.\nThus, including a homophily term allows us to model nodal covariate effects; how actor characteristics affect tie formation—while keeping the model simple and tractable.\n\n\nExample: Teenage Friends and Lifestyle Study data\nFor an example including the homophily statistic, we use the the “Teenage Friends and Lifestyle Study” data called s50 in the networkdata package. The s50 dataset is a subset of 50 pupils over a three-year panel study of adolescent friendship networks and health behaviors conducted in a school in the West of Scotland (West and Sweeting 1996). This excerpt was created for illustrative purposes and includes dynamic friendship data over three waves, capturing changes in social ties alongside attributes like gender, sport participation, and substance use. We will focus on the homophily based on smoking behavior.\nWe only consider cross-sectional network data here so we only focus on the third wave network (we will in later chapters look at modeling longitudinal network data). Let’s prepare the data for the ergm specification. Note that we here instead convert the graph object inot a network one using the package integraph in order to preserve the node attribute of interest\n\n# Load data as graph object from networkdata and extract the third wave network\ns50_g &lt;- networkdata::s50[[3]]\n\n# Convert to network object using intergraph\ns50_net &lt;- asNetwork(s50_g)\n\n# check network obejct and stored attributes\ns50_net\n\n Network attributes:\n  vertices = 50 \n  directed = FALSE \n  hyper = FALSE \n  loops = FALSE \n  multiple = FALSE \n  bipartite = FALSE \n  total edges= 77 \n    missing edges= 0 \n    non-missing edges= 77 \n\n Vertex attribute names: \n    smoke vertex.names \n\nNo edge attributes\n\n\nWe now test whether students tend to form friendships with others who have the same smoking behavior. The s50 dataset includes a categorical node attribute called smoke, with three levels: non-smoker (1), occasional smoker (2), and regular smoker (3). To test for smoking-based homophily, we fit an ERGM that includes an edge term and a nodematch(\"smoke\") term.\n\n# Fit ERGM with homophily on smoking status\nmodel_smoke &lt;- ergm(s50_net ~ edges + nodematch(\"smoke\"))\nsummary(model_smoke)\n\nCall:\nergm(formula = s50_net ~ edges + nodematch(\"smoke\"))\n\nMaximum Likelihood Results:\n\n                Estimate Std. Error MCMC % z value Pr(&gt;|z|)    \nedges            -3.0196     0.1902      0 -15.878   &lt;1e-04 ***\nnodematch.smoke   0.5736     0.2425      0   2.365    0.018 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n     Null Deviance: 1698.2  on 1225  degrees of freedom\n Residual Deviance:  569.4  on 1223  degrees of freedom\n \nAIC: 573.4  BIC: 583.6  (Smaller is better. MC Std. Err. = 0)\n\n\nThe model includes two terms:\n\nedges: controls for the overall density of the network (should always be included)\nnodematch(\"smoke\"): counts the number of ties where both nodes have the same smoking status.\n\nThis specification assumes that the tendency to form ties is homogeneous across dyads, and the homophily term preserves dyadic independence, as it depends only on the attributes of the two individuals involved in each potential tie.\nWe interpret the output as follows. The edges term gives the baseline log-odds of a tie between two students, regardless of smoking behavior. The nodematch(“smoke”) term estimates the additional log-odds of a tie when two students share the same smoking status.\nSince the coefficient for nodematch(\"smoke\") is positive and significant, we see that students tend to form ties with peers who have similar smoking habits (homophily).\n\n\n\n\n\n\nNoteNote on nodematch()\n\n\n\nNote that nodematch(\"smoke\") adds one statistic to the model: the total number of edges connecting nodes that share the same value of the smoke attribute. It does not distinguish between which category is shared, it treats all matches equally. So in our case, it would count:\n\nTies between two non-smokers\nTies between two occasional smokers\nTies between two regular smokers\n\nAll of those contribute to the same term. It does not tell you whether one group is more homophilous than another. If we want more detail, we can instead use nodemix(\"smoke\") as it can separate estimates for each combination of categories (e.g., smoker-smoker, smoker-non-smoker, etc.), that is you get detailed between-group dynamics.\n\n\n\n\n2. Dyadic Dependence: Reciprocity\nIn ERGMs, we can explicitly model dyadic dependence, that is, the statistical relationship between the presence or absence of ties within pairs of nodes (dyads). This is particularly important in directed networks, where the direction of a tie from node \\(u\\) to node \\(v\\) may be statistically related to the reverse tie from \\(v\\) to \\(u\\).\nTo capture this, we include a reciprocity term in the model specification. This term counts the number of mutually connected dyads* i.e., pairs where both \\(y_{uv} = 1\\) and \\(y_{vu} = 1\\). Including this term allows the model to account for the observed tendency in many social networks for actors to reciprocate ties—such as returning a favor, replying to a message, or forming a mutual friendship.\nAn ERGM with both an edge and a reciprocity statistic is specified as:\n\\(P(G) = \\frac{1}{\\kappa} \\exp\\left(\\theta_1 \\cdot L + \\theta_2 \\cdot R\\right)\\)\nwhere \\(L = \\sum_{u \\neq v} y_{uv}\\) is the total number of directed ties (edges) and \\(R = \\sum_{u \\neq v} y_{uv} \\cdot y_{vu}\\).\nA positive value for \\(\\theta_2\\) indicates a tendency toward reciprocation, while a negative value suggests an avoidance of mutual ties.\n\nExample: Coleman Friendship\nWe use the coleman data from earlier to specify an ERGM including reciprocity as statistic.The fitted ERGM includes two terms: a baseline edges term and a mutual term that captures reciprocity (i.e., the tendency for ties to be reciprocated):\n\n# Fit ERGM with reciprocity term \nmodel_rec &lt;- ergm(coleman_mat ~ edges + mutual)\nsummary(model_rec)\n\nCall:\nergm(formula = coleman_mat ~ edges + mutual)\n\nMonte Carlo Maximum Likelihood Results:\n\n       Estimate Std. Error MCMC % z value Pr(&gt;|z|)    \nedges  -3.72240    0.09533      0  -39.05   &lt;1e-04 ***\nmutual  3.77117    0.22453      0   16.80   &lt;1e-04 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n     Null Deviance: 7286  on 5256  degrees of freedom\n Residual Deviance: 1712  on 5254  degrees of freedom\n \nAIC: 1716  BIC: 1729  (Smaller is better. MC Std. Err. = 1.338)\n\n\nThe edge term has a strong negative estimate (\\(\\hat{\\theta}_{\\text{edges}} = -3.72\\), \\(p &lt; 0.0001\\)), indicating that ties are generally infrequent in the network. This negative coefficient reflects the baseline log-odds of a directed tie existing between two nodes in the absence of any reciprocation or other structural tendencies.\nIn contrast, the mutual term has a strong positive estimate (\\(\\hat{\\theta}_{\\text{mutual}} = 3.77\\), \\(p &lt; 0.0001\\)), which suggests a highly significant tendency for reciprocated ties. In other words, when a tie exists from node \\(u\\) to node \\(v\\), the likelihood that node \\(u\\) also ties back to node \\(v\\) is much greater than would be expected by chance, controlling for overall network sparsity.\nThis combination of results suggests that while directed ties are rare in general, when ties do form, they are very likely to be mutual; a common feature in social networks. Overall, this ERGM confirms that dyadic dependence in the form of reciprocity is a defining feature of the Coleman data network.\n\n\n\n3. Markov Dependence: Triangles and Stars\nMarkov random graphs are a foundational class of models in network statistics, based on the idea that the presence or absence of a tie between two nodes depends only on ties that are “locally adjacent”—specifically, those that share a node. This assumption, known as Markov dependence, implies that edges are conditionally independent unless they share a common node. This implies that the presence or absence of a tie between two nodes may influence other ties in the network, but only if those ties are adjacent. This locality of dependence makes the model tractable while capturing meaningful structural effects.\nFormally introduced by Frank and Strauss (1986), Markov random graphs were inspired by analogous models in spatial statistics, e.g., Besag’s auto-logistic models which similarly use local dependencies to define global probability distributions (Besag 1972). The key insight is that network ties can exhibit local dependencies—like clustering or centralization—that can be modeled using well-defined building blocks.\nUnder Markov dependence, ERGMs are typically specified using edges, \\(k\\)-stars, and triangle terms, since these configurations reflect dependencies between adjacent ties. Examples of such configurations are shown in Figure 17.1.\n\n\n\n\n\n\nFigure 17.1: Common network configurations used in Markov-dependent ERGMs. The top row shows star terms (edges, 2-star, 3-star, etc.) capturing degree centralization, while the triangle term reflects triadic closure.\n\n\n\nA triangle occurs when three nodes are all pairwise connected. In undirected networks, this represents a basic unit of triadic closure; the idea that “a friend of a friend is likely to become a friend.” In ERGM terms, a triangle statistic is defined as:\n\\(T = \\sum_{u &lt; v &lt; w} y_{uv} \\cdot y_{uw} \\cdot y_{vw}\\)\nIncluding a triangle term in the model allows us to test for transitivity. A positive coefficient on this term suggests a tendency toward forming closed triads, while a negative coefficient implies avoidance of such closure.\nIn directed networks, we distinguish between:\n\nTransitive triads: if \\(u \\to v\\) and \\(v \\to w\\), then \\(u \\to w\\)\nCyclic triads: if \\(u \\to v\\), \\(v \\to w\\), and \\(w \\to u\\)\n\nEach type can be specified as a separate statistic in an ERGM to assess different triadic dynamics.\nStar configurations capture the tendency for nodes to have many ties, i.e., to be “popular” or “active” in the network. A \\(k\\)-star is a configuration where a single node is connected to \\(k\\) others. The star statistic of order \\(k\\) is:\n\\(S_k = \\sum_u \\sum_{v_1 &lt; \\dots &lt; v_k \\neq u} y_{uv_1} \\cdot \\dots \\cdot y_{uv_k}\\)\nIncluding star terms allows the model to capture degree heterogeneity, reflecting whether some individuals tend to form many more ties than others. Note that in directed networks, we can specify:\n\nOut-stars (activity): a node sending many ties\nIn-stars (popularity): a node receiving many ties\n\nA positive parameter on star terms suggests that nodes with many connections are more likely to gain additional ties—a form of preferential attachment or “rich-get-richer” dynamics.\n\nExample: Florentine Business Network\nTo illustrate the use of Markov-dependent configurations in ERGMs, we fit a model to the Florentine business network, the classic dataset representing marriage and business ties among Renaissance-era families in Florence. Whie rather small, this undirected network contains important social patterns such as clustering and centralization, making it an ideal candidate for modeling local dependence structures such as edges, stars, and triangles.\nThe goal of this example is to assess how these network structures contribute to the overall topology of the network using an ERGM based on Markov dependence.\nFirst we visualize the using by loading the data from the networkdata package to obtain it as graph object and to be able to use ggraph:\n\nflob_g &lt;- networkdata::flo_business\nclass(flob_g)  # Confirms it's a 'graph' class object\n\n[1] \"igraph\"\n\nflob_p &lt;- ggraph(flob_g, layout = \"stress\") + \n  geom_edge_link0(edge_colour = \"#666060\", \n                  edge_width = 0.8, edge_alpha = 1) +\n  geom_node_point(fill = \"#808080\", colour = \"#808080\",  \n                  size = 7, shape = 21, stroke = 0.9) +\n  theme_graph() + \n  theme(legend.position = \"none\") +\n  ggtitle(\"Florentine business network\")\n\nflob_p\n\n\n\n\n\n\n\n\nWe specify an ERGM including:\n\nedges: to capture baseline tie propensity (density),\nkstar(2) and kstar(3): to model degree centralization (nodes connected to 2 or 3 others),\ntriangle: to capture triadic closure (transitivity).\n\nThe data is loaded from the ergm package and stored as a network class object.\n\n# Load the Florentine business network data\ndata(florentine)\n\n# The object is a network object representing business ties\nflob_net &lt;- flobusiness\nclass(flob_net)  # Confirms it's a 'network' class object\n\n[1] \"network\"\n\n# Fit ERGM with Markov dependence terms\nset.seed(77)\nmodel_markov &lt;- ergm(flob_net ~ edges + kstar(2) + kstar(3) + triangle)\n\n\n# Display summary of the model\nsummary(model_markov)\n\nCall:\nergm(formula = flob_net ~ edges + kstar(2) + kstar(3) + triangle)\n\nMonte Carlo Maximum Likelihood Results:\n\n         Estimate Std. Error MCMC % z value Pr(&gt;|z|)    \nedges     -4.3454     1.2406      0  -3.503 0.000461 ***\nkstar2     1.1257     0.7074      0   1.591 0.111535    \nkstar3    -0.6467     0.4120      0  -1.570 0.116440    \ntriangle   1.1618     0.6883      0   1.688 0.091416 .  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n     Null Deviance: 166.36  on 120  degrees of freedom\n Residual Deviance:  79.49  on 116  degrees of freedom\n \nAIC: 87.49  BIC: 98.64  (Smaller is better. MC Std. Err. = 0.2036)\n\n\nThe model output provides coefficients for each configuration:\n\nedges: Significantly negative indicating that ties are relatively sparse overall.\nkstar(2): Not statistically significant suggesting no strong tendency toward moderate centralization (i.e., nodes connected to two others).\nkstar(3): Also not statistically significant meaning the network does not exhibit a clear preference for or against nodes forming many connections (larger stars).\ntriangle: Significantly positive indicating a strong tendency for triadic closure; that is, if two families are both connected to a third, they are more likely to also be connected to each other.\n\nThis suggests the Florentine business network is locally clustered (triadic), but does not support highly centralized hub-like nodes. The network’s structure reflects a preference for mutual interdependence rather than hierarchical dominance.\nThe good news is that by including triangle and star terms in an ERGM, we can move beyond modeling just dyadic interactions and begin to account for local clustering, degree distributions, and complex relational tendencies. The bad news is that they don’t always work.\nWhile Markov random graph models offer a powerful way to represent local dependence through structures like edges, stars, and triangles, they come with a significant challenge: model degeneracy.\nModel degeneracy occurs when an ERGM assigns overwhelming probability to a small set of unrealistic graph configurations—such as the empty graph (no ties) or the complete graph (every possible tie), even though the observed network lies somewhere in between. This typically manifests during estimation as:\n\nExtremely poor convergence of the MCMC algorithm,\nImplausible simulated networks that look nothing like the observed data,\nA breakdown in model fit across key structural statistics (e.g., triad census, degree distribution).\n\nFor example, when fitting a model to the Florentine business network, a model with edges, stars, and triangles may produce simulated networks that deviate substantially from the observed triad census despite having reasonable parameter estimates. This discrepancy arises because certain parameter combinations (especially involving triangles and high-order stars) can push the model into degenerate territory, where the likelihood surface becomes unstable or flat. Thus, careful diagnostic checks are essential when modeling endogenous network structure implying dyadic dependence. We will cover this in more detail in Section 17.2.2.\nThe degeneracy problem highlights a key limitation of Markov dependence: not all local dependence assumptions lead to coherent global models. Particularly when clustering is strong, models relying solely on Markov terms (edges, stars, triangles) can become computationally fragile or even mathematically incoherent. In the next section, we will examine how social circuit dependence offers an alternative approach to modeling complex interdependence while mitigating model degeneracy.\n\n\n\n4. Social Circuit Dependence: GWESP\nIn response to the degeneracy issues inherent in classic Markov ERGMs, researchers have proposed alternative specifications that build more stable dependence structures into the model. One promising approach is based on social circuit dependence Snijders et al. (2006).\nSocial circuit dependence modifies the way tie dependencies are specified in the model. Instead of assuming that two ties are conditionally dependent simply because they share a node (as in Markov dependence), social circuit dependence restricts dependence to occur only when two ties would complete a 4-cycle in the network. Under social circuit dependence, network ties are assumed to self-organize through 4-cycles, i.e., closed paths involving four distinct nodes. Two potential ties are considered conditionally dependent only if they would complete a 4-cycle in the network. In other words, the existence of a tie between \\((u, v)\\) is only dependent on a tie between \\((w, z)\\) if adding both would close such a circuit.\nExamples of configurations based on this depdence assumption are shown in Figure 17.2.\n\n\n\n\n\n\nFigure 17.2: Common configurations in social circuit–based ERGM specifications. The top row illustrates \\(k\\)-paths, which capture extended local connectivity through shared paths of increasing length. The bottom row depicts \\(k\\)-triangles, representing increasing levels of triadic closure.\n\n\n\nThis more constrained form of local dependence avoids the pitfalls of degeneracy by reducing the likelihood that the model overemphasizes clustering or centralization. It reflects more realistic assumptions about how social ties form: not purely through dyadic exchange or triadic closure, but through broader patterns of interconnection.\nThe Geometrically Weighted Edgewise Shared Partner (GWESP) statistic is a commonly used term in ERGMs to capture triadic closure, the tendency for connected nodes to have shared partners, as sown in Figure 17.3. Unlike a simple triangle count, GWESP down-weights the contribution of additional shared partners to help prevent model degeneracy and improve stability.\n\n\n\n\n\n\nFigure 17.3: Visualization of the GWESP (Geometrically Weighted Edgewise Shared Partner) statistic. The tie between nodes a and b is supported by k shared partners, each contributing diminishing weight to the likelihood of that tie being present.\n\n\n\nTo formalize this, we let:\n\n\\(y_{uv} = 1\\) if there is a tie between nodes \\(u\\) and \\(v\\),\n\\(p_{uv}\\) be the number of shared partners between nodes \\(u\\) and \\(v\\) (i.e., nodes \\(w\\) such that \\(y_{uw} = y_{vw} = 1\\)),\n\\(\\alpha\\) be a decay parameter (usually a small positive number, e.g., 0.25).\n\nThen the GWESP statistic is defined as:\n\\[\n\\text{GWESP}(G; \\alpha) = \\sum_{u &lt; v} y_{uv} \\cdot \\left(1 - (1 - e^{-\\alpha})^{p_{uv}} \\right)\n\\]\n\nThis formula sums over all existing ties in the graph.\nFor each tie \\((u, v)\\), it calculates a weighted function of the number of their shared partners.\nThe decay parameter \\(\\alpha\\) controls how quickly the contribution of additional shared partners diminishes.\n\nWhen \\(\\alpha\\) is close to zero, the statistic approaches a simple count of edges with at least one shared partner. When \\(\\alpha\\) is larger, the contribution of each additional shared partner is increasingly discounted.\nGWESP provides a smooth and stable way to model transitivity in networks. It is often used in place of triangle counts to reduce the risk of model degeneracy; where the model produces unrealistic networks concentrated on extremes (e.g., empty or complete graphs).\nIn practice, GWESP is typically included in ERGMs with a fixed decay value using the gwesp() term in ergm::ergm().\nA commonly used value for \\(\\alpha\\) is 0.693, which is approximately \\(\\log(2)\\). This choice has a convenient and interpretable consequence: With \\(\\alpha = \\log(2)\\), each additional shared partner contributes about half as much as the one before. For example, the first shared partner contributes ~50% of the max possible increment, the second adds ~25%, the third adds ~12.5%, and so on. This exponential discounting reflects a realistic assumption: having one or two mutual friends greatly increases the chance of tie formation, but the influence of additional mutual friends diminishes quickly.\n\n\n\n\n\n\nNoteNote on choice of \\(\\alpha\\) in GWESP\n\n\n\nIn some models, it may be beneficial to fix \\(\\alpha\\) to a value based on theory or prior experience (common in applied work), or estimate \\(\\alpha\\) directly from the data (fixed = FALSE in gwesp()), though this can lead to convergence issues or overfitting. In practice, using gwesp(0.693, fixed = TRUE) is often a safe and interpretable starting point.\n\n\nNext we demonstrate how to fit an ERGM using GWESP and interpret the results.\n\nExample: Lawyers Network - Cowork Among Partners\nTo demonstrate how ERGMs capture social circuit dependence, we will use the same subset of this network as earlier: we want to check whether or not the partners of the firm more frequently work together with other partners having the same practice, whilst also including a statistic related to triadic clustering. We import the data as a graph object from the networkdata package:\n\ndata(\"law_cowork\")\n\nWe then create an adjacency matrix from the directed graph for the first 36 lawyers in the network corresponding to the partners of the firm (see attribute ‘status’). Note that we this time create the network object ourselves from the symmetrized adjacency matrix.\nThis example includes the following statistics:\n\nedges: baseline tie probability,\nnodecov(\"practice\"): effect of practice area on tie activity,\nnodematch(\"practice\"): homophily within practice areas,\ngwesp(0.693, fixed = TRUE): transitive closure (triadic clustering).\n\n\n\n\n\n\n\nNoteNote on nodecov(\"...\")\n\n\n\nThe nodecov(\"...\") term in an ERGM includes a node-level covariate effect, where the probability of forming a tie is modeled as a function of the attribute value for each node. Specifically, for an undirected network, it sums the attribute values of both nodes involved in each dyad. A positive coefficient indicates that nodes with higher values on the given attribute are more active in forming ties (i.e., they tend to have higher degree). If you’re modeling a binary categorical attribute (e.g., practice = 0 or 1), then the statistic tests whether being in group 1 (e.g., corporate practice) increases a lawyer’s general tendency to form ties, regardless of whom they connect with.\n\n\nThese reflect both attribute-based processes and structural dependencies consistent with social circuit theory.A positive and significant `gwesp coefficient would support the idea of social circuit dependence, where ties are not just formed dyadically or through attributes, but also through embedded collaboration patterns within the firm’s structure.\nThe code below creates the co-work network object for the 36 partners, adds the practice attribute as a binary variable and fits the ERGM with the above defined statistics:\n\ndata(\"law_cowork\")\n\n# symmetrize the matrix to create and undirected graph\nlaw_mat_cwdir &lt;- as_adjacency_matrix(law_cowork, sparse = FALSE)\nlaw_mat_cwdir &lt;- law_mat_cwdir[1:36,1:36] # only partners\nlaw_mat_cw &lt;- (law_mat_cwdir == t(law_mat_cwdir) & law_mat_cwdir ==1) + 0\n\n# save the binary attribute 'practice' (1 = litigation, 2 = corporate) from the graph object as a vector:\nlaw_attr.pract &lt;- vertex_attr(law_cowork)$pract[1:36] - 1. # recode attribute from 2/1 to 1/0\n\n# We create a network object and add the binary node attribute 'practice':\nlaw_net &lt;- as.network(law_mat_cw, directed = FALSE) \nlaw_net %v% \"practice\" &lt;- law_attr.pract\n\n\n# Fit ERGM with attribute and structural terms\nset.seed(7)\nmodel_sc &lt;- ergm(law_net ~ edges\n                + nodecov(\"practice\") + match(\"practice\")\n                + gwesp(0.693, fixed = TRUE)\n)\n\nsummary(model_sc)\n\nCall:\nergm(formula = law_net ~ edges + nodecov(\"practice\") + match(\"practice\") + \n    gwesp(0.693, fixed = TRUE))\n\nMonte Carlo Maximum Likelihood Results:\n\n                   Estimate Std. Error MCMC % z value Pr(&gt;|z|)    \nedges              -4.41055    0.32152      0 -13.718  &lt; 1e-04 ***\nnodecov.practice    0.17687    0.07541      0   2.345 0.019012 *  \nnodematch.practice  0.60527    0.16972      0   3.566 0.000362 ***\ngwesp.fixed.0.693   1.15376    0.15739      0   7.331  &lt; 1e-04 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n     Null Deviance: 873.4  on 630  degrees of freedom\n Residual Deviance: 503.3  on 626  degrees of freedom\n \nAIC: 511.3  BIC: 529.1  (Smaller is better. MC Std. Err. = 0.4067)\n\n\nLet’s interpret the output: The fitted model includes four terms: edges, nodecov(\"practice\"), nodematch(\"practice\"), and gwesp(0.693, fixed = TRUE). Each coefficient represents the log-odds change in the probability of a tie associated with that network statistic, controlling for the others. What do these estimates tell us?\n\nedges (\\(\\hat\\theta = -4.41\\), p &lt; 0.001): Ties are rare overall; the network is sparse.\nnodecov(\"practice\") (\\(\\hat\\theta = 0.18\\), p &lt; 0.05): Lawyers from a given practice area (e.g., corporate) are slightly more likely to form ties overall.\nnodematch(\"practice\") (\\(\\hat\\theta = 0.61\\), p &lt; 0.001): Strong evidence of homophily, lawyers are significantly more likely to collaborate within their own practice area.\ngwesp(0.693) (\\(\\hat\\theta = 1.15\\), p &lt; 0.001): High and significant triadic closure effect, indicating a strong tendency for collaboration among those with shared partners—consistent with social circuit dependence.\n\nTaken together, the results indicate that processes of attribute-related activity, assortative mixing by attribute (homophily), and structural closure (via triadic dependence) operate concurrently in shaping tie formation within the Lazega co-working network.\n\n\n\n\n17.2.2 Model Estimation\nEstimating the parameters of an Exponential Random Graph Model (ERGM) involves finding the parameter vector \\(\\boldsymbol{\\theta}\\) that maximizes the likelihood of observing the network \\(G_{\\text{obs}}\\) under the specified model:\n\\[\n\\hat{\\boldsymbol{\\theta}} = \\arg\\max_{\\boldsymbol{\\theta}} \\, P(G_{\\text{obs}} \\mid \\boldsymbol{\\theta}) = \\frac{\\exp\\left( \\boldsymbol{\\theta}^\\top \\mathbf{s}(G_{\\text{obs}}) \\right)}{\\kappa(\\boldsymbol{\\theta})}\n\\]\nHere, \\(\\mathbf{s}(G)\\) is a vector of network statistics (e.g., number of edges, mutual ties, triangles), and \\(\\kappa(\\boldsymbol{\\theta})\\) is a normalizing constant that sums the exponential terms over all possible networks with the same number of nodes:\n\\[\n\\kappa(\\boldsymbol{\\theta}) = \\sum_{G'} \\exp\\left( \\boldsymbol{\\theta}^\\top \\mathbf{s}(G') \\right)\n\\]\nThis computation is analytically intractable for all but the smallest networks due to the exponential number of possible configurations.\nTo overcome this, ERGMs use Markov Chain Monte Carlo Maximum Likelihood Estimation (MCMCMLE). The idea is to simulate many networks from the current estimate of the model and compare their statistics to those of the observed network. The process iteratively updates the parameter estimates so that the simulated statistics converge toward the observed ones.\nThe estimation process, as implemented in the ergm package, proceeds as follows:\n\nInitial Parameter Estimation (MPLE):\nThe process begins with a Maximum Pseudo-Likelihood Estimate (MPLE), which provides a fast but approximate estimate of \\(\\boldsymbol{\\theta}\\). This serves as the starting point for simulation.\nMCMC Simulation:\nGiven the current parameter estimates, networks are simulated from the model using Markov Chain Monte Carlo (MCMC) methods; typically a form of Metropolis-Hastings sampling.\nLikelihood Approximation and Parameter Updating:\nThe simulated networks are used to approximate the log-likelihood gradient, which informs updates to \\(\\boldsymbol{\\theta}\\). These updates move the model toward parameter values that better reproduce the observed network statistics.\nIteration:\nSteps 2 and 3 are repeated iteratively. At each round, the model simulates networks, compares the simulated statistics to the observed ones, and updates the parameters. The goal is to minimize the difference between observed and expected statistics under the model.\n\nThis process continues until convergence is reached, i.e., when the simulated network statistics closely match those of the observed network and parameter updates stabilize.\n\n\n\n\n\n\nNoteNote on Metropolis-Hastings sampling\n\n\n\nIn the ergm package, the Metropolis–Hastings (MH) algorithm is used for MCMC simulation. It evaluates how a proposed tie change affects the network statistics (e.g., edges, triangles) and adjusts the likelihood ratio accordingly. This makes MH well-suited for models with dyad dependence, where toggling one tie may alter the configuration of multiple others. Specifically, the algorithm works as follows:\n\nPropose a change to the current network state, typically by randomly toggling a tie (i.e., adding or removing an edge between two nodes).\nCompute an acceptance probability, based on the change in the model’s sufficient statistics and the corresponding change in likelihood.\nAccept or reject the proposed change with that probability, ensuring the Markov chain samples from the correct distribution.\n\nOver many iterations, this process produces a sample of networks from the ERGM’s specified distribution, which is then used for parameter estimation and simulation.\n\n\nIn summary, the simulation-based procedure involves two key steps:\n\nMCMC Sampling: Generate random networks from the current parameter estimates using a Markov chain.\nScore Updating: Adjust the parameter values to reduce the difference between simulated and observed network statistics.\n\nBecause ERGMs rely on MCMC simulation to estimate model parameters, it is important to check whether the estimation process has properly converged. MCMC diagnostics help assess whether the Markov chain has explored the space of possible networks sufficiently and whether the simulated networks reflect a stable distribution. Poor convergence can lead to unreliable parameter estimates and misleading inferences. Key diagnostics include\n\ntrace plots (to check stability of simulated statistics over time)\nautocorrelation plots (to assess dependence between samples)\nand comparisons between observed and simulated statistics.\n\nA well-fitting model should not only reproduce the observed statistics on average but also generate networks with similar variability and structure. Running these diagnostics ensures that conclusions drawn from the model are based on a valid and stable estimation process.\nWhile MCMC diagnostics are useful in general, they are especially critical when estimating dyad-dependent ERGMs; models in which the probability of a tie depends on the presence or absence of other ties in the network. This includes models with terms like triangles, \\(k\\)-stars, or other structural dependencies. In these cases, the network space becomes highly constrained and interdependent, making it more challenging for the Markov chain to explore effectively. As a result, convergence issues are more common, and MCMC diagnostics play a key role in ensuring that the simulation-based estimation has produced reliable parameter values.\nFor dyad-independent models (e.g., those using only nodal covariates like homophily), estimation is generally simpler and may not require full MCMC sampling or diagnostics. You can actually think of the dyad-independent ERGM as the mathematically equivalent to a logistic regression model applied to all possible dyads in the network. In fact, the ergm package uses maximum pseudo-likelihood estimation (MPLE) for dyad-independent models, which is logistic regression under the hood.\n\nModel Instability and Degeneracy\nWhile MCMC-based estimation enables flexible ERGM modeling, it also introduces potential pitfalls. In particular, unstable ERGMs can give rise to multi-modal probability distributions over the space of possible networks, where most of the probability mass concentrates on a small and unrealistic subset of configurations.\nThis phenomenon is known as model degeneracy. A degenerate ERGM assigns near-zero probability to most networks in the model space, and high probability only to extreme graphs, such as the completely empty or fully connected one. Since real social networks rarely resemble either of these extremes, such a distribution indicates a serious modeling failure.\nSigns of degeneracy include:\n\nSimulated networks that are always empty or complete, regardless of the observed data.\nPoor convergence of MCMC chains, or failure to reach a stable equilibrium (stationary distribution).\nLarge gaps between observed and simulated statistics, especially for higher-order configurations like triangles.\n\nMore technically, MCMC simulation is particularly vulnerable to problems like instability and degeneracy when:\n\nBurn-in period is too short: The burn-in period refers to the initial phase of MCMC sampling during which the Markov chain has not yet stabilized. Early samples may be heavily influenced by the starting values and not representative of the true model distribution. If this phase is too short, those unrepresentative samples may skew the estimation.\nPost-burn-in sampling is too sparse: After burn-in, we collect samples intended to represent the stationary distribution of the model. If too few of these post-burn-in samples are collected, or if they are too highly correlated (e.g., due to short MCMC intervals), the resulting estimates may be unreliable or noisy.\nThe model includes strong dependence terms (e.g., many triangle-based or star-based statistics) without sufficient balancing components such as edge terms or decay-weighted statistics like gwesp(). These terms can make the network space highly constrained and difficult for the MCMC algorithm to explore effectively.\n\n\n\n\n\n\n\nTipRunning MCMC Diagnostics\n\n\n\nAfter fitting an ERGM, use mcmc.diagnostics() to evaluate whether the MCMC estimation has converged and whether the Markov chain has mixed well. This function provides trace plots, density plots, and numerical summaries for each model parameter:\n\n# MCMC convergence check\nmcmc.diagnostics(model)\n\n\n\nWe will consider the diagnostics for the model_sc on the lawyer data set above:\n\nExample: Lawyers Network - Cowork Among Partners\nThe mcmc.diagnostics() function in the ergm package produces trace plots and density plots for each model parameter, allowing us to evaluate the stability and mixing of the MCMC chain. The daignostics consists of two parts. The first part is a plot:\n\n\n\n\nmcmc.diagnostics(model_sc, which = \"plots\")\n\n\n\n\n\n\n\n\n\nNote: MCMC diagnostics shown here are from the last round of\n  simulation, prior to computation of final parameter estimates.\n  Because the final estimates are refinements of those used for this\n  simulation run, these diagnostics may understate model performance.\n  To directly assess the performance of the final model on in-model\n  statistics, please use the GOF command: gof(ergmFitObject,\n  GOF=~model).\n\n\n\n\nFigure 17.4: MCMC trace and density plots for ERGM parameter estimates. The trace plots (left) show the sampled values of the score function across iterations for each model term. Good mixing is indicated by irregular, stationary fluctuations around zero. The density plots (right) display the distribution of these values; approximately symmetric and centered curves suggest that the MCMC sampler has reached a stable equilibrium and is drawing from the target distribution.\n\n\n\nThe MCMC diagnostic output includes two main types of plots as seen in Figure 17.4: trace plots and density plots, which together help assess whether the Markov chain has properly converged.\nThe trace plots (left panels of Figure 17.4) display the sampled values of the score function—essentially, the gradient of the log-likelihood—for each model term across the course of the MCMC simulation. In a well-behaved chain, these traces should appear as irregular but stable fluctuations around zero, without discernible trends, drifts, or flat segments. Such patterns indicate good mixing, where the sampler is exploring the parameter space effectively rather than getting stuck.\nIn the Lazega model, all four trace plots (for edges, nodecov.practice, nodematch.practice, and gwesp.fixed.0.693) exhibit the expected noisy, stable behavior. There are no signs of poor mixing or convergence issues.\nThe density plots (right panels of Figure 17.4) complement this by summarizing the distribution of the sampled scores for each term. A roughly symmetric, bell-shaped curve centered near zero suggests that the chain has reached its stationary distribution. Here too, all four parameters display smooth, unimodal density curves with means close to zero, providing further evidence that the MCMC sampler has stabilized and that the parameter estimates are reliable.\nTogether, these visual diagnostics indicate that the model has converged appropriately and that the MCMC estimation procedure has produced a stable and interpretable result.\nIn addition to the trace and density plots, the mcmc.diagnostics() function provides a set of detailed numerical diagnostics that assess both the stability of the simulated statistics and the convergence behavior of the Markov chain. These diagnostics offer a more granular view of the estimation process and help confirm whether the model has been reliably estimated. We’ll interpret the output in the following.\n\nmcmc.diagnostics(model_sc, which = \"texts\")\n\nSample statistics summary:\n\nIterations = 82944:1648640\nThinning interval = 1024 \nNumber of chains = 1 \nSample size per chain = 1530 \n\n1. Empirical mean and standard deviation for each variable,\n   plus standard error of the mean:\n\n                    Mean    SD Naive SE Time-series SE\nedges              4.193 27.69   0.7079          2.030\nnodecov.practice   3.084 27.03   0.6911          1.699\nnodematch.practice 2.555 17.11   0.4374          1.312\ngwesp.fixed.0.693  7.441 54.95   1.4048          3.982\n\n2. Quantiles for each variable:\n\n                      2.5%    25%   50%   75%  97.5%\nedges               -58.00 -13.00  7.00 24.00  50.00\nnodecov.practice    -62.00 -11.75  7.00 22.00  44.77\nnodematch.practice  -34.77  -7.75  4.00 15.00  32.00\ngwesp.fixed.0.693  -112.95 -27.27 11.86 46.56 105.24\n\n\nAre sample statistics significantly different from observed?\n              edges nodecov.practice nodematch.practice gwesp.fixed.0.693\ndiff.      4.193464       3.08431373         2.55490196        7.44060041\ntest stat. 2.065603       1.81503409         1.94692678        1.86876829\nP-val.     0.038866       0.06951867         0.05154351        0.06165506\n                 (Omni)\ndiff.                NA\ntest stat. 2.018915e+01\nP-val.     5.549297e-04\n\nSample statistics cross-correlations:\n                       edges nodecov.practice nodematch.practice\nedges              1.0000000        0.8661702          0.9385818\nnodecov.practice   0.8661702        1.0000000          0.8058032\nnodematch.practice 0.9385818        0.8058032          1.0000000\ngwesp.fixed.0.693  0.9930583        0.8706234          0.9326873\n                   gwesp.fixed.0.693\nedges                      0.9930583\nnodecov.practice           0.8706234\nnodematch.practice         0.9326873\ngwesp.fixed.0.693          1.0000000\n\nSample statistics auto-correlation:\nChain 1 \n             edges nodecov.practice nodematch.practice gwesp.fixed.0.693\nLag 0    1.0000000        1.0000000          1.0000000         1.0000000\nLag 1024 0.7830828        0.6976898          0.7312736         0.7512523\nLag 2048 0.6230787        0.5053844          0.5691328         0.5927850\nLag 3072 0.5060997        0.3787281          0.4569260         0.4783505\nLag 4096 0.4117891        0.3017643          0.3746105         0.3901042\nLag 5120 0.3397918        0.2345665          0.3080189         0.3204952\n\nSample statistics burn-in diagnostic (Geweke):\nChain 1 \n\nFraction in 1st window = 0.1\nFraction in 2nd window = 0.5 \n\n             edges   nodecov.practice nodematch.practice  gwesp.fixed.0.693 \n          1.183854           1.102967           1.430812           1.202088 \n\nIndividual P-values (lower = worse):\n             edges   nodecov.practice nodematch.practice  gwesp.fixed.0.693 \n         0.2364708          0.2700415          0.1524842          0.2293296 \nJoint P-value (lower = worse):  0.646062 \n\nNote: MCMC diagnostics shown here are from the last round of\n  simulation, prior to computation of final parameter estimates.\n  Because the final estimates are refinements of those used for this\n  simulation run, these diagnostics may understate model performance.\n  To directly assess the performance of the final model on in-model\n  statistics, please use the GOF command: gof(ergmFitObject,\n  GOF=~model).\n\n\nOne key output compares the mean of the simulated statistics to the corresponding observed values. For most terms, including nodecov.practice, nodematch.practice, and the gwesp statistic—there is no significant difference between simulated and observed statistics, indicating a good model fit. The only exception is the edges term, which shows a small but statistically significant deviation (\\(p\\) = 0.0389). This suggests a slight misfit on overall tie density, though the magnitude of the difference is modest and not cause for concern.\nThe cross-correlations between simulated statistics are moderately high but consistent with the expected dependencies among model terms. For example, ties that contribute to homophily (nodematch.practice) also increase the total number of edges, leading to a natural correlation. None of the observed correlations approach levels (e.g., &gt; 0.95) that would indicate redundancy or raise concerns about model identifiability or degeneracy.\nLooking at auto-correlation, we see that values for all parameters decay steadily with increasing lags. This pattern is typical of a well-mixing MCMC chain and indicates that the sampler is not highly autocorrelated, which would reduce efficiency.\nFinally, the Geweke burn-in diagnostic compares the means of the early and late segments of the MCMC chain. All terms yield non-significant \\(p\\)-values (ranging from 0.15 to 0.27), suggesting that the Markov chain has likely reached stationarity. The joint \\(p\\)-value of 0.64 further supports this conclusion.\nTaken together, these diagnostics confirm that the MCMC estimation for the ERGM is well-behaved. The parameter estimates appear stable, the chain has mixed adequately, and the model reproduces key features of the observed network. While the slight misfit on the edges term is noted, it does not compromise the validity of the overall model, which can be interpreted with confidence.\nAs noted, MCMC requires more care and feeding, meaning users often need to tune control parameters, assess diagnostics, and sometimes revise the model specification to ensure convergence and realism. Practically, this may involve:\n\nIncreasing MCMC.burnin and MCMC.samplesize in control.ergm(),\nReplacing raw triangle or star terms with smoother alternatives (e.g., gwesp() or gwdsp()),\nSimplifying the model to reduce overfitting or internal tension among terms.\n\n\n\n\n\n\n\nNoteNote on MCMC control parameters\n\n\n\nControl parameters for the MCMC algorithm can be found here help(control.ergm)\n\n\nFor a detailed technical treatment of model instability in ERGMs, see Schweinberger (2011). Alternative specification strategies that improve stability are discussed in Snijders et al. (2006), while the formulation and rationale behind the gwesp term, and curved exponential family models more broadly, are addressed in Hunter and Handcock (2006).\n\n\n\n\n17.2.3 Model Fit\nAfter confirming that the MCMC estimation has converged properly (as diagnosed through trace plots, numerical summaries, and Geweke statistics) the next essential step is to assess the fit of the model to the observed network.\nWhile MCMC diagnostics ensure that parameter estimates are stable and reliable, they do not tell us whether the model is substantively adequate, that is, whether it captures the essential structure of the network. For this, we turn to goodness-of-fit (GOF) diagnostics.\nThere are two complementary questions in ERGM model checking:\n\nDid the model reproduce the terms included in the formula?\nThis is effectively ensured by the maximum likelihood estimation procedure. If convergence has been achieved, the model will reproduce the sufficient statistics (e.g., number of edges, triangles) used in the specification.\nDoes the model reproduce broader structural properties of the network?\nThis is the more important and revealing test. GOF checks compare simulated networks to the observed one on key network features not explicitly included in the model. These typically include:\n\nDegree distribution (node-level centrality)\nEdgewise shared partners (ESP) (local clustering)\nGeodesic distances (overall reachability)\n\n\nThis second category helps detect model misspecification, i.e., features of the network that the model fails to account for.\n\n\n\n\n\n\nTipRunning Goodness-of-Fit Checks in ERGM\n\n\n\nThe ergm package provides the gof() function, which performs a simulation-based goodness-of-fit assessment. This function compares the observed network to a distribution of networks simulated from the fitted model, focusing on structural features not directly modeled.\nThe typical workflow is:\n\n# Run GOF assessment\ngof_model &lt;- gof(model)\n\n# View numeric summary\nsummary(gof_model)\n\n# Plot GOF results\nplot(gof_model)\n\n\n\n\nExample: Lawyers Network - Cowork Among Partners\nWe now assess the fit of the ERGM specified for Lazega’s co-working network, which included:\n\nedges: baseline tie probability,\nnodecov(\"practice\"): effect of practice area on tie activity,\nnodematch(\"practice\"): homophily within practice areas,\ngwesp(0.693, fixed = TRUE): transitive closure (triadic clustering).\n\nThe goal is to evaluate whether the model, beyond reproducing the modeled statistics, also captures broader structural features of the observed network.\nWe use the gof() function from the ergm package to simulate networks under the fitted model and compare them to the observed network on several unmodeled statistics:\n\n\n\n\ngof_sc &lt;- gof(model_sc, plotlogodds=TRUE) # this will produce 4 plots\npar(mfrow = c(2, 2)) # figure orientation with 2 rows and 2 columns\nplot(gof_sc)\n\n\n\n\n\n\n\n\n\n\nFigure 17.5: Goodness-of-fit (GOF) diagnostics for the ERGM fitted to the Lazega lawyers co-working network. Each plot compares statistics from networks simulated under the model to those observed in the empirical network (blue diamonds). The top-left panel confirms that the model reproduces the sufficient statistics (model terms). The other panels assess fit to un-modeled structural features: the degree distribution, edgewise shared partners (not used as it is explicitly controlled for in the model), and geodesic distances. The observed values fall mostly within the distribution of simulated networks, indicating that the model captures key features of the network’s local and global structure.\n\n\n\n\n\n\n\n\n\nNoteInterpreting th GOF plots\n\n\n\nIn the GOF plots generated by the ergm package, the meaning of the black line and blue diamond differs slightly depending on the panel:\n\nModel statistics plot:\n\nBlue diamond: The observed value of each modeled statistic in the empirical network.\nBlack line: The mean value of that statistic across the simulated networks.\n\nOther GOF plots (e.g., degree, geodesic distance, ESP):\n\nBlack line: The observed distribution of the statistic (e.g., degree counts) in the empirical network.\nBlue diamonds: The mean of the simulated distributions at each value or bin.\n\n\nGood model fit is indicated when the black line lies within the range of the simulated distributions (shown as boxplots or shaded areas), and aligns closely with the blue diamonds.\n\n\nThis goodness-of-fit check produces plots for several structural features, including both modeled and un-modeled terms. The GOF plots in Figure 17.5 compare statistics from the observed network (black lines or blue diamonds) to distributions of the same statistics across networks simulated from the model:\n\nModel statistics: the sufficient statistics explicitly specified in the model formula (e.g., edges, nodematch, gwesp). This plot shows whether the model reproduces the statistics it was explicitly fit to match. Each boxplot represents the distribution of simulated values for a modeled term (e.g., edges, nodematch.practice, gwesp), and the blue diamond marks the observed value. Good fit is indicated when the diamond lies near the center of the box. This panel essentially checks if maximum likelihood estimation succeeded.\nDegree distribution: the number of ties per node. The model accurately captures the variability in the number of ties per node if the observed degree frequencies fall within the simulated distribution at most points.\nGeodesic distances: shortest paths between nodes. If the model reflects the observed network’s overall connectivity, the distribution of shortest path lengths should align closely with those from the simulations.\nEdgewise shared partners (ESP): the number of common neighbors shared by directly connected pairs of nodes. This panel can typically indicate clustering tendencies. Note that ESP should not be interpreted as a fit criterion in the example shown in Figure 17.5, since it is directly modeled via the gwesp term. Including it in GOF checks may simply reflect that the model was designed to reproduce it.\n\n\n\n\n\n\n\nNoteNote on using log-odds instead\n\n\n\nNormally, GOF plots show the proportion (e.g., of nodes with a given degree) on the y-axis. Using the following and setting plotlogodds = TRUE transforms those proportions to log-odds:\n\\[\n\\text{log-odds}(p) = \\log\\left(\\frac{p}{1 - p}\\right)\n\\] In other words, you instead run gof(model, plotlogodds=TRUE).\nThis is useful when you’re interested in small differences in low-probability bins (e.g., rare degrees or long geodesic distances). Values close to 0 (e.g., rare degrees or high distances) are spread out and more visible. Differences between observed and simulated values in the tails become more pronounced. Plots become less intuitive if you’re unfamiliar with log-odds, so interpret with care.\n\n\nIn addition to the plots shown in Figure 17.5, we can also look at the printed GOF summary:\n\ngof_sc\n\n\nGoodness-of-fit for degree \n\n         obs min mean max MC p-value\ndegree0    2   0 3.42  10       0.90\ndegree1    3   0 2.24   8       0.74\ndegree2    2   0 2.14   8       1.00\ndegree3    4   0 2.72   7       0.66\ndegree4    2   0 2.81   8       1.00\ndegree5    4   0 2.83   7       0.60\ndegree6    4   0 3.25   7       0.88\ndegree7    1   0 2.77   7       0.34\ndegree8    1   0 2.50   8       0.64\ndegree9    5   0 2.83   9       0.34\ndegree10   1   0 2.48   9       0.62\ndegree11   1   0 1.84   5       0.92\ndegree12   2   0 1.39   6       0.70\ndegree13   3   0 1.06   5       0.20\ndegree14   0   0 0.73   4       0.98\ndegree15   1   0 0.37   3       0.58\ndegree16   0   0 0.24   2       1.00\ndegree17   0   0 0.14   3       1.00\ndegree18   0   0 0.16   2       1.00\ndegree19   0   0 0.04   1       1.00\ndegree20   0   0 0.02   1       1.00\ndegree21   0   0 0.02   1       1.00\n\nGoodness-of-fit for edgewise shared partner \n\n      obs min  mean max MC p-value\nesp0    5   0  4.86  11       1.00\nesp1   16   6 16.27  31       0.98\nesp2   29   8 26.01  44       0.62\nesp3   17   8 25.21  41       0.28\nesp4   23   4 19.69  51       0.74\nesp5   11   0 11.76  27       1.00\nesp6   10   0  5.62  20       0.38\nesp7    4   0  2.47  10       0.56\nesp8    0   0  0.87   7       1.00\nesp9    0   0  0.33   3       1.00\nesp10   0   0  0.04   1       1.00\nesp11   0   0  0.04   1       1.00\nesp12   0   0  0.02   1       1.00\n\nGoodness-of-fit for minimum geodesic distance \n\n    obs min   mean max MC p-value\n1   115  56 113.19 170       0.92\n2   275 106 264.57 393       0.92\n3   148  11 106.60 192       0.26\n4    21   0  18.60  66       0.76\n5     2   0   2.61  19       0.62\n6     0   0   0.21   5       1.00\n7     0   0   0.01   1       1.00\nInf  69   0 124.21 434       0.84\n\nGoodness-of-fit for model statistics \n\n                        obs      min     mean      max MC p-value\nedges              115.0000 56.00000 113.1900 170.0000       0.92\nnodecov.practice   129.0000 74.00000 130.0500 174.0000       1.00\nnodematch.practice  72.0000 40.00000  70.8400 119.0000       0.94\ngwesp.fixed.0.693  181.2969 69.40236 177.6489 291.4062       0.76\n\n\nThe numerical output provides a detailed assessment of how well the fitted ERGM reproduces both the modeled terms and modeled structural features of the observed network.\nThe degree distribution is well captured across nearly all values, with observed counts generally falling within the range of those generated by simulations. Most Monte Carlo \\(p\\)-values are high (e.g., &gt; 0.5), indicating no significant deviation between the observed and simulated degree frequencies.\nMinimum geodesic distances, a measure of global connectivity, show reasonable agreement between observed and simulated networks. While the fit is slightly weaker at a geodesic distance of 3 (\\(p = 0.08\\)), all other distances are well within acceptable bounds, suggesting that the model captures the overall reachability structure effectively.\nLastly, the model terms explicitly included in the formula (edges, nodecov.practice, nodematch.practice, and gwesp) are all closely matched by the simulated networks, with very high \\(p\\)-values across the board. This confirms that the maximum likelihood estimation successfully reproduced the sufficient statistics.\nThese diagnostics together indicate that the model performs well. It reproduces the modeled terms accurately, and also generalizes to capture broader structural features such as degree variation and path lengths. Since the model has also passed MCMC convergence diagnostics, we can conclude that the ERGM is both well-estimated and substantively valid in representing the network-generating processes in the Lazega law firm.",
    "crumbs": [
      "Inferential Network Analysis",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Exponential Random Graph Models (ERGMs)</span>"
    ]
  },
  {
    "objectID": "inferential/ergm.html#references",
    "href": "inferential/ergm.html#references",
    "title": "17  Exponential Random Graph Models (ERGMs)",
    "section": "References",
    "text": "References\n\n\n\n\nBesag, Julian E. 1972. “Nearest-Neighbour Systems and the Auto-Logistic Model for Binary Data.” Journal of the Royal Statistical Society: Series B (Methodological) 34 (1): 75–83.\n\n\nFrank, Ove, and David Strauss. 1986. “Markov Graphs.” Journal of the American Statistical Association 81 (395): 832–42.\n\n\nHunter, David R, and Mark S Handcock. 2006. “Inference in Curved Exponential Family Models for Networks.” Journal of Computational and Graphical Statistics 15 (3): 565–83.\n\n\nPattison, Philippa, and Garry Robins. 2002. “Neighborhood–Based Models for Social Networks.” Sociological Methodology 32 (1): 301–37.\n\n\nSchweinberger, Michael. 2011. “Instability, Sensitivity, and Degeneracy of Discrete Exponential Families.” Journal of the American Statistical Association 106 (496): 1361–70.\n\n\nSnijders, Tom AB, Philippa E Pattison, Garry L Robins, and Mark S Handcock. 2006. “New Specifications for Exponential Random Graph Models.” Sociological Methodology 36 (1): 99–153.\n\n\nWest, P, and H Sweeting. 1996. “Background, Rationale and Design of the West of Scotland 11 to 16 Study.” MRC Medical Sociology Unit Working Paper, no. 52: 1.",
    "crumbs": [
      "Inferential Network Analysis",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Exponential Random Graph Models (ERGMs)</span>"
    ]
  },
  {
    "objectID": "inferential/saom.html",
    "href": "inferential/saom.html",
    "title": "18  Stochastic Actor Oriented Models (SAOMs)",
    "section": "",
    "text": "18.1 Packages Needed for this Chapter\nUp to this point, we have focused on modeling social networks as cross-sectional data, that is, as static snapshots of tie patterns observed at a single point in time. Models such as Erdős–Rényi, preferential attachment, and Exponential Random Graph Models (ERGMs) allow us to account for structural dependencies, attribute effects, and network complexity within such static networks.\nHowever, social networks are inherently dynamic. Ties are not fixed; they emerge, dissolve, and evolve through time as a result of decisions made by individual actors. Treating network data as static ignores this core aspect of social life. A more realistic modeling approach must account for the sequential, actor-driven nature of network evolution.\nThis brings us to Stochastic Actor-Oriented Models (SAOMs), a class of models explicitly designed for longitudinal network data. Unlike cross-sectional models that treat the network as a single outcome, SAOMs treat the network as a continuous-time stochastic process driven by individual actor decisions. In this framework, changes to the network occur one tie at a time, reflecting micro-level decisions made by actors based on preferences, opportunities, and constraints.\nSAOMs are particularly well-suited for panel data, i.e., multiple observations of the same network over time where both tie structures and actor attributes may co-evolve. This allows researchers to address questions such as:\nSAOMs are one of two main frameworks for dynamic network modeling. The other is the Temporal Exponential Random Graph Model (TERGM). While TERGMs extend ERGMs to panel data by conditioning on past networks, SAOMs take a process-based view that explicitly models how ties change over time due to actor decisions.\nTable 18.1 shows the core differences between ERGMs. While ERGMs model the global structure of a network at a single point in time, SAOMs treat network evolution as a dynamic process driven by actors making sequential decisions. In othr words, SAOMs extend the logic of ERGMs into the temporal domain, capturing the mechanisms of change rather than just the final structure.\nIn what follows, we focus on the SAOM framework, its components, estimation process, and how it can be used to explain and simulate network evolution.\nlibrary(igraph)\nlibrary(ggraph)\nlibrary(graphlayouts)\nlibrary(networkdata)\nlibrary(intergraph)\nlibrary(RSiena)",
    "crumbs": [
      "Inferential Network Analysis",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Stochastic Actor Oriented Models (SAOMs)</span>"
    ]
  },
  {
    "objectID": "inferential/saom.html#modeling-network-evolution",
    "href": "inferential/saom.html#modeling-network-evolution",
    "title": "18  Stochastic Actor Oriented Models (SAOMs)",
    "section": "18.2 Modeling Network Evolution",
    "text": "18.2 Modeling Network Evolution\nMost real-world social networks are dynamic systems. Relationships between individuals form, dissolve, and evolve as a result of ongoing social interactions, personal decisions, and contextual factors. When we observe a single snapshot of a network, we can only speculate about the processes that produced it. By contrast, longitudinal network data, repeated observations of a network over time, allow us to move from static description to dynamic explanation.\nLongitudinal network data consist of a set of actors \\(N = \\{1, 2, \\dots, n\\}\\) and a series of observed adjacency matrices: \\[\nx(t_0),\\ x(t_1),\\ \\dots,\\ x(t_M)\n\\] where each matrix captures the presence or absence of ties between actors at a particular time point.\nThese repeated measures make it possible to ask and answer key questions:\n\nHow frequently do actors change their ties?\nWhat drives the formation, maintenance, or dissolution of ties?\nHow do individual attributes (e.g., gender, age, group membership) shape network dynamics?\nCan we predict how the network will evolve in the future?\nHow do both endogenous (network-based) and exogenous (attribute-based) factors jointly shape the network?\n\nMoreover, understanding why networks change requires distinguishing between competing explanations. For instance, if we observe that two similar actors are connected at time \\(t_1\\), this might reflect selection (they formed a tie because of their similarity) or influence (they became similar after forming a tie). Only a longitudinal framework allows us to tease apart these mechanisms.\nTime also matters for structural tendencies. Consider transitivity: when we observe a triadic closure (i.e., if \\(i\\) is tied to \\(j\\) and \\(j\\) to \\(h\\), then \\(i\\) may become tied to \\(h\\)), we cannot know whether it reflects an intentional closure or merely a residual pattern without knowing the order in which ties appeared.\nStochastic Actor-Oriented Models (SAOMs) offer a solution by treating network change as a continuous-time, actor-driven process. In this framework:\n\nThe network evolves through a sequence of micro-steps where individual actors have opportunities to change their outgoing ties.\nEach actor evaluates the current network and makes decisions based on preferences (e.g., for reciprocation, closure, or similarity).\nThe model simulates the timing and direction of these changes between observation moments (waves).\n\nThis actor-oriented perspective aligns closely with how ties form in real life: individuals decide whom to connect with (or disconnect from), guided by structural cues and attribute-based tendencies.\nIn the next section, we will formalize this logic, introduce the core components of SAOMs, and show how the RSiena package implements these models for empirical analysis.",
    "crumbs": [
      "Inferential Network Analysis",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Stochastic Actor Oriented Models (SAOMs)</span>"
    ]
  },
  {
    "objectID": "inferential/saom.html#stochastic-processes-and-continuous-time-markov-chains",
    "href": "inferential/saom.html#stochastic-processes-and-continuous-time-markov-chains",
    "title": "18  Stochastic Actor Oriented Models (SAOMs)",
    "section": "18.3 Stochastic Processes and Continuous-Time Markov Chains",
    "text": "18.3 Stochastic Processes and Continuous-Time Markov Chains\nTo understand the dynamics of network evolution in Stochastic Actor-Oriented Models (SAOMs), it is helpful to first grasp the concept of a stochastic process; a collection of random variables indexed by time:\n\\[\n\\{ X(t), t \\in T \\}\n\\] where:\n\n\\(T\\) is the index set (typically representing time),\n\\(S\\) is the state space, the set of all possible values that \\(X(t)\\) can take.\n\nA particular type of stochastic process relevant to SAOMs is the continuous-time Markov chain (CTMC). A CTMC is defined by:\n\nA finite state space \\(S\\) (e.g., actor states or network configurations),\nA continuous time domain \\(t \\in [0, \\infty)\\),\nThe Markov property: the future state depends only on the present state, not the past: \\[\nP(X(t_j) = x_j \\mid X(t) = x(t), \\, \\forall t \\leq t_i) = P(X(t_j) = x_j \\mid X(t_i) = x_i)\n\\] This memoryless property allows us to model tie changes or actor decisions that depend only on the current configuration.\n\nThe CTMC evolves through a sequence of randomly timed transitions. Each state is held for a random duration, and transitions to the next state are governed by probability. More formally, a CTMC is characterized by:\n\nWhen a change occurs — governed by the holding time, typically modeled with an exponential distribution.\nWhat the next state is — determined by the jump matrix, which specifies the transition probabilities between states.\n\nTogether, holding time and jump probabilities define the full behavior of the CTMC.\n\nExample: A Cat’s Daily Activities\nTo bring the concept of a continuous-time Markov chain (CTMC) to life (or to all nine lives of our cat) consider a model of a house cat’s daily activities. At any given moment, the cat is in one of the following behavioral states:\n\n\\(0\\): Sleeping\n\\(1\\): Eating\n\\(2\\): Playing\n\\(3\\): Plotting chaos (e.g., knocking things off shelves)\n\nWe define \\(X(t)\\) as the cat’s current activity at time \\(t\\). The process \\(\\{X(t), t \\geq 0\\}\\) satisfies:\n\nA finite state space \\(S = \\{0, 1, 2, 3\\}\\)\n\nContinuous transitions over time\n\nThe Markov property: next state depends only on the current state\n\nThe cat transitions between states at random times. Each stay in a state lasts for a random holding time, and transitions to the next state occur probabilistically based on a jump matrix.\nThe holding time \\(T_i\\) in state \\(i\\) is modeled using an exponential distribution: \\[\nf_{T_i}(t) = \\lambda_i e^{-\\lambda_i t}, \\quad t &gt; 0\n\\]\n\n\\(\\lambda_i\\) is the rate of leaving state \\(i\\).\n\\(\\mathbb{E}[T_i] = \\frac{1}{\\lambda_i}\\) is the expected duration in state \\(i\\).\n\nThe exponential distribution’s memoryless property means that the probability of remaining in a state is independent of how long the cat has already been in it: \\[\nP(T_i &gt; s + t \\mid T_i &gt; t) = P(T_i &gt; s)\n\\] So, even after two hours of napping, the chance that the cat naps another 30 minutes is the same as if it had just started.\nOnce the holding time ends, the cat jumps to a new state. The transition matrix \\(P = (p_{ij})\\) governs this: \\[\np_{ij} = P(X(t') = j \\mid X(t) = i)\n\\] For each state \\(i\\), the row of probabilities \\(p_{ij}\\) must sum to 1: \\[\n\\sum_{j \\in S} p_{ij} = 1\n\\]\nFigure 18.1 illustrates a single realization of such a process. This visual shows how a process starting in state 0 might stay there for some time, then jump to state 2, then state 3, and so on, with irregular intervals between jumps.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 18.1: A realization of a continuous-time Markov chain (CTMC) showing the cat transitioning through behavioral states (Sleep → Play → Chaos → Sleep → Eat). Horizontal segments show how long each activity lasts (holding time), while vertical arrows show transitions (jumps) between states.\n\n\n\nBelow is a hypothetical transition matrix \\(P\\) for the cat’s behavioral states. Each row corresponds to the current state, and each column to the next state:\n\n\n\n\nSleep\nEat\nPlay\nChaos\n\n\n\n\nSleep\n0.00\n0.70\n0.83\n0.56\n\n\nEat\n0.71\n0.00\n0.81\n0.20\n\n\nPlay\n0.05\n0.82\n0.00\n0.45\n\n\nChaos\n0.91\n0.46\n0.61\n0.00\n\n\n\n\nNote: Diagonal entries (e.g., Sleep → Sleep) are set to zero for interpretability. They can be included to model the probability of no state change.\n\nNext, we can combine the states and transitions into a directed graph showing which states can be reached from one another, and with what likelihood. This is shown in Figure 18.2.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 18.2: Directed graph representation of the CTMC jump chain. Each node is a state (e.g., Sleep, Eat), and arrows represent possible transitions with associated probabilities.\n\n\n\nFigure 18.2 is a visual representation of this matrix. Each arrow in the graph corresponds to a non-zero entry \\(p_{ij}\\) in the matrix. The curved edges indicate transitions between pairs of states, and the labels on the arrows match the values in the matrix. Together, the matrix and the graph describe a jump chain over the set of behavioral states. These transitions are stochastic (i.e., random), and their dynamics unfold in continuous time, which is what differentiates CTMCs from discrete-time Markov models.\nIn summary, a Continuous-Time Markov Chain (CTMC):\n\nDetermines how long the system remains in a state using the holding time, typically modeled as exponentially distributed.\nUses a transition matrix (or jump matrix) to govern which state is entered next.\nIs memoryless and evolves in continuous time, meaning the future depends only on the present state and not the past.\n\nThese principles underpin Stochastic Actor-Oriented Models (SAOMs), where actors make sequential and probabilistic changes to their network ties or attributes, driven solely by the current network state.",
    "crumbs": [
      "Inferential Network Analysis",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Stochastic Actor Oriented Models (SAOMs)</span>"
    ]
  },
  {
    "objectID": "inferential/saom.html#formal-definition-of-saoms-as-continuous-time-markov-chains",
    "href": "inferential/saom.html#formal-definition-of-saoms-as-continuous-time-markov-chains",
    "title": "18  Stochastic Actor Oriented Models (SAOMs)",
    "section": "18.4 Formal Definition of SAOMs as Continuous-Time Markov Chains",
    "text": "18.4 Formal Definition of SAOMs as Continuous-Time Markov Chains\nStochastic Actor-Oriented Models (SAOMs), introduced by Snijders (1996), provide a principled framework for analyzing how social networks evolve over time. To formally define the model, we start by framing them as a type of continuous-time Markov chain (CTMC), operating on a network space.\nAs previously introduced, a Continuous-Time Markov Chain (CTMC) is characterized by three key components:\n\nA finite state space: For SAOMs, this space includes all possible directed networks (i.e., all adjacency matrices) that can be constructed from \\(n\\) actors.\nA continuous-time process: Network changes (such as the creation or dissolution of ties) occur at random, unpredictable points in time.\nThe Markov property: The likelihood of a transition depends solely on the network’s current configuration, not on how it arrived there.\n\nIn the sections that follow, we explore how each of these components applies specifically to SAOMs.\n\n18.4.1 State Space of Networks\nLet \\(X\\) be the set of all possible adjacency matrices (i.e., network configurations) defined on \\(n\\) actors. Each matrix corresponds to a different possible state of the network. The size of this space is:\n\\[\n|X| = 2^{n(n-1)}\n\\]\nThis comes from the fact that each of the \\(n(n-1)\\) possible directed ties between distinct actors can independently be either present (1) or absent (0).\nFor instance, with a 4-node directed network shown in Figure 18.3, we can represent different adjacency matrices as the network evolves over time. We see a step-by-step representation of how a network evolves through successive tie changes, starting from an empty network and progressing toward a fully connected one.\n\n\n\n\n\n\nFigure 18.3: An example of how a directed network grows step by step as ties are added. Each panel pairs the network with its adjacency matrix, where a 1 indicates a directed tie. The process moves from an empty network to a fully connected one.\n\n\n\n\n\n18.4.2 Continuous-time process\nStochastic Actor-Oriented Models (SAOMs) are based on the assumption that social networks evolve as a continuous-time Markov process. This means the network changes gradually and randomly over time through small, actor-driven steps such as adding or removing ties.\nHowever, in empirical studies, we rarely observe the full trajectory of these changes. Instead, we only see the network at a limited number of discrete observation moments (e.g., survey waves at \\(t_1\\), \\(t_2\\), \\(\\ldots\\)). The actual tie changes (who added or dropped a connection and when) occur in the unobserved latent process between these time points.\nThis distinction is critical:\n\nObserved process: snapshots of the network at specific time points.\nLatent process: the underlying continuous sequence of micro-steps, where actors make sequential, stochastic decisions based on the current state of the network.\n\nSAOMs aim to reconstruct and simulate this hidden evolution process, inferring the actor behaviors that most likely produced the observed transitions.\n[image] network states evolve at \\(t_0\\), \\(t_1\\), …, but what we observe are discrete “snapshots” of this hidden continuous-time trajectory.\n\n\n18.4.3 The Markov Property\nA central assumption of Stochastic Actor-Oriented Models (SAOMs) is that the probability of moving to a new network state depends only on the current state, not on the path taken to get there. This is the Markov property and is formally defined as: \\[\nP(X(t_j) = x_j \\mid X(t) = x(t), \\, \\forall t \\leq t_i) = P(X(t_j) = x_j \\mid X(t_i) = x_i)\n\\] where \\(X(t)\\) denotes the state (i.e., network configuration) at time \\(t\\). In other words, the future evolution of the process depends solely on the present state \\(x_i\\) and not on the entire sequence of previous states. The model is memoryless: it “forgets” the past once the current state is known.\nIn the context of SAOMs, this means that when an actor is given the opportunity to make a change, such as forming or dissolving a tie, they do so based only on the present network structure and covariate information. There is no dependence on the path the network took to reach its current configuration.\nThis assumption significantly simplifies the modeling of network dynamics. It eliminates the need to track full network histories and allows for tractable simulation-based estimation. Conceptually, it also aligns with many social processes where actors respond to their current social environment rather than recalling a complete relational past.\nNonetheless, the Markov assumption is a modeling abstraction. While it facilitates analysis and interpretation, it may not capture certain behaviors where history matters such as long-term reciprocity, reputation building, or delayed responses. Still, for many applications, it offers a powerful and flexible framework for understanding how networks evolve over time.\n\n\n18.4.4 Actor-Oriented Modeling Assumptions\nA central challenge in modeling network dynamics is the vast number of possible future configurations. For a directed network with \\(n\\) actors, there are \\(2^{n(n-1)}\\) possible states—making exhaustive evaluation of all potential transitions computationally infeasible.\nSAOMs address this by adopting an actor-driven approach. Instead of modeling global changes to the network, SAOMs assume that individual actors make decisions about their outgoing ties through a series of small, sequential updates. This simplifies the modeling process while remaining grounded in a realistic representation of social behavior.\nThe SAOM framework relies on three core assumptions:\n\nOne actor at a time: At each micro-step, a single actor is randomly selected and given the opportunity to revise their outgoing ties. This reflects individual, sequential decision-making.\nOne tie at a time: The selected actor may consider one outgoing tie—either forming it, dissolving it, or choosing to leave it unchanged. Only one tie can be changed per step, which keeps the space of possible transitions manageable.\nActor-controlled change: Each actor controls only their own outgoing ties. Tie changes arise solely from the actor’s individual evaluation of the network and are not the result of simultaneous or coordinated actions.\n\nTogether, these assumptions define a process of sequential micro-steps, each involving:\n\nSelecting an actor at random,\nConsidering one possible tie change,\nAnd waiting a randomly determined time before the next opportunity arises.\n\nThis process unfolds in continuous time and follows the structure of a continuous-time Markov chain (CTMC). It consists of two key components: the holding time; which determines how long the current network state persists, and the jump chain; which defines the probability of transitioning to a new network state via a single tie change.\n\n\n18.4.5 The Holding Time and The Jump Chain\nThe holding time in SAOMs refers to the waiting period before an actor is given the opportunity to change one of their outgoing ties. In accordance with the properties of a continuous-time Markov chain (CTMC), this waiting time is modeled as an exponentially distributed random variable.\nFor actor \\(i\\), the holding time \\(T_i\\) has the following probability density function: \\[\nf_{T_i}(t) = \\lambda_i e^{-\\lambda_i t}, \\quad \\lambda_i &gt; 0, \\quad t &gt; 0\n\\] Here, \\(\\lambda_i\\) is the rate parameter, determining how frequently actor \\(i\\) receives opportunities to change their ties. The choice of \\(\\lambda_i\\) defines how the actor selection process unfolds:\n\nHomogeneous specification: All actors have the same rate \\(\\lambda\\), implying equal opportunity: \\[\n\\lambda_i = \\lambda \\quad \\text{for all } i, \\qquad P(i \\text{ has opportunity for change}) = \\frac{1}{n}\n\\]\nHeterogeneous specification: The rate varies across actors based on covariates or network structure: \\[\n\\lambda_i = \\lambda_i(\\theta, x, v)\n\\] where \\(\\theta\\) is a parameter vector, \\(x\\) is the current network configuration, and \\(v\\) are actor-specific covariates. In this case: \\[\nP(i \\text{ has opportunity for change}) = \\frac{\\lambda_i(\\theta, x, v)}{\\sum_{j=1}^{n} \\lambda_j(\\theta, x, v)}\n\\] This flexibility allows the model to reflect actor-level heterogeneity in the speed of network change. However, for simplicity and interpretability, we often assume that all actors have the same average rate of change—that is, \\(\\lambda\\) is constant across actors, but may vary over time. In this case, actors are equally likely to be selected at any given moment, though the overall tempo of change can still shift over time depending on the time-varying rate parameter.\n\nThis setup relies on the memoryless property of the exponential distribution: \\[\nP(T_i &gt; s + t \\mid T_i &gt; t) = P(T_i &gt; s)\n\\] That is, the probability that actor \\(i\\) will be selected in the next instant is unaffected by how long they have already been waiting. This property aligns naturally with the continuous-time Markov assumption and greatly simplifies simulation. In summary, the holding time determines when micro-steps occur. It is random, memoryless, and—depending on the specification—either homogeneous or heterogeneous across actors, shaping the rhythm of network evolution in SAOMs.\nOnce an actor has been selected and the waiting time (holding time) has elapsed, the next question becomes: what change (if any) will this actor make to the network? This is where the jump chain which governs what change occurs once an actor is selected. At each micro-step, actor \\(i\\) evaluates a set of feasible alternatives \\(\\{x^{(1)}, x^{(2)}, \\dots, x^{(J)}\\}\\), each differing from the current network \\(x\\) by a change in one of \\(i\\)’s outgoing ties—either forming, dissolving, or maintaining a tie.\nEach alternative \\(x^{(j)}\\) is assigned a utility:\n\\[\nU_{ij} = F_{ij} + \\varepsilon_{ij}\n\\]\nHere, \\(F_{ij}\\) represents the deterministic component of the utility function—typically defined through an evaluation function that incorporates structural effects like reciprocity, transitivity, or covariate similarity. The \\(\\varepsilon_{ij}\\) term introduces randomness via a Gumbel-distributed disturbance, reflecting unobserved preferences or decision noise.\nThis setup defines a random utility model, where actor \\(i\\) selects among alternatives probabilistically rather than deterministically. Under the Gumbel assumption, the choice probabilities follow the familiar multinomial logit:\n\\[\np_{ij} = \\frac{\\exp(F_{ij})}{\\sum_{h=1}^{J} \\exp(F_{ih})}\n\\]\nThese probabilities form a jump matrix \\(P = (p_{ij})\\), describing the likelihood of transitioning from one network state to another. Because only one actor can change one tie at each micro-step, the number of feasible transitions is small, and the matrix is extremely sparse; a feature that makes SAOMs computationally tractable even for large networks.\nWe can now elaborate more precisely on how this deterministic component \\(F_{ij}\\) is constructed. It is typically written as an objective function composed of weighted network statistics:\n\\[\nf_i(\\theta, x') = \\sum_{k=1}^{K} \\theta_k \\cdot s_k(x', v)\n\\] where:\n\n\\(x'\\) is the candidate network after a tie change,\n\\(s_k(x', v)\\) is the \\(k\\)-th effect evaluated on \\(x'\\) (e.g., number of mutual ties),\n\\(\\theta_k\\) is the parameter associated with that effect.\n\nThis formulation provides a clear interpretation: actors evaluate the desirability of possible tie changes based on known network mechanisms and covariates. The random utility formulation introduces variability in behavior while allowing estimation of interpretable model parameters \\(\\theta\\).\nIn summary, the jump chain models which network configuration the process will move to, conditional on actor selection. Combined with the holding time, which governs when actors are selected, it defines the dynamics of the SAOM as a continuous-time Markov process. Actor decisions are sequential, stochastic, and based only on the current network—preserving the Markov property and allowing for rich but tractable modeling of network evolution.\n\n\n18.4.6 Endogenous Effects in the Objective Function\nThe objective function introduced above is composed of a linear combination of effects, each capturing a specific structural pattern or social mechanism. These effects are conceptually similar to those used in Exponential Random Graph Models (ERGMs), although they differ in interpretation due to the dynamic and actor-oriented nature of SAOMs.These effects fall into two main categories:\n\nEndogenous effects: derived from the structure of the network itself.\nExogenous effects: related to external actor attributes (covered in the next section).\n\nThe objective function for actor \\(i\\) evaluating network \\(x'\\) can be written as: \\[\nf_i(x') = \\sum_k \\theta_k \\cdot s_{ik}(x’)\n\\] where \\(\\theta_k\\) is the parameter for effect \\(k\\), and \\(s_{ik}(x')\\) is the statistic for effect \\(k\\) as computed for actor \\(i\\) in network \\(x'\\). Each effect enters the evaluation function with an associated parameter to be estimated from data. The weighted sum of these effects (plus random noise) determines the actor’s utility for each potential tie change. This guides the probabilistic decision-making process at the heart of the SAOM framework.\nThis section focuses on a few commonly used endogenous effect; those based solely on the structure of the network (without using external covariates). Many other effects can be modeled, depending on the theoretical focus and complexity of the network under study.\n\nOutdegree Effect\nSimilar to the edges term in ERGM, this effect reflects the tendency (or cost) of maintaining ties. It counts the number of outgoing ties from actor \\(i\\) in the new network configuration \\(x'\\):\n\\[\ns_i^{\\text{out}}(x’) = \\sum_j x’_{ij}\n\\] This term typically has a negative coefficient, penalizing actors for having too many ties, and thus introducing a cost to maintaining social relationships.\n\n\nReciprocity Effect\nThe reciprocity effect captures the tendency for actors to form mutual ties (equivalent to the mutual term in ERGMs). It counts how many of actor i’s outgoing ties are reciprocated: \\[\ns_i^{\\text{rec}}(x') = \\sum_j x'{ij} x'{ji}\n\\] A positive coefficient for this effect reflects a preference for mutual connections, e.g., “I send a tie to those who also send one to me.”\n\n\nTransitive Triplets Effect\nClosely related to the triangles or GWESP terms in ERGMs, this effect models triadic closure. The transitive effect reflects a tendency for triadic closure. If actor \\(i\\) sends a tie to \\(h\\), and \\(h\\) sends a tie to \\(j\\), then \\(i\\) is more likely to send a tie to \\(j\\):\n\\[\ns_i^{\\text{trans}}(x’) = \\sum_{j,h} x’{ij} x’{ih} x’_{hj}\n\\] This effect models social closure or hierarchy and contributes to the formation of cohesive subgroups or cliques.\n\n\nThree-Cycle Effect\nThe three-cycle effect (think cyclic triads in ERGMs) captures the tendency of actors to form circular structures in the network, i.e. it captures feedback loops in the network. Specifically, it counts the number of directed cycles of length three involving actor \\(i\\): \\[\ns_i^{\\text{cyc}}(x') = \\sum_{j,h} x'{ij} x'{jh} x'_{hi}\n\\] This effect often receives a negative coefficient, discouraging cyclic patterns that may indicate instability or lack of hierarchy.\nNext, we introduce exogenous effects, which link actor behavior to covariates such as attributes or group memberships.\n\n\n\n18.4.7 Exogenous Effects in the Objective Function\nIn addition to modeling endogenous network tendencies, like reciprocity or transitivity, SAOMs allow the inclusion of exogenous covariates that influence tie formation. These covariates may pertain to individual-level attributes (e.g., age, status, gender) or to dyadic relationships (e.g., geographical distance, attribute similarity). Just like in ERGMs, these effects enter the objective function, which represents the utility an actor associates with each possible network configuration.\nBelow are common types of exogenous effects.\n\nIndividual-Level Covariate Effects\nThese are based on node-level covariates and can enter the objective function in two ways:\n\nCovariate-Ego Effect\nThis effect captures whether actors with a particular covariate value are more (or less) likely to form ties. \\[\ns_i^{\\text{ego}}(x', v) = v_i \\sum_j x'_{ij}\n\\] Here, \\(v_i\\) is the covariate value for actor \\(i\\) (the sender), and the statistic counts how many ties they initiate. If the covariate is binary, this can test whether having a trait (e.g., being in a particular group) is associated with being more active.\nCovariate-Alter Effect\nThis reflects a preference for forming ties to others with certain attribute values. \\[\ns_i^{\\text{alter}}(x', v) = \\sum_j x'_{ij} v_j\n\\] Actors are more likely to send ties to alters with higher (or lower) covariate values, depending on the sign of the corresponding parameter.\n\n\n\nDyadic Covariate Effects\nSAOMs also allow modeling based on dyadic similarity, capturing whether actors prefer to connect with others who are similar in some attribute.\nThe statistic for covariate-related similarity \\[\ns_i^{\\text{sim}}(x', v) = \\sum_j x'_{ij} \\left( 1 - \\frac{|v_i - v_j|}{\\text{Range}(v)} \\right)\n\\] increases when \\(i\\) sends ties to actors \\(j\\) who are similar in covariate values. The similarity is normalized to a [0,1] scale, where 1 means perfect similarity. For binary covariates, this simplifies to an indicator function: \\[\ns_i^{\\text{sim}}(x', v) = \\sum_j x'_{ij} \\cdot I(v_i = v_j)\n\\]\nThis captures simple homophily; a tendency to connect with others who share the same attribute (e.g., same gender or group membership).\n\n\n\n18.4.8 Choosing Effects for the Objective Function\nWhen specifying a Stochastic Actor-Oriented Model (SAOM), selecting which effects to include in the actor’s objective function is the first step. At a minimum, outdegree (a baseline tendency to form ties) and reciprocity (the preference for mutual ties) must always be included. Beyond these, additional effects should be guided by substantive theory or research hypotheses. For example, if theory suggests that individuals tend to befriend friends of their friends, a transitive triplet effect may be appropriate. If social similarity (homophily) based on covariates like gender is expected, covariate-related similarity effects can be added. These choices tailor the model to reflect specific structural patterns or attribute-driven mechanisms. While only a few examples are shown here, SAOMs offer a broad range of endogenous and exogenous effects that researchers can incorporate to test theoretically meaningful network processes.",
    "crumbs": [
      "Inferential Network Analysis",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Stochastic Actor Oriented Models (SAOMs)</span>"
    ]
  },
  {
    "objectID": "inferential/saom.html#parameter-estimation-and-interpration",
    "href": "inferential/saom.html#parameter-estimation-and-interpration",
    "title": "18  Stochastic Actor Oriented Models (SAOMs)",
    "section": "18.5 Parameter Estimation and Interpration",
    "text": "18.5 Parameter Estimation and Interpration\nIn Stochastic Actor-Oriented Models (SAOMs), each parameter \\(\\beta_k\\) quantifies the importance of a specific effect \\(s_{ik}(x')\\) in the actor’s decision-making process during network evolution. These parameters help us understand how different structural or covariate-based features influence the likelihood of a network configuration.\nThe interpretation of \\(\\beta_k\\) is as follows:\n\nIf \\(\\theta_k = 0\\), the statistic \\(s_{ik}(x')\\) has no effect on the transition probabilities and thus plays no role in the network dynamics.\nIf \\(\\theta_k &gt; 0\\), actor \\(i\\) is more likely to move toward a network configuration \\(x'\\) with a higher value of \\(s_{ik}(x')\\).\nIf \\(\\beta_k &lt; 0\\), configurations with lower values of \\(s_{ik}(x')\\) are more attractive to the actor.\n\nImportantly, these preferences are assumed to be stable over time. That is, the entire set of model parameters \\(\\{\\theta_1, \\dots, \\theta_K\\}\\) is constant, meaning the way actors evaluate alternatives does not change during the observation window. \\[\n\\theta_1, \\dots, \\theta_K \\text{ are constant over time}\n\\] Parameter estimation in SAOMs is performed using simulation-based methods, as implemented in the RSiena R package (SIENA: Simulation Investigation for Empirical Network Analysis).\nThe core idea is to simulate the network evolution process under the assumed model, compare the simulated outcomes with observed data, and iteratively adjust parameters to improve the fit. This is known as the method of moments, where the simulated statistics are aligned with their empirical counterparts.\nThe estimation process typically involves:\n\nDefining the observed networks at multiple time points.\nSpecifying effects (endogenous and exogenous) to include in the model.\nRunning simulations to estimate parameters via iterative optimization.\n\nTo assess whether a particular effect significantly contributes to the network dynamics, we can conduct a hypothesis test for each parameter \\(\\theta_k\\). We test the following:\n\nNull Hypothesis (\\(H_0\\)): The observed network patterns related to effect \\(k\\) are due to chance. \\[\nH_0 : \\theta_k = 0\n\\]\nAlternative Hypothesis (\\(H_1\\)): The effect has a systematic, non-random influence on tie changes. \\[\nH_1 : \\theta_k \\ne 0\n\\] Under standard conditions, we use the ratio of the parameter estimate to its standard error (i.e., a t-score) as a test statistic:\n\n\\[\n\\left| \\frac{\\theta_k}{\\text{s.e.}(\\theta_k)} \\right| \\ge 2 \\Rightarrow \\text{Reject } H_0\n\\]\nIf the absolute t-score exceeds 2, we consider the parameter significantly different from 0 at approximately the 5% significance level.",
    "crumbs": [
      "Inferential Network Analysis",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Stochastic Actor Oriented Models (SAOMs)</span>"
    ]
  },
  {
    "objectID": "inferential/saom.html#rsiena",
    "href": "inferential/saom.html#rsiena",
    "title": "18  Stochastic Actor Oriented Models (SAOMs)",
    "section": "18.6 RSiena",
    "text": "18.6 RSiena\nRSiena is an R package for estimating and simulating SAOMs for network and behavior dynamics. It is specifically designed to model longitudinal network data, where social ties and actor attributes evolve over time.\nIn RSiena, the distinction between modeling the evolution of networks and the co-evolution of networks and behavior begins with how the dependent variable is defined.\nIf only the network is included as a dependent variable, the model focuses on network evolution. This framework captures how actors form, maintain, or dissolve ties over time, and it can include covariates (such as gender or smoking status) to explain social selection. In this case, individual attributes influence the likelihood of tie formation or dissolution, but actors’ own behaviors remain fixed over time.\nAlternatively, RSiena allows for co-evolution models by including both a network and a behavioral variable as dependent variables. This setup enables the simultaneous modeling of two processes: how networks evolve and how actors’ behaviors change. This makes it possible to study social influence, where individuals may adopt behaviors similar to those of their network peers. The model thus estimates both the selection of network ties based on covariates and the influence of peers on individual behavior over time.\nBelow, we consider a few models that are run in RSiena, and for each model we consider the following:\n\nPreparing the data\nImport and format the network and covariate data into the appropriate structure. This includes creating adjacency matrices for networks over time and vectors or matrices for actor-level covariates. These are then converted into sienaDependent, coCovar, or varCovar objects.\nSpecifying the model\nDefine the model structure by selecting the effects to include. Use getEffects() to retrieve the default effects object and includeEffects() to specify additional endogenous and covariate-related effects.\nEstimating the model  Set up the estimation procedure using sienaAlgorithmCreate() and run the model with siena07(). Evaluate model convergence and estimation diagnostics to ensure reliable results.\nInterpreting the results\nAnalyze the estimated parameters to understand the underlying social processes. This involves examining effect directions, magnitudes, and statistical significance to interpret mechanisms of network change or co-evolution.\n\n\nExample: Teenage Friends and Lifestyle Study\nWe use a running example with data from the “Teenage Friends and Lifestyle Study” (West and Sweeting 1996), which was also used in the previous chapter on ERGMs. In contrast to the earlier example that focused on a single time point and only the subset of 50 pupils, we here model the dynamics of friendship ties across all three observed waves and with the 129 pupils over all three time points, and including relevant actor-level covariates such as gender and smoking behavior. The dataset is called glasgow129 in the networkdata package, which includes the following:\n\nNetworks: Binary, directed friendship ties measured at three time points.\nActors: 129 pupils measured at three time points.\nMany covariates, but we focus on\n\nSex: 1 = Male, 2 = Female\nSmoking: 1 = No, 2 = Occasional, 3 = Regular\n\n\nWe load the data from the networkdata package and extract a graph object for each time peiod:\n\n# Load data as graph object from networkdata and extract all three wave networks\nglasgow_g1 &lt;- networkdata::glasgow129[[1]]\nglasgow_g2 &lt;- networkdata::glasgow129[[2]]\nglasgow_g3 &lt;- networkdata::glasgow129[[3]]\n\nThese igraph objects are our network snapshots and consider various SAOMs. We begin by converting each network snapshot into a binary adjacency matrix. These matrices represent the presence or absence of directed ties between actors at each time point.\n\n# Convert to adjacency matrices\nnet1 &lt;- as.matrix(as_adjacency_matrix(glasgow_g1, sparse = FALSE))\nnet2 &lt;- as.matrix(as_adjacency_matrix(glasgow_g2, sparse = FALSE))\nnet3 &lt;- as.matrix(as_adjacency_matrix(glasgow_g3, sparse = FALSE))\n\nWe then combine them into a 3-dimensional array with dimensions [actor × actor × wave], which is the required format for longitudinal network data in RSiena:\n\n# Combine into 3D array [actor x actor x wave]\nnet_array &lt;- array(c(net1, net2, net3), dim = c(129, 129, 3))\n\n\n\n18.6.1 Network Dynamics\n\n18.6.1.1 Model 1: Structural Effects\nWe start by estimating a simple model specification that includes only two effects: an outdegree (density) term and a reciprocity term, capturing baseline tie propensity and mutual tie preference without incorporating more complex structural or covariate-based influences. In other words, we are not including any covariates, so the model is specified purely in terms of endogenous network dynamics.\nWe convert the 3D network array net_array into a sienaDependent object using the sienaDependent() function. This step tells RSiena that the array represents a dependent network variable observed over multiple waves. The resulting object will be used as input in the model definition and estimation.\n\n# Convert array into RSiena dependent object\nsiena_net &lt;- sienaDependent(net_array)\n\nAfter defining the network as a sienaDependent object, we now wrap it into a complete RSiena data object using sienaDataCreate(). This function prepares the data for model specification and estimation by bundling all input variables (here just the network) into a structure that the estimation algorithm can use.\nSince we are modeling only the network (and no actor covariates yet), we simply pass siena_net as the argument.\n\n# Create a Siena data object\ndata_mod1 &lt;- sienaDataCreate(siena_net)\n\nIn RSiena, model specification is driven by the concept of “effects” which represent the structural features, covariate influences, and behavioral mechanisms that are hypothesized to drive network or behavioral change. By default, getEffects() lists all possible effects available for the data structure (e.g., network dynamics, covariates, behaviors). It doesn’t add any effects to the model yet though, it just initializes the structure that will hold them.\nWe begin by calling getEffects() on our friend_data object to create a default effects object. This object holds all possible effects for the given data structure and is used as a base for customization. You can have a look at these effects directly by calling it:\n\n# Define the effects: start with default effect object\nmy_effects &lt;- getEffects(data_mod1)\nmy_effects\n\n  effectName                         include fix   test  initialValue parm\n1 constant siena_net rate (period 1) TRUE    FALSE FALSE    7.45424   0   \n2 constant siena_net rate (period 2) TRUE    FALSE FALSE    6.82927   0   \n3 outdegree (density)                TRUE    FALSE FALSE   -1.61299   0   \n4 reciprocity                        TRUE    FALSE FALSE    0.00000   0   \n\n\nThese are the exact effects included by default:\n\nRate parameters for each wave transition (e.g., Period 1 and Period 2)\nOutdegree (density): capturing the baseline propensity to send ties\nReciprocity: modeling the tendency to reciprocate received ties\n\nSince our simple model includes only these two structural effects, there is no need to add further effects using includeEffects() at this stage.\nBefore running the estimation, we need to define the estimation algorithm that RSiena will use. This is done with the function sienaAlgorithmCreate(), which sets up the computational parameters for the simulation-based estimation process.\nIn the example below, we assign the algorithm to the object my_algorithm. The projname argument specifies a name for the project; RSiena will use this name to organize temporary output files and logs related to the estimation.\n\n# Define the estimation algorithm\nmy_algorithm &lt;- sienaAlgorithmCreate(projname = \"saom_1\", seed = 123)\n\nIf you use this algorithm object, siena07 will create/use an output file saom_1.txt .\n\n\n\n\n\n\n\n\nNoteNote on setting the seed\n\n\n\nSetting a seed as argument ensures reproducible results across runs given same R setup. Due to the stochastic nature of the algorithm and differences in computational environments, minor variations may still occur across:\n\ndifferent versions of R or RSiena,\ndifferent operating systems,\nor when using parallel processing.\n\n\n\nThis algorithm object is later passed into the siena07() function to control how the estimation is performed. You can customize various settings like number of iterations, convergence thresholds, or use defaults (as we do here) for a basic estimation.\nNow that we’ve set up the data, specified the effects, and defined the estimation algorithm, we can estimate the Stochastic Actor-Oriented Model using the siena07() function. This function runs the RSiena estimation procedure. It takes the following inputs:\n\nmy_algorithm: the algorithm settings we defined earlier.\ndata = friend_data: the network data in RSiena format.\neffects = my_effects: the specified model effects (in this case, outdegree and reciprocity).\nbatch = TRUE: suppresses user prompts during the estimation, making it suitable for scripted or automated runs.\n\n\n# Estimate the SAOM\nsaom_1 &lt;- siena07(\n  my_algorithm,\n  data = data_mod1,\n  effects = my_effects,\n  batch = TRUE\n)\n\nTo print a summary of the estimation results:\n\n# View results\nsaom_1\n\nEstimates, standard errors and convergence t-ratios\n\n                                   Estimate   Standard   Convergence \n                                                Error      t-ratio   \n\nRate parameters: \n  0.1      Rate parameter period 1  8.5797  ( 0.6983   )             \n  0.2      Rate parameter period 2  7.2383  ( 0.6048   )             \n\nOther parameters: \n  1.  eval outdegree (density)     -2.4114  ( 0.0407   )   -0.0013   \n  2.  eval reciprocity              2.7049  ( 0.0858   )   -0.0252   \n\nOverall maximum convergence ratio:    0.0396 \n\n\nTotal of 2412 iteration steps.\n\n\nA nicer summary of the model is obtained by running the following which saves the table of results as an html file (including significance stars for easy interpretation):\n\nsiena.table(saom_1, type=\"html\", sig=TRUE)\n\n\n\nSo, how do we interpret the parameter estimates?\nBefore interpreting parameter estimates, it’s essential to assess whether the estimation has converged adequately. In RSiena, convergence is assessed using two main criteria:\n\nOverall maximum convergence ratio:  This value summarizes how far the simulated statistics deviate from the observed ones. A value below 0.25 is generally considered acceptable, with values below 0.15 indicating very good convergence.\nIndividual convergence t-ratios:\nFor each effect, the t-ratio compares the observed statistic to the average from the simulated networks. If the estimation has converged, these t-ratios should be close to zero. Values below 0.10 are ideal, and below 0.20 is generally acceptable.\n\n\nConvergence Assessment\nAll convergence t-ratios in our Model 1 ouput are below 0.03, which suggests that the simulated statistics align closely with the observed network statistics for each parameter. This implies that the estimated model reproduces the key structural features of the observed data with high precision. Additionally, the overall maximum convergence ratio is 0.04, which is well below the commonly used threshold of 0.10, and even under the more conservative cutoff of 0.05. This means that none of the parameters are associated with poor convergence behavior. Taken together, these diagnostics confirm that the SAOM estimation process has stabilized, and the parameter estimates can be interpreted with confidence.\n\n\nRate Parameters\nThe rate parameters capture how frequently actors are given the opportunity to change their outgoing ties between two consecutive observation moments. These are modeled as Poisson-like intensities in a continuous-time Markov framework. The table below provides the estimates and their interpretation:\n\nRate 1: Estimate = 8.580 (SE = 0.698)\nOn average, each actor had around 9 opportunities to change a tie between waves 1 and 2.\nRate 2: Estimate = 7.238 (SE = 0.605)\nOn average, each actor had around 7 opportunities to change a tie between waves 2 and 3.\n\nThese values do not indicate how many changes were actually made, but how often actors could have made a change. Since actors may choose not to change a tie (i.e., they maintain the current state), the actual number of changes is usually lower than the number of opportunities. This is why rate parameters typically exceed the observed average number of tie changes per actor.\n\n\n\nNetwork Dynamics\nOnce an actor has the opportunity to change, their decision is guided by an objective function that evaluates possible alternative network states. Each effect included in the model represents a social mechanism or hypothesis about actor preferences. The estimated coefficients reflect the direction and strength of these preferences. The table below summarizes the main effects from the model:\n\nOutdegree (density): Estimate = –2.411 (SE = 0.041) Strong negative effect: actors are generally reluctant to form many ties.\nReciprocity: Estimate = 2.705 (SE = 0.086)\nStrong positive effect: actors strongly prefer to reciprocate existing ties. |\n\nThe interpretation of each parameter is as follows:\nOutdegree (density) parameter: \\(\\theta_\\text{out} = -2.411\\)   A strong negative value suggests that actors prefer sparser networks. Forming new ties is costly or undesirable unless balanced by other effects. The more ties an actor has, the lower their objective function, making additional ties increasingly unlikely.\nReciprocity parameter: \\(\\theta_\\text{rec} = 2.705\\)\nThis strong positive value reflects a strong preference for reciprocating existing ties. If another actor has already sent a tie to ego, ego is much more likely to return the tie.\nBoth estimates are statistically significant, as their t-ratios are well above 2. This supports the conclusion that density aversion and reciprocity are key mechanisms shaping the network.\nThese effects form part of each actor’s utility function when evaluating potential changes. For actor \\(i\\), the objective function can be written as:\n\\[\nf_i(x') = \\theta_\\text{out} \\sum_{j=1}^n x'_{ij} + \\theta_\\text{rec} \\sum_{j=1}^n x'_{ij} x'_{ji}\n\\]\nLet’s consider two scenarios:\n\nAdding a reciprocated tie\nSuppose \\(x_{ji} = 1\\) (the other actor already sent a tie), and ego adds \\(x_{ij} = 1\\) to reciprocate. The utility change is:\n\n\\[\n-2.411 + 2.705 = 0.294\n\\]\nThis results in a positive utility, indicating that the move is likely.\n\nAdding a non-reciprocated tie  If \\(x_{ji} = 0\\), the tie is unreciprocated. The utility is:\n\n\\[\n-2.411\n\\]\nThis is negative utility, indicating that the change is unlikely.\nOverall, the estimation results reveal that actors prefer to reciprocate existing ties and are generally cautious about forming many new ties. Reciprocated ties offer positive utility, while unreciprocated ties are penalized. These preferences are consistent with well-known social mechanisms: a desire for mutual relationships and an aversion to the costs of maintaining numerous social ties.\n\n\n18.6.1.2 Model 2: Structural + Covariate Effects\nSociological theory suggests that, beyond general tendencies for reciprocity and transitivity, actors are influenced by individual characteristics when forming or maintaining ties. In adolescent friendship networks, one of the most important predictors of tie formation is behavioral similarity, such as shared lifestyle choices or habits.\nHaving established the basic SAOM using only endogenous structural effects above (like outdegree and reciprocity), we now extend the model to account for actor-level covariates. By incorporating exogenous attributes such as sex and smoking behavior, we can investigate how individual characteristics shape tie formation in addition to the structural patterns already captured. This richer specification allows us to test more nuanced hypotheses about social selection and influence mechanisms working within the network.\nAs mentioned in Section 18.4.7, incorporating covariates into the objective function means we need to differentiate between:\n\nEgo effects: How an actor’s own covariate value influences their tendency to form or maintain ties (e.g., do smokers send more ties?).\nAlter effects: How the recipient’s covariate value affects the likelihood of receiving a tie (e.g., are smokers more likely to be chosen as friends?).\nSimilarity effects: Whether actors prefer to form ties with others who are similar to themselves in a given attribute (e.g., are adolescents more likely to befriend others with similar smoking behavior?).\n\nThis new specification remains actor-oriented and stochastic, but adds an important layer of covariate-based preference, allowing us to distinguish between purely structural mechanisms and individual-level traits influencing network evolution. It also aligns more closely with theoretical insights from social psychology and adolescent health research.\nBuilding on the simple structural model (Model 1) that included only the outdegree (density) and reciprocity effects, we now extend our specification to incorporate exogenous covariate information resulting in a more nuanced model of friendship dynamics.\nIn our Model 2 here, we treat sex as a fixed covariate and smoking as a changing covariate, measured at each wave. We now include the following effects in addition to the structural ones:\n\nEgo effect of smoking: Does an adolescent’s smoking status affect how many ties they send?\nAlter effect of smoking: Are smokers more popular in the network?\nSimilarity in smoking: Are adolescents more likely to form ties with others who smoke similarly?\nSex effects: These can be included to examine tie formation with respect to the sex of the pupil.\n\nIn sum, Model 2 captures not only who adolescents choose to befriend structurally, but also how individual traits shape social dynamics providing a more realistic and theoretically grounded model of network evolution.\nWe begin by converting the 3D network array we created above into a sienaDependent object:\n\n# Convert array into RSiena dependent object\nsiena_net &lt;- sienaDependent(net_array)\n\nLike before, this object represents the longitudinal network that serves as the dependent variable in the SAOM. Different from before, we extract covariate values from the network’s node attributes. Sex (sex.F) is treated as a time-invariant covariate, while smoking behavior (familysmoking) is treated as time-varying across the three network waves.\n\n# THIS NEEDS CHANGING IN NEETWORK DATA!\n# Extract sex (constant over all waves) and recode to numeric: 1 = Female, 0 = Male\nsex_char &lt;- V(glasgow_g3)$sex.F\nsex_num &lt;- ifelse(sex_char == \"F\", 1,\n                  ifelse(sex_char == \"M\", 0, NA))\n# convert to fixed covariate object                  \nsex_cov &lt;- coCovar(sex_num)\n\n# Extract smoking behavior (assumed changing)\nsmoke1 &lt;- as.numeric(V(glasgow_g1)$tobacco)\nsmoke2 &lt;- as.numeric(V(glasgow_g2)$tobacco)\nsmoke3 &lt;- as.numeric(V(glasgow_g3)$tobacco)\nsmoking_array &lt;- cbind(smoke1, smoke2, smoke3)\n# convert to changing covariate object\nsmoking_cov &lt;- varCovar(smoking_array)\n\nNext we combine the network and covariates into a single Siena data object that will be used for estimation.\n\ndata_mod2 &lt;- sienaDataCreate(siena_net, sex_cov, smoking_cov)\n\nWe define the effects to be included in the model. This includes structural effects (like outdegree, reciprocity, and transitive triplets) as well as ego, alter, and similarity effects for both sex and smoking covariates.\n\n# Start with the default effect set\nmy_effects &lt;- getEffects(data_mod2)\n\n# Add structural effects\nmy_effects &lt;- includeEffects(my_effects, outdegree, recip, transTrip)\n\n  effectName          include fix   test  initialValue parm\n1 reciprocity         TRUE    FALSE FALSE          0   0   \n2 transitive triplets TRUE    FALSE FALSE          0   0   \n\n# Add effects for sex\nmy_effects &lt;- includeEffects(my_effects, egoX, altX, simX, interaction1 = \"sex_cov\")\n\n  effectName         include fix   test  initialValue parm\n1 sex_cov alter      TRUE    FALSE FALSE          0   0   \n2 sex_cov ego        TRUE    FALSE FALSE          0   0   \n3 sex_cov similarity TRUE    FALSE FALSE          0   0   \n\n# Add effects for smoking\nmy_effects &lt;- includeEffects(my_effects, egoX, altX, simX, interaction1 = \"smoking_cov\")\n\n  effectName             include fix   test  initialValue parm\n1 smoking_cov alter      TRUE    FALSE FALSE          0   0   \n2 smoking_cov ego        TRUE    FALSE FALSE          0   0   \n3 smoking_cov similarity TRUE    FALSE FALSE          0   0   \n\n\nWe define the estimation settings using sienaAlgorithmCreate(). A seed is included to support reproducibility of the results.\n\nmy_algorithm &lt;-  sienaAlgorithmCreate(projname = \"saom_2\", seed = 123)\n\nIf you use this algorithm object, siena07 will create/use an output file saom_2.txt .\n\n\nWe now estimate the model using the siena07() function, which runs the simulation-based estimation procedure.\n\nsaom_2 &lt;- siena07(\n  my_algorithm,\n  data = data_mod2,\n  effects = my_effects,\n  batch = TRUE\n)\n\nFinally, as before, we display the model results as an HTML table:\n\nsiena.table(saom_2, type=\"html\", sig=TRUE)\n\n\n\nWe now interpret the results from Model 2, starting with an assessment of convergence, followed by the interpretation of parameter estimates.\n\nConvergence Assessment\nIn Model 2, all convergence t-ratios are below 0.07, and the overall maximum convergence ratio is 0.11. This indicates good convergence, and we can safely interpret the estimated parameters.\n\n\nRate Parameters\nRate parameters determine how frequently actors are allowed to change their outgoing ties between time periods. These are estimated as:\n\nPeriod 1: 10.761 (SE = 1.054)\nPeriod 2: 8.981 (SE = 0.850)\n\nThese values suggest that, on average, each actor had about 10 opportunities to change a tie between wave 1 and 2, and about 9 opportunities between wave 2 and 3. These values reflect the speed of network change over time but do not represent the number of actual changes.\n\n\n\nNetwork Dynamics\n\nOutdegree (Density): \\(-2.859\\) (SE = 0.053)\nThis strong negative value reflects a general reluctance to form many ties, leading to sparse networks.\nReciprocity: \\(1.991\\) (SE = 0.089)\nIndicates a strong tendency for actors to reciprocate existing ties.\nTransitive Triplets: \\(0.448\\) (SE = 0.026)\nSuggests a preference for transitive closure; if \\(i\\) is tied to \\(j\\), and \\(j\\) is tied to \\(k\\), then \\(i\\) is more likely to tie to \\(k\\).\n\nThese three effects together capture core structural properties of the network dynamics: sparsity, mutuality, and local clustering. We now look at the covariate related structural effects and start with the covariate sex:\n\nSex Alter: \\(-0.154\\) (SE = 0.097, not significant)\nThere is no clear evidence that actors avoid or prefer alters based on their sex.\nSex Ego: \\(0.159\\) (SE = 0.103, not significant)\nIndicates no significant difference in tie formation activity between male and female actors.\nSex Similarity: \\(0.912\\) (SE = 0.093)\nShows a strong and significant tendency for actors to form ties with others of the same sex which is a clear sign of sex-based homophily.\n\nThe SAOM estimation includes three effects based on the covariate sex, coded as 0 for boys and 1 for girls, and centered around the mean value\n\nmean(sex_cov)\n\n[1] 0.4341085\n\n\ni.e., \\(\\bar{s} = 0.434\\). So approximately 43.4% of actors are girls in this dataset. Centering covariates is a default behavior in RSiena, ensuring that estimated effects are interpreted relative to the average actor. After centering, the transformed covariate values become:\nBecause all covariates are mean-centered, the contribution of a tie from actor \\(i\\) to \\(j\\) to the actor’s objective function is: \\[\ns_i - \\bar{s} =\n\\begin{cases}\n-0.434 & \\text{for boys} \\\\\n0.566 & \\text{for girls}\n\\end{cases}\n\\] This transformation affects how the ego and alter effects are applied to the objective function. The objective function for actor \\(i\\) includes covariate effects as follows:\n\\[\nf_i(x’) = \\theta_{\\text{ego}}(s_i - \\bar{s}) + \\theta_{\\text{alter}}(s_j - \\bar{s}) + \\theta_{\\text{sim}} \\cdot I(s_i = s_j)\n\\]\nwhere\n\n\\(s_i\\) and \\(s_j\\) are the actor’s sex values (0 or 1),\n\\(\\bar{s} = 0.434\\) is the sample mean,\n\\(I(s_i = s_j)\\) is an indicator variable which equals to 1 if \\(i\\) and \\(j\\) share the same sex, and 0 otherwise.\n\nTable 18.2 below lists some example contributions to the objective function\n\n\n\nTable 18.2: Sex-based contributions to the objective function under different ego-alter combinations.\n\n\n\n\n\n\n\n\n\n\n\n\n\nEgo\nAlter\nEgo Term\nAlter Term\nSimilarity Term\nTotal Contribution\n\n\n\n\nMale\nMale\n\\(0.159 \\times (-0.434) = -0.069\\)\n\\(-0.154 \\times (-0.434) = +0.067\\)\n\\(+0.912\\)\n\\(0.910\\)\n\n\nMale\nFemale\n\\(-0.069\\)\n\\(-0.154 \\times 0.566 = -0.087\\)\n\\(0\\)\n\\(-0.156\\)\n\n\nFemale\nMale\n\\(0.159 \\times 0.566 = +0.090\\)\n\\(+0.067\\)\n\\(0\\)\n\\(0.157\\)\n\n\nFemale\nFemale\n\\(+0.090\\)\n\\(-0.087\\)\n\\(+0.912\\)\n\\(0.915\\)\n\n\n\n\n\n\nThese values show that same-sex ties, especially between girls, are more likely due to the strong similarity effect. Cross-sex ties are less likely, receiving lower utility in the objective function.\nNext, we look at the behavioral covariate smoking. From the above output we note the following:\n\nSmoking Alter: \\(0.109\\) (SE = 0.061, not significant)\nSuggests no evidence that smoking status affects how attractive an actor is as a tie recipient.\nSmoking Ego: \\(0.077\\) (SE = 0.063, not significant)\nNo effect of smoking behavior on how active an actor is in sending ties.\nSmoking Similarity: \\(0.380\\) (SE = 0.117)\nThis positive and significant parameter indicates a clear tendency toward homophily with respect to smoking behavior: actors are more likely to form or maintain friendships with others who share a similar smoking status.\n\nBased on the model output and the centered smoking covariate, we interpret the smoking-related effects in the SAOM model. The smoking variable is ordinal with three categories: 1 = no smoking, 2 = occasional smoking, and 3 = regular smoking. The covariate is centered by subtracting its mean. In this case, the mean value is computed in R as:\n\nmean(smoking_cov)\n\n[1] 1.377261\n\n\nWe now interpret the covariate effects related to smoking behavior. In this model, smoking is treated as a three-level categorical covariate: 1 = Non-smoker, 2 = Occasional, 3 = Regular. The mean of the covariate, computed via mean(smoke_cov), is approximately \\(\\bar{s} = 1.377\\).\nThe centered values become:\n\\[\ns_i - \\bar{s} =\n\\begin{cases}\n-0.377 & \\text{if } s_i = 1 \\quad \\text{(Non-smoker)} \\\\\n0.623  & \\text{if } s_i = 2 \\quad \\text{(Occasional smoker)} \\\\\n1.623  & \\text{if } s_i = 3 \\quad \\text{(Regular smoker)}\n\\end{cases}\n\\]\nThe contribution of \\(x_{ij}\\) (a tie from actor \\(i\\) to actor \\(j\\)) to the objective function is given by:\n\\[\n\\theta_{\\text{ego}}(s_i - \\bar{s}) + \\theta_{\\text{alter}}(s_j - \\bar{s}) + \\theta_{\\text{sim}} \\left(1 - \\frac{|s_i - s_j|}{R_s}\\right)\n\\] whree \\(R_s\\) refers to the range of the covariate \\(s\\). Substituting the estimated values from the model:\n\\[\n0.077(s_i - 1.377) + 0.109(s_j - 1.377) + 0.380 \\left(1 - \\frac{|s_i - s_j|}{2} \\right)\n\\]\nThis function combines ego and alter contributions based on smoking level, and adds a similarity effect that rewards ties between actors with similar smoking levels.\nTable 18.3 shows how smoking-related covariate effects contribute to the objective function in SAOM. Each cell computes the total effect of a potential tie from ego to alter based on their respective smoking levels. The function combines ego and alter contributions based on smoking level, and adds a similarity effect that rewards ties between actors with similar smoking levels.\n\n\n\nTable 18.3: Smoking-related contributions to the objective function under different ego-alter combinations.\n\n\n\n\n\n\n\n\n\n\n\n\n\nEgo\nAlter\nEgo Term\nAlter Term\nSimilarity Term\nTotal Contribution\n\n\n\n\nNon-smoker\nNon-smoker\n\\(0.077 \\cdot (-0.377) = -0.029\\)\n\\(0.109 \\cdot (-0.377) = -0.041\\)\n\\(0.380 \\cdot 1 = 0.380\\)\n\\(0.310\\)\n\n\nNon-smoker\nOccasional\n\\(-0.029\\)\n\\(0.109 \\cdot 0.623 = 0.068\\)\n\\(0.380 \\cdot (1 - 0.5) = 0.190\\)\n\\(0.229\\)\n\n\nNon-smoker\nRegular\n\\(-0.029\\)\n\\(0.109 \\cdot 1.623 = 0.177\\)\n\\(0.380 \\cdot (1 - 1) = 0\\)\n\\(0.148\\)\n\n\nOccasional\nNon-smoker\n\\(0.077 \\cdot 0.623 = 0.048\\)\n\\(-0.041\\)\n\\(0.190\\)\n\\(0.197\\)\n\n\nOccasional\nOccasional\n\\(0.048\\)\n\\(0.068\\)\n\\(0.380\\)\n\\(0.496\\)\n\n\nOccasional\nRegular\n\\(0.048\\)\n\\(0.177\\)\n\\(0.190\\)\n\\(0.415\\)\n\n\nRegular\nNon-smoker\n\\(0.077 \\cdot 1.623 = 0.125\\)\n\\(-0.041\\)\n\\(0\\)\n\\(0.084\\)\n\n\nRegular\nOccasional\n\\(0.125\\)\n\\(0.068\\)\n\\(0.190\\)\n\\(0.383\\)\n\n\nRegular\nRegular\n\\(0.125\\)\n\\(0.177\\)\n\\(0.380\\)\n\\(0.682\\)\n\n\n\n\n\n\nWe see from Table 18.3 that actors with similar smoking behavior are more likely to form ties, especially regular-to-regular smokers, indicating a strong homophily pattern. The ego and alter terms slightly amplify or reduce this tendency depending on individual smoking levels.\n\n\n\n18.6.2 Network and Behavioral Dynamics\n\n18.6.2.1 Model 3:\n\n\n\n\nSnijders, Tom AB. 1996. “Stochastic Actor-Oriented Models for Network Change.” Journal of Mathematical Sociology 21 (1-2): 149–72.\n\n\nWest, P, and H Sweeting. 1996. “Background, Rationale and Design of the West of Scotland 11 to 16 Study.” MRC Medical Sociology Unit Working Paper, no. 52: 1.",
    "crumbs": [
      "Inferential Network Analysis",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Stochastic Actor Oriented Models (SAOMs)</span>"
    ]
  },
  {
    "objectID": "tidy/introduction.html",
    "href": "tidy/introduction.html",
    "title": "20  Introduction",
    "section": "",
    "text": "20.1 Required libraries\nThe main focus of this part is to introduce the tidy approach for network analysis.\nTo run all the code in this part, you need to install and load two packages.\ntidygraph implements the tidy approach for network analysis. networkdata contains a diverse set of network dataset.\nlibrary(tidygraph)\n\n\nAttaching package: 'tidygraph'\n\n\nThe following object is masked from 'package:stats':\n\n    filter\n\nlibrary(networkdata)\nMake sure you have at least the version given below. Some of the examples may not be backward compatible.\npackageVersion(\"tidygraph\")\n\n[1] '1.3.1'\n\npackageVersion(\"networkdata\")\n\n[1] '0.2.1'",
    "crumbs": [
      "Tidy Network Analysis",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "tidy/introduction.html#required-libraries",
    "href": "tidy/introduction.html#required-libraries",
    "title": "20  Introduction",
    "section": "",
    "text": "install.packages(\"tidygraph\")\ndevtools::install_github(\"schochastics/networkdata\")",
    "crumbs": [
      "Tidy Network Analysis",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "tidy/introduction.html#what-is-tidy-network-data",
    "href": "tidy/introduction.html#what-is-tidy-network-data",
    "title": "20  Introduction",
    "section": "20.2 What is tidy network data?",
    "text": "20.2 What is tidy network data?\nOn first glance, there is not much tidiness in networks or the ways it is usually encoded, like a graph, adjacency matrix, edgelist, etc. How should this fit into a single data frame? If you are an avid igraph user, then you may suspect the answer. It doesn’t fit, but it fits in two with graph_from_data_frame() which takes two data frames, one for nodes and one for edges, as input. In other words, we can represent a network as two separate data frames. One for the nodes and node attributes, and one for the edges and edge attributes. Working with these two data frames together is the premise for the tidygraph package. If you are interested in more technical details on how this is implemented under the hood, see the introductory blog post for the package.",
    "crumbs": [
      "Tidy Network Analysis",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "tidy/introduction.html#why-tidy-network-data",
    "href": "tidy/introduction.html#why-tidy-network-data",
    "title": "20  Introduction",
    "section": "20.3 Why tidy network data?",
    "text": "20.3 Why tidy network data?\nThis is a good question. If you aren’t a fan of the tidyverse, then you should probably move along and stick with established packages such as igraph or sna which offer the exact same functionalities (tidygraph actually imports most of igraph). If you appreciate the tidyverse, then there is no need for convincing you that this is a good idea. If you are indifferent, then I hope I can make a case for the tidy framework below. To start off with, the package does a great job to harmonize many network analytic tasks. For instance, you do not need to know all the different centrality indices that are implemented. You simply type centrality_ and press tab in the RStudio console and get all functions that allow the calculation of a centrality index. Other node level functions are accessible via node_*() and edge level measures via edge_*().",
    "crumbs": [
      "Tidy Network Analysis",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "tidy/tidygraph-basics.html",
    "href": "tidy/tidygraph-basics.html",
    "title": "21  Basics of tidygraph",
    "section": "",
    "text": "21.1 Graph structures\nWe’ll use the famous Florentine Family marriage dataset as a running example. The dataset is in igraph format but can be converted to a tbl_graph object with as_tbl_graph().\ndata(\"flo_marriage\")\nflo_tidy &lt;- as_tbl_graph(flo_marriage)\nflo_tidy\n\n# A tbl_graph: 16 nodes and 20 edges\n#\n# An undirected simple graph with 2 components\n#\n# Node Data: 16 × 4 (active)\n   name         wealth `#priors` `#ties`\n   &lt;chr&gt;         &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;\n 1 Acciaiuoli       10        53       2\n 2 Albizzi          36        65       3\n 3 Barbadori        55         0      14\n 4 Bischeri         44        12       9\n 5 Castellani       20        22      18\n 6 Ginori           32         0       9\n 7 Guadagni          8        21      14\n 8 Lamberteschi     42         0      14\n 9 Medici          103        53      54\n10 Pazzi            48         0       7\n11 Peruzzi          49        42      32\n12 Pucci             3         0       1\n13 Ridolfi          27        38       4\n14 Salviati         10        35       5\n15 Strozzi         146        74      29\n16 Tornabuoni       48         0       7\n#\n# Edge Data: 20 × 2\n   from    to\n  &lt;int&gt; &lt;int&gt;\n1     1     9\n2     2     6\n3     2     7\n# ℹ 17 more rows\nThis new graph class just subclasses igraph and simply represents the network in a tidy fashion, printing two data frames, one for nodes and one for edges.\nclass(flo_tidy)\n\n[1] \"tbl_graph\" \"igraph\"\nAny function in R that expects an igraph object as input will also accept a tbl_graph.\nThe function tbl_graph() can be used to create a network from scratch with two data frames. It is basically equivalent to graph_from_data_frame().\nTo create random graphs with the usual generators, check out the create_*() and play_*() families of functions.",
    "crumbs": [
      "Tidy Network Analysis",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Basics of tidygraph</span>"
    ]
  },
  {
    "objectID": "tidy/tidygraph-basics.html#standard-verbs",
    "href": "tidy/tidygraph-basics.html#standard-verbs",
    "title": "21  Basics of tidygraph",
    "section": "21.2 Standard verbs",
    "text": "21.2 Standard verbs\nThe tidy framework, specifically thinking about dplyr, is about providing verbs which help to solve common data manipulation tasks, such as mutate(), select(), filter(), and summarise(). The challange for the tbl_graph objects is that these verbs somehow need to work with two different data frames. The way tidygraph solves this is via a pointer to the data frame which is supposed to be manipulated. This pointer can be changed with the verb activate(). By default the nodes are activated, which can also be seen with the print function (see line 5 in the output of flo_tidy). To activate the edge data frame, simply use activate(\"edges\").\n\nflo_tidy %&gt;% activate(\"edges\")\n\n# A tbl_graph: 16 nodes and 20 edges\n#\n# An undirected simple graph with 2 components\n#\n# Edge Data: 20 × 2 (active)\n    from    to\n   &lt;int&gt; &lt;int&gt;\n 1     1     9\n 2     2     6\n 3     2     7\n 4     2     9\n 5     3     5\n 6     3     9\n 7     4     7\n 8     4    11\n 9     4    15\n10     5    11\n11     5    15\n12     7     8\n13     7    16\n14     9    13\n15     9    14\n16     9    16\n17    10    14\n18    11    15\n19    13    15\n20    13    16\n#\n# Node Data: 16 × 4\n  name       wealth `#priors` `#ties`\n  &lt;chr&gt;       &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;\n1 Acciaiuoli     10        53       2\n2 Albizzi        36        65       3\n3 Barbadori      55         0      14\n# ℹ 13 more rows\n\n\nAny data manipulation would now be done on the edge data frame.\nHaving “activated” a data frame, many of the known dplyr verbs can be used to manipulate the data frame. The activation process might indicate that edges and nodes can only be manipulated separately, which is certainly not desirable. It is, however, possible to gain access to the edge data frame when nodes are activated via the .E(). Similarly, nodes can be accessed via .N() when edges are activated. In the below example, we activate the edges and create a new edge attribute which indicates if a family is connected to the Medici or not.\n\nflo_tidy &lt;- flo_tidy %&gt;% \n  activate(\"edges\") %&gt;% \n  mutate(to_medici=(.N()$name[from]==\"Medici\" | .N()$name[to]==\"Medici\"))\n\nThis particular use case is helpful for visualizations.\n\nggraph(flo_tidy, \"stress\") +\n    geom_edge_link0(aes(edge_color = to_medici)) +\n    geom_node_point(shape = 21, size = 10, fill = \"grey66\") +\n    geom_node_text(aes(label = name)) +\n    theme_graph()\n\n\n\n\n\n\n\n\nThe dplyr verb filter() can be used to obtain a subgraph that satisfies given conditions on the nodes. Note that in the case that you filter on nodes, also edges will be effected. If a node does not satisfy the condition, then all edges connected to that node disappear. This is not the case for edges though.\n\nflo_tidy %&gt;%\n    activate(\"edges\") %&gt;%\n    filter(to_medici) %&gt;%\n    ggraph(\"stress\", bbox = 10) +\n    geom_edge_link0(edge_color = \"black\") +\n    geom_node_point(shape = 21, size = 10, fill = \"grey66\") +\n    geom_node_text(aes(label = name)) +\n    theme_graph()",
    "crumbs": [
      "Tidy Network Analysis",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Basics of tidygraph</span>"
    ]
  },
  {
    "objectID": "tidy/tidygraph-basics.html#joins",
    "href": "tidy/tidygraph-basics.html#joins",
    "title": "21  Basics of tidygraph",
    "section": "21.3 Joins",
    "text": "21.3 Joins",
    "crumbs": [
      "Tidy Network Analysis",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Basics of tidygraph</span>"
    ]
  },
  {
    "objectID": "tidy/tidygraph-basics.html#new-verbs",
    "href": "tidy/tidygraph-basics.html#new-verbs",
    "title": "21  Basics of tidygraph",
    "section": "21.4 New Verbs",
    "text": "21.4 New Verbs",
    "crumbs": [
      "Tidy Network Analysis",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Basics of tidygraph</span>"
    ]
  },
  {
    "objectID": "tidy/tidygraph-descriptive.html",
    "href": "tidy/tidygraph-descriptive.html",
    "title": "22  Descriptive Network Analysis",
    "section": "",
    "text": "22.1 Centrality\nThe package includes all centrality indices implemented in igraph and additionally all that are made available in the netrankr package. All indices can be found in the function group centrality_*().\nflo_tidy %&gt;%\n    activate(\"nodes\") %&gt;%\n    mutate(\n        degree = centrality_degree(),\n        betweenness = centrality_betweenness()\n    ) %&gt;%\n    ggraph(\"stress\", bbox = 10) +\n    geom_edge_link0(edge_color = \"black\") +\n    geom_node_point(shape = 21, aes(size = degree, fill = betweenness)) +\n    geom_node_text(aes(label = name)) +\n    scale_fill_gradient(low = \"#104E8B\", high = \"#CD2626\") +\n    scale_size(range = c(4, 10)) +\n    theme_graph()",
    "crumbs": [
      "Tidy Network Analysis",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Descriptive Network Analysis</span>"
    ]
  },
  {
    "objectID": "tidy/tidygraph-descriptive.html#clustering",
    "href": "tidy/tidygraph-descriptive.html#clustering",
    "title": "22  Descriptive Network Analysis",
    "section": "22.2 Clustering",
    "text": "22.2 Clustering\nSimilar to centrality, all clustering algorithms from igraph are available via group_*()\n\n# create random graph with group structure (igraph equivalent is sample_islands())\nplay_islands(4, 12, 0.8, 4) %&gt;%\n    mutate(community = as.factor(group_louvain())) %&gt;%\n    ggraph(layout = \"stress\") +\n    geom_edge_link0() +\n    geom_node_point(aes(fill = community), shape = 21, size = 6) +\n    theme_graph()\n\n\n\n\n\n\n\n\nCoupling this with what we learned above, we can color the edges according to the cluster they belong to.\n\nplay_islands(4, 12, 0.8, 4) %&gt;%\n    mutate(community = as.factor(group_louvain())) %&gt;%\n    activate(\"edges\") %&gt;%\n    mutate(community = as.factor(ifelse(.N()$community[from] == .N()$community[to], .N()$community[from], 5))) %&gt;%\n    ggraph(layout = \"stress\") +\n    geom_edge_link0(aes(edge_colour = community), show.legend = FALSE) +\n    geom_node_point(aes(fill = community), shape = 21, size = 6) +\n    scale_fill_brewer(palette = \"Set3\") +\n    scale_edge_color_brewer(palette = \"Set3\") +\n    theme_graph(background = \"grey88\")",
    "crumbs": [
      "Tidy Network Analysis",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Descriptive Network Analysis</span>"
    ]
  },
  {
    "objectID": "tidy/tidygraph-descriptive.html#other-node-or-edge-level-functions",
    "href": "tidy/tidygraph-descriptive.html#other-node-or-edge-level-functions",
    "title": "22  Descriptive Network Analysis",
    "section": "22.3 Other node or edge level functions",
    "text": "22.3 Other node or edge level functions\ntidygraphs harmonizes many other available functions in igraph to make them easier accessible. The best way to check what is available is to look at the function groups node_*() and edge_*(). Some simple examples are shown below.\n\n# the node id of the Medici is 9\nflo_tidy %&gt;%\n    activate(\"nodes\") %&gt;%\n    mutate(dist2Medici = node_distance_to(nodes = 9)) %&gt;%\n    activate(\"edges\") %&gt;%\n    mutate(edge2Medici = edge_is_incident(9)) %&gt;%\n    ggraph(\"stress\") +\n    geom_edge_link0(aes(edge_color = edge2Medici)) +\n    geom_node_point(aes(fill = dist2Medici), size = 9, shape = 21) +\n    theme_graph()",
    "crumbs": [
      "Tidy Network Analysis",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Descriptive Network Analysis</span>"
    ]
  },
  {
    "objectID": "tidy/tidygraph-summary.html",
    "href": "tidy/tidygraph-summary.html",
    "title": "23  Summary",
    "section": "",
    "text": "The tidy framework works well in general but there are some shortcomings. So far, only basic network analytic methods are supported. This is enough for many tasks but as soon as more advanced techniques are needed, you are forced to switch to the “untidy” way again. A lot of coding but also conceptual work is needed to advance the framework further. For instance, how do ERGMs or SAOMs fit into this? Maybe there is a way to use tidymodels, but that is beyond the scope for now.",
    "crumbs": [
      "Tidy Network Analysis",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Summary</span>"
    ]
  },
  {
    "objectID": "summary.html",
    "href": "summary.html",
    "title": "27  Summary",
    "section": "",
    "text": "In summary, this book has no content whatsoever.",
    "crumbs": [
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>Summary</span>"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Barabási, Albert-László, and Réka Albert. 1999. “Emergence of\nScaling in Random Networks.” Science 286 (5439): 509–12.\n\n\nBender, Edward A, and E Rodney Canfield. 1978. “The Asymptotic\nNumber of Labeled Graphs with Given Degree Sequences.”\nJournal of Combinatorial Theory, Series A 24 (3): 296–307.\n\n\nBesag, Julian E. 1972. “Nearest-Neighbour Systems and the\nAuto-Logistic Model for Binary Data.” Journal of the Royal\nStatistical Society: Series B (Methodological) 34 (1): 75–83.\n\n\nBurt, Ronald. 1992. Structural Holes: The Social Structure of\nCompetition. Harvard University Press.\n\n\nCartwright, Dorwin, and Frank Harary. 1956. “Structural Balance: A\nGeneralization of Heider’s Theory.” Psychological Review\n63 (5): 277.\n\n\nColeman, James Samuel. 1964. Introduction to Mathematical\nSociology.\n\n\nErdos, Paul, and Alfréd Rényi. 1959. “On Random Graphs i.”\nPubl. Math. Debrecen 6 (290-297): 18.\n\n\nFrank, Ove, and David Strauss. 1986. “Markov Graphs.”\nJournal of the American Statistical Association 81 (395):\n832–42.\n\n\nGouldner, Alvin W. 1960. “The Norm of Reciprocity: A Preliminary\nStatement.” American Sociological Review, 161–78.\n\n\nHeider, Fritz. 1946. “Attitudes and Cognitive\nOrganization.” The Journal of Psychology 21 (1): 107–12.\n\n\nHunter, David R, and Mark S Handcock. 2006. “Inference in Curved\nExponential Family Models for Networks.” Journal of\nComputational and Graphical Statistics 15 (3): 565–83.\n\n\nLazega, Emmanuel. 2001. The Collegial Phenomenon: The Social\nMechanisms of Cooperation Among Peers in a Corporate Law\nPartnership. Oxford University Press, USA.\n\n\nMcPherson, Miller, Lynn Smith-Lovin, and James M Cook. 2001.\n“Birds of a Feather: Homophily in Social Networks.”\nAnnual Review of Sociology 27 (1): 415–44.\n\n\nMerton, Robert K. 1968. “The Matthew Effect in Science.”\nScience 159 (3810): 56–63.\n\n\nPattison, Philippa, and Garry Robins. 2002. “Neighborhood–Based\nModels for Social Networks.” Sociological Methodology 32\n(1): 301–37.\n\n\nSchweinberger, Michael. 2011. “Instability, Sensitivity, and\nDegeneracy of Discrete Exponential Families.” Journal of the\nAmerican Statistical Association 106 (496): 1361–70.\n\n\nSnijders, Tom AB. 1996. “Stochastic Actor-Oriented Models for\nNetwork Change.” Journal of Mathematical Sociology 21\n(1-2): 149–72.\n\n\nSnijders, Tom AB, Philippa E Pattison, Garry L Robins, and Mark S\nHandcock. 2006. “New Specifications for Exponential Random Graph\nModels.” Sociological Methodology 36 (1): 99–153.\n\n\nWatts, Duncan J, and Steven H Strogatz. 1998. “Collective Dynamics\nof ‘Small-World’networks.” Nature 393 (6684): 440–42.\n\n\nWest, P, and H Sweeting. 1996. “Background, Rationale and Design\nof the West of Scotland 11 to 16 Study.” MRC Medical\nSociology Unit Working Paper, no. 52: 1.",
    "crumbs": [
      "References"
    ]
  }
]