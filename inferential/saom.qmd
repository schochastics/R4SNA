# Stochastic Actor Oriented Models (SAOMs) {#sec-saom}
Up to this point, we have focused on modeling social networks as **cross-sectional data**, that is, as static snapshots of tie patterns observed at a single point in time. Models such as Erdős–Rényi, preferential attachment, and Exponential Random Graph Models (ERGMs) allow us to account for structural dependencies, attribute effects, and network complexity within such static networks.

However, social networks are inherently dynamic. Ties are not fixed; they emerge, dissolve, and evolve through time as a result of decisions made by individual actors. Treating network data as static ignores this core aspect of social life. A more realistic modeling approach must account for the sequential, actor-driven nature of network evolution.

This brings us to **Stochastic Actor-Oriented Models (SAOMs)**, a class of models explicitly designed for **longitudinal network data**. Unlike cross-sectional models that treat the network as a single outcome, SAOMs treat the network as a continuous-time stochastic process driven by individual actor decisions. In this framework, changes to the network occur one tie at a time, reflecting micro-level decisions made by actors based on preferences, opportunities, and constraints.

SAOMs are particularly well-suited for panel data, i.e., multiple observations of the same network over time where both tie structures and actor attributes may co-evolve. This allows researchers to address questions such as:

- How often do actors initiate or terminate ties?
- Do actors prefer to form ties based on similarity (selection) or become more similar after forming ties (influence)?
- How do structural tendencies like reciprocity or transitivity manifest through sequential decision-making?
- Can we simulate or predict how the network may evolve in future time points?


SAOMs are one of two main frameworks for dynamic network modeling. The other is the **Temporal Exponential Random Graph Model (TERGM)**. While TERGMs extend ERGMs to panel data by conditioning on past networks, SAOMs take a process-based view that explicitly models *how* ties change over time due to actor decisions.

@tbl-ergmsaom shows the core differences between ERGMs. While ERGMs model the global structure of a network at a single point in time, SAOMs treat network evolution as a dynamic process driven by actors making sequential decisions. In othr words, SAOMs extend the logic of ERGMs into the temporal domain, capturing the mechanisms of change rather than just the final structure.



::: {#tbl-ergmsaom}
|                     | ERGMs                | SAOMs              |
|------------------------------------------------------------|------------------------------------------------------------------------|--------------------------------------------------------------------------------|
| **Data type**              | Cross-sectional (static snapshot)                     | Longitudinal (panel data, repeated observations)             |
| **Unit of analysis**       | The entire network                                     | Individual actor decisions                                   |
| **Modeling approach**      | Statistical dependence among ties                     | Actor-based tie change process                              |
| **Time modeling**          | No time dimension                                     | Continuous-time change process                              |
| **Common mechanisms**      | Reciprocity, transitivity, nodal covariates           | Same, but expressed as actor preferences over changes        |
| **Estimation method**      | MCMC Maximum Likelihood (MCMCMLE)                     | Simulation-based Method of Moments (MoM)                    |
| **Outcome**                | Probability of the observed network                   | Simulated network trajectories over time                     |
| **In R**               | `ergm` (or `statnet`)                                   | `RSiena`                                        |

Comparison between Exponential Random Graph Models (ERGMs) and Stochastic Actor-Oriented Models (SAOMs).  The two approaches differ in their treatment of time, unit of analysis, estimation, and interpretation.
:::


In what follows, we focus on the SAOM framework, its components, estimation process, and how it can be used to explain and simulate network evolution.

## Packages Needed for this Chapter
```{r}
#| message: false
library(RSiena)
```

## Modeling Network Evolution

Most real-world social networks are dynamic systems. Relationships between individuals form, dissolve, and evolve as a result of ongoing social interactions, personal decisions, and contextual factors. When we observe a single snapshot of a network, we can only speculate about the processes that produced it. By contrast, longitudinal network data, repeated observations of a network over time, allow us to move from static description to dynamic explanation.

Longitudinal network data consist of a set of actors $N = \{1, 2, \dots, n\}$ and a series of observed adjacency matrices:
$$
x(t_0),\ x(t_1),\ \dots,\ x(t_M)
$$
where each matrix captures the presence or absence of ties between actors at a particular time point.

These repeated measures make it possible to ask and answer key questions:

- How frequently do actors change their ties?
- What drives the formation, maintenance, or dissolution of ties?
- How do individual attributes (e.g., gender, age, group membership) shape network dynamics?
- Can we predict how the network will evolve in the future?
- How do both endogenous (network-based) and exogenous (attribute-based) factors jointly shape the network?

Moreover, understanding *why* networks change requires distinguishing between competing explanations. For instance, if we observe that two similar actors are connected at time $t_1$, this might reflect selection (they formed a tie because of their similarity) or influence (they became similar after forming a tie). Only a longitudinal framework allows us to tease apart these mechanisms.

Time also matters for structural tendencies. Consider transitivity: when we observe a triadic closure (i.e., if $i$ is tied to $j$ and $j$ to $h$, then $i$ may become tied to $h$), we cannot know whether it reflects an intentional closure or merely a residual pattern without knowing the order in which ties appeared.

Stochastic Actor-Oriented Models (SAOMs) offer a solution by treating network change as a **continuous-time, actor-driven process**. In this framework:

- The network evolves through a sequence of micro-steps where individual actors have opportunities to change their outgoing ties.
- Each actor evaluates the current network and makes decisions based on preferences (e.g., for reciprocation, closure, or similarity).
- The model simulates the timing and direction of these changes between observation moments (waves).

This actor-oriented perspective aligns closely with how ties form in real life: individuals decide whom to connect with (or disconnect from), guided by structural cues and attribute-based tendencies.

In the next section, we will formalize this logic, introduce the core components of SAOMs, and show how the `RSiena` package implements these models for empirical analysis.


## Stochastic Processes and Continuous-Time Markov Chains
To understand the dynamics of network evolution in Stochastic Actor-Oriented Models (SAOMs), it is helpful to first grasp the concept of a **stochastic process**; a collection of random variables indexed by time:

$$
\{ X(t), t \in T \}
$$
where:

- $T$ is the index set (typically representing time),
- $S$ is the **state space**, the set of all possible values that $X(t)$ can take.

A particular type of stochastic process relevant to SAOMs is the **continuous-time Markov chain (CTMC)**. A CTMC is defined by:

1. A finite **state space** $S$ (e.g., actor states or network configurations),
2. A **continuous time domain** $t \in [0, \infty)$,
3. The **Markov property**: the future state depends only on the present state, not the past:
$$
P(X(t_j) = x_j \mid X(t) = x(t), \, \forall t \leq t_i) = P(X(t_j) = x_j \mid X(t_i) = x_i)
$$
This **memoryless** property allows us to model tie changes or actor decisions that depend only on the current configuration. 

The CTMC evolves through a sequence of randomly timed transitions. Each state is held for a random duration, and transitions to the next state are governed by probability. More formally, a CTMC is characterized by:

-	When a change occurs — governed by the holding time, typically modeled with an exponential distribution.
-	What the next state is — determined by the jump matrix, which specifies the transition probabilities between states.

Together, **holding time** and **jump probabilities** define the full behavior of the CTMC.

#### Example: A Cat’s Daily Activities   {.unnumbered}
To bring the concept of a continuous-time Markov chain (CTMC) to life (or to all nine lives of our cat) consider a model of a house cat’s daily activities. At any given moment, the cat is in one of the following behavioral states:

- $0$: Sleeping 
- $1$: Eating 
- $2$: Playing 
- $3$: Plotting chaos (e.g., knocking things off shelves) 

We define $X(t)$ as the cat’s current activity at time $t$. The process $\{X(t), t \geq 0\}$ satisfies:

- A finite state space $S = \{0, 1, 2, 3\}$  
- Continuous transitions over time  
- The Markov property: next state depends only on the current state  

The cat transitions between states at random times. Each stay in a state lasts for a random holding time, and transitions to the next state occur probabilistically based on a jump matrix. 

The holding time $T_i$ in state $i$ is modeled using an exponential distribution:
$$
f_{T_i}(t) = \lambda_i e^{-\lambda_i t}, \quad t > 0
$$

- $\lambda_i$ is the rate of leaving state $i$.
- $\mathbb{E}[T_i] = \frac{1}{\lambda_i}$ is the expected duration in state $i$.

The exponential distribution’s **memoryless property** means that the probability of remaining in a state is independent of how long the cat has already been in it:
$$
P(T_i > s + t \mid T_i > t) = P(T_i > s)
$$
So, even after two hours of napping, the chance that the cat naps another 30 minutes is the same as if it had just started.

Once the holding time ends, the cat jumps to a new state. The **transition matrix** $P = (p_{ij})$ governs this:
$$
p_{ij} = P(X(t') = j \mid X(t) = i)
$$
For each state $i$, the row of probabilities $p_{ij}$ must sum to 1:
$$
\sum_{j \in S} p_{ij} = 1
$$


@fig-catex illustrates a single realization of such a process. This visual shows how a process starting in state 0 might stay there for some time, then jump to state 2, then state 3, and so on, with irregular intervals between jumps.


::: {#fig-catex}
```{r}
#| fig-height: 4
#| fig-width: 7
#| message: false
#| echo: false

library(ggplot2)
jump_times <- c(0, 2, 5, 8, 14)
states <- c(0, 2, 3, 0, 1)  # Sleep → Play → Chaos → Sleep → Eat

segment_data <- data.frame(
  x = jump_times[-length(jump_times)],
  xend = jump_times[-1],
  y = states[-length(states)]
)

jump_points <- data.frame(
  x = jump_times[-1],
  y = states[-1]
)

ggplot() +
  geom_segment(data = segment_data, aes(x = x, xend = xend, y = y, yend = y),
               linewidth = 1.2, color = "black") +
  geom_point(data = jump_points, aes(x = x, y = y), shape = 21, fill = "black", size = 3) +
  annotate("segment", x = 0, xend = 2, y = -0.3, yend = -0.3,
           arrow = arrow(type = "closed", length = unit(0.2, "cm")),
           color = "#1c9099", linewidth = 1.5) +
  annotate("text", x = 1, y = -0.5, label = "Holding time", color = "#1c9099", size = 4) +
  annotate("segment", x = 8, xend = 8, y = 3, yend = 0.09,
           arrow = arrow(type = "closed", length = unit(0.2, "cm")),
           color = "darkorange", linewidth = 1.5) +
  annotate("text", x = 8.2, y = 1.5, label = "Jump", color = "darkorange", size = 4, angle = 90) +
  labs(x = "Time", y = "State") +
  scale_y_continuous(breaks = 0:3,
                     labels = c("Sleep", "Eat", "Play", "Chaos")) +
  theme_minimal(base_size = 10) +
  theme(panel.grid.minor = element_blank(),
        axis.title.y = element_text(angle = 0, vjust = 1.2))
```
A realization of a continuous-time Markov chain (CTMC)  showing the cat transitioning through behavioral states (Sleep → Play → Chaos → Sleep → Eat). Horizontal segments show how long each activity lasts (holding time), while vertical arrows show transitions (jumps) between states.

:::



Below is a hypothetical transition matrix $P$ for the cat's behavioral states. Each row corresponds to the current state, and each column to the next state:

| | Sleep | Eat | Play | Chaos |
|--------|:-----:|:---:|:----:|:-----:|
| **Sleep** | 0.00  | 0.70 | 0.83 | 0.56 |
| **Eat**   | 0.71  | 0.00 | 0.81 | 0.20 |
| **Play**  | 0.05  | 0.82 | 0.00 | 0.45 |
| **Chaos** | 0.91  | 0.46 | 0.61 | 0.00 |

> *Note: Diagonal entries (e.g., Sleep → Sleep) are set to zero for interpretability. They can be included to model the probability of no state change.*


Next, we can combine the states and transitions into a directed graph showing which states can be reached from one another, and with what likelihood. This is shown in @fig-catex2.



::: {#fig-catex2}
```{r}
#| fig-height: 5
#| fig-width: 5
#| message: false
#| echo: false
library(igraph)
library(ggraph)
library(tidygraph)
library(ggplot2)

# Define all 4 states
state_names <- c("Sleep", "Eat", "Play", "Chaos")

# Create all combinations of from → to (including self-loops)
transitions <- expand.grid(from = state_names, to = state_names, stringsAsFactors = FALSE)

# Assign random-ish transition probabilities (you can edit or normalize)
set.seed(77)
transitions$prob <- round(runif(nrow(transitions), min = 0.05, max = 0.95), 2)
# Create graph
g <- tbl_graph(nodes = data.frame(name = state_names), edges = transitions, directed = TRUE)

# Plot using geom_edge_arc to support curvature
ggraph(g, layout = "circle") +
  geom_edge_arc(aes(label = prob),
                arrow = arrow(length = unit(3, "mm"), type = "closed"),
                end_cap = circle(8, "mm"),
                start_cap = circle(8, "mm"),
                label_colour = "black",
                label_size = 3,
                alpha = 0.8,
                strength = 0.15,
                angle_calc = "along",
                label_dodge = unit(2.5, "mm"),
                show.legend = FALSE) +
  geom_node_circle(aes(r = 0.1), fill = "lightgrey", color = "black") +
  geom_node_text(aes(label = name), size = 3, fontface = "bold") +
  theme_void() 


```
Directed graph representation of the CTMC jump chain. Each node is a state (e.g., Sleep, Eat), and arrows represent possible transitions with associated probabilities.
:::

@fig-catex2 is a visual representation of this matrix. Each arrow in the graph corresponds to a non-zero entry $p_{ij}$ in the matrix. The curved edges indicate transitions between pairs of states, and the labels on the arrows match the values in the matrix. Together, the matrix and the graph describe a **jump chain** over the set of behavioral states. These transitions are stochastic (i.e., random), and their dynamics unfold in continuous time, which is what differentiates CTMCs from discrete-time Markov models.

In summary, a Continuous-Time Markov Chain (CTMC):

- Determines how long the system remains in a state using the holding time, typically modeled as exponentially distributed.
-	Uses a transition matrix (or jump matrix) to govern which state is entered next.
-	Is memoryless and evolves in continuous time, meaning the future depends only on the present state and not the past.

These principles underpin Stochastic Actor-Oriented Models (SAOMs), where actors make sequential and probabilistic changes to their network ties or attributes, driven solely by the current network state.


## Formal Definition of SAOMs as Continuous-Time Markov Chains
Stochastic Actor-Oriented Models (SAOMs), introduced by @snijders1996stochastic, provide a principled framework for analyzing how social networks evolve over time.  To formally define the model, we start by framing them as a type of continuous-time Markov chain (CTMC), operating on a network space.

As previously introduced, a Continuous-Time Markov Chain (CTMC) is characterized by three key components:

-	A finite state space: For SAOMs, this space includes all possible directed networks (i.e., all adjacency matrices) that can be constructed from $n$ actors.
-	A continuous-time process: Network changes (such as the creation or dissolution of ties) occur at random, unpredictable points in time.
-	The Markov property: The likelihood of a transition depends solely on the network’s current configuration, not on how it arrived there.

In the sections that follow, we explore how each of these components applies specifically to SAOMs.

### State Space of Networks
Let $X$ be the set of all possible adjacency matrices (i.e., network configurations) defined on $n$ actors. Each matrix corresponds to a different possible state of the network. The size of this space is:

$$
|X| = 2^{n(n-1)}
$$

This comes from the fact that each of the $n(n-1)$ possible directed ties between distinct actors can independently be either present (1) or absent (0).

For instance, with a 4-node directed network shown in @fig-saomex, we can represent different adjacency matrices as the network evolves over time. We see a step-by-step representation of how a network evolves through successive tie changes, starting from an empty network and progressing toward a fully connected one.

::: {#fig-saomex .center}
![](img/‎‎saom-state-ex.png){width="100%"}

An example of how a directed network grows step by step as ties are added. Each panel pairs the network with its adjacency matrix, where a 1 indicates a directed tie. The process moves from an empty network to a fully connected one.
:::

### Continuous-time process
Stochastic Actor-Oriented Models (SAOMs) are based on the assumption that social networks evolve as a continuous-time Markov process. This means the network changes gradually and randomly over time through small, actor-driven steps such as adding or removing ties.

However, in empirical studies, we rarely observe the full trajectory of these changes. Instead, we only see the network at a limited number of discrete observation moments (e.g., survey waves at $t_1$, $t_2$, $\ldots$). The actual tie changes (who added or dropped a connection and when) occur in the unobserved latent process between these time points.

This distinction is critical:

-	Observed process: snapshots of the network at specific time points.
-	Latent process: the underlying continuous sequence of micro-steps, where actors make sequential, stochastic decisions based on the current state of the network.

SAOMs aim to reconstruct and simulate this hidden evolution process, inferring the actor behaviors that most likely produced the observed transitions.

[image] network states evolve at $t_0$, $t_1$, …, but what we observe are discrete “snapshots” of this hidden continuous-time trajectory.


### The Markov Property 
A central assumption of Stochastic Actor-Oriented Models (SAOMs) is that the probability of moving to a new network state depends only on the current state, not on the path taken to get there. This is the Markov property and is formally defined as:
$$
P(X(t_j) = x_j \mid X(t) = x(t), \, \forall t \leq t_i) = P(X(t_j) = x_j \mid X(t_i) = x_i)
$$
where $X(t)$ denotes the state (i.e., network configuration) at time $t$. In other words, the future evolution of the process depends solely on the present state $x_i$ and not on the entire sequence of previous states. The model is memoryless: it "forgets" the past once the current state is known.

In the context of SAOMs, this means that when an actor is given the opportunity to make a change, such as forming or dissolving a tie, they do so based only on the present network structure and covariate information. There is no dependence on the path the network took to reach its current configuration.

This assumption significantly simplifies the modeling of network dynamics. It eliminates the need to track full network histories and allows for tractable simulation-based estimation. Conceptually, it also aligns with many social processes where actors respond to their current social environment rather than recalling a complete relational past.

Nonetheless, the Markov assumption is a modeling abstraction. While it facilitates analysis and interpretation, it may not capture certain behaviors where history matters such as long-term reciprocity, reputation building, or delayed responses. Still, for many applications, it offers a powerful and flexible framework for understanding how networks evolve over time. 


### Actor-Oriented Modeling Assumptions
A central challenge in modeling network dynamics is the vast number of possible future configurations. For a directed network with $n$ actors, there are $2^{n(n-1)}$ possible states—making exhaustive evaluation of all potential transitions computationally infeasible.

SAOMs address this by adopting an **actor-driven approach**. Instead of modeling global changes to the network, SAOMs assume that individual actors make decisions about their outgoing ties through a series of small, sequential updates. This simplifies the modeling process while remaining grounded in a realistic representation of social behavior.

The SAOM framework relies on three core assumptions:

1. *One actor at a time*: At each micro-step, a single actor is randomly selected and given the opportunity to revise their outgoing ties. This reflects individual, sequential decision-making.

2. *One tie at a time*: The selected actor may consider one outgoing tie—either forming it, dissolving it, or choosing to leave it unchanged. Only one tie can be changed per step, which keeps the space of possible transitions manageable.

3. *Actor-controlled change*: Each actor controls only their own outgoing ties. Tie changes arise solely from the actor’s individual evaluation of the network and are not the result of simultaneous or coordinated actions.

Together, these assumptions define a process of sequential micro-steps, each involving:

- Selecting an actor at random,
- Considering one possible tie change,
- And waiting a randomly determined time before the next opportunity arises.

This process unfolds in continuous time and follows the structure of a continuous-time Markov chain (CTMC). It consists of two key components:  the holding time; which determines how long the current network state persists,   and the jump chain; which defines the probability of transitioning to a new network state via a single tie change.


### The Holding Time and The Jump Chain
The **holding time** in SAOMs refers to the waiting period before an actor is given the opportunity to change one of their outgoing ties. In accordance with the properties of a continuous-time Markov chain (CTMC), this waiting time is modeled as an exponentially distributed random variable.

For actor $i$, the holding time $T_i$ has the following probability density function:
$$
f_{T_i}(t) = \lambda_i e^{-\lambda_i t}, \quad \lambda_i > 0, \quad t > 0
$$
Here, $\lambda_i$ is the rate parameter, determining how frequently actor $i$ receives opportunities to change their ties. The choice of $\lambda_i$ defines how the actor selection process unfolds:

- Homogeneous specification: All actors have the same rate $\lambda$, implying equal opportunity:
$$
\lambda_i = \lambda \quad \text{for all } i, \qquad P(i \text{ has opportunity for change}) = \frac{1}{n}
$$
- Heterogeneous specification: The rate varies across actors based on covariates or network structure:
$$
\lambda_i = \lambda_i(\theta, x, v)
$$
where $\theta$ is a parameter vector, $x$ is the current network configuration, and $v$ are actor-specific covariates. In this case:
$$
P(i \text{ has opportunity for change}) = \frac{\lambda_i(\theta, x, v)}{\sum_{j=1}^{n} \lambda_j(\theta, x, v)}
$$
This flexibility allows the model to reflect actor-level heterogeneity in the speed of network change. However, for simplicity and interpretability, we often assume that all actors have the same average rate of change—that is, $\lambda$ is constant across actors, but may vary over time. In this case, actors are equally likely to be selected at any given moment, though the overall tempo of change can still shift over time depending on the time-varying rate parameter.

This setup relies on the memoryless property of the exponential distribution:
$$
P(T_i > s + t \mid T_i > t) = P(T_i > s)
$$
That is, the probability that actor $i$ will be selected in the next instant is unaffected by how long they have already been waiting. This property aligns naturally with the continuous-time Markov assumption and greatly simplifies simulation. In summary, the holding time determines when micro-steps occur. It is random, memoryless, and—depending on the specification—either homogeneous or heterogeneous across actors, shaping the rhythm of network evolution in SAOMs.


Once an actor has been selected and the waiting time (holding time) has elapsed, the next question becomes: *what* change (if any) will this actor make to the network? This is where the **jump chain** which governs what change occurs once an actor is selected. At each micro-step, actor $i$ evaluates a set of feasible alternatives $\{x^{(1)}, x^{(2)}, \dots, x^{(J)}\}$, each differing from the current network $x$ by a change in one of $i$’s outgoing ties—either forming, dissolving, or maintaining a tie.

Each alternative $x^{(j)}$ is assigned a utility:

$$
U_{ij} = F_{ij} + \varepsilon_{ij}
$$

Here, $F_{ij}$ represents the deterministic component of the utility function—typically defined through an evaluation function that incorporates structural effects like reciprocity, transitivity, or covariate similarity. The $\varepsilon_{ij}$ term introduces randomness via a Gumbel-distributed disturbance, reflecting unobserved preferences or decision noise.

This setup defines a **random utility model**, where actor $i$ selects among alternatives probabilistically rather than deterministically. Under the Gumbel assumption, the choice probabilities follow the familiar multinomial logit:

$$
p_{ij} = \frac{\exp(F_{ij})}{\sum_{h=1}^{J} \exp(F_{ih})}
$$

These probabilities form a jump matrix $P = (p_{ij})$, describing the likelihood of transitioning from one network state to another. Because only one actor can change one tie at each micro-step, the number of feasible transitions is small, and the matrix is extremely sparse; a feature that makes SAOMs computationally tractable even for large networks.

We can now elaborate more precisely on how this deterministic component $F_{ij}$ is constructed. It is typically written as an **objective function** composed of weighted network statistics:

$$
f_i(\theta, x') = \sum_{k=1}^{K} \theta_k \cdot s_k(x', v)
$$

where:

- $x'$ is the candidate network after a tie change,
- $s_k(x', v)$ is the $k$-th effect evaluated on $x'$ (e.g., number of mutual ties),
- $\theta_k$ is the parameter associated with that effect.

This formulation provides a clear interpretation: actors evaluate the desirability of possible tie changes based on known network mechanisms and covariates. The random utility formulation introduces variability in behavior while allowing estimation of interpretable model parameters $\theta$.

In summary, the jump chain models which network configuration the process will move to, conditional on actor selection. Combined with the holding time, which governs *when* actors are selected, it defines the dynamics of the SAOM as a continuous-time Markov process. Actor decisions are sequential, stochastic, and based only on the current network—preserving the Markov property and allowing for rich but tractable modeling of network evolution.

### Endogenous Effects in the Objective Function
The objective function introduced above is composed of a linear combination of effects, each capturing a specific structural pattern or social mechanism. These effects are conceptually similar to those used in Exponential Random Graph Models (ERGMs), although they differ in interpretation due to the dynamic and actor-oriented nature of SAOMs.These effects fall into two main categories:

- Endogenous effects: derived from the structure of the network itself.
- Exogenous effects: related to external actor attributes (covered in the next section).

The objective function for actor $i$ evaluating network $x'$ can be written as:
$$
f_i(x') = \sum_k \theta_k \cdot s_{ik}(x’)
$$
where	$\theta_k$ is the parameter for effect $k$, and $s_{ik}(x')$ is the statistic for effect $k$ as computed for actor $i$ in network $x'$. Each effect enters the evaluation function with an associated parameter to be estimated from data. The weighted sum of these effects (plus random noise) determines the actor’s utility for each potential tie change. This guides the probabilistic decision-making process at the heart of the SAOM framework.

This section focuses on a few commonly used endogenous effect; those based solely on the structure of the network (without using external covariates). Many other effects can be modeled, depending on the theoretical focus and complexity of the network under study.


#### Outdegree Effect {.unnumbered}
Similar to the `edges` term in ERGM, this effect reflects the tendency (or cost) of maintaining ties. It counts the number of outgoing ties from actor $i$ in the new network configuration $x'$:

$$
s_i^{\text{out}}(x’) = \sum_j x’_{ij}
$$
This term typically has a negative coefficient, penalizing actors for having too many ties, and thus introducing a cost to maintaining social relationships.

#### Reciprocity Effect  {.unnumbered}
The reciprocity effect captures the tendency for actors to form mutual ties (equivalent to the `mutual` term in ERGMs). It counts how many of actor i’s outgoing ties are reciprocated:
$$
s_i^{\text{rec}}(x') = \sum_j x'{ij} x'{ji}
$$
A positive coefficient for this effect reflects a preference for mutual connections, e.g., *“I send a tie to those who also send one to me.”*

#### Transitive Triplets Effect  {.unnumbered}
Closely related to the `triangles` or `GWESP` terms in ERGMs, this effect models triadic closure. The transitive effect reflects a tendency for triadic closure. If actor $i$ sends a tie to $h$, and $h$ sends a tie to $j$, then $i$ is more likely to send a tie to $j$:

$$
s_i^{\text{trans}}(x’) = \sum_{j,h} x’{ij} x’{ih} x’_{hj}
$$
This effect models social closure or hierarchy and contributes to the formation of cohesive subgroups or cliques.

#### Three-Cycle Effect {.unnumbered}
The three-cycle effect (think cyclic triads in ERGMs) captures the tendency of actors to form circular structures in the network, i.e. it captures feedback loops in the network. Specifically, it counts the number of directed cycles of length three involving actor $i$:
$$
s_i^{\text{cyc}}(x') = \sum_{j,h} x'{ij} x'{jh} x'_{hi}
$$
This effect often receives a negative coefficient, discouraging cyclic patterns that may indicate instability or lack of hierarchy.

Next, we introduce exogenous effects, which link actor behavior to covariates such as attributes or group memberships.


### Exogenous Effects in the Objective Function
In addition to modeling endogenous network tendencies, like reciprocity or transitivity, SAOMs allow the inclusion of exogenous covariates that influence tie formation. These covariates may pertain to individual-level attributes (e.g., age, status, gender) or to dyadic relationships (e.g., geographical distance, attribute similarity). Just like in ERGMs, these effects enter the objective function, which represents the utility an actor associates with each possible network configuration.

Below are common types of exogenous effects.


#### Individual-Level Covariate Effects {.unnumbered}
These are based on node-level covariates and can enter the objective function in two ways:

1.	Covariate-Ego Effect\
This effect captures whether actors with a particular covariate value are more (or less) likely to form ties.
$$
s_i^{\text{ego}}(x', v) = v_i \sum_j x'_{ij}
$$
Here, $v_i$ is the covariate value for actor $i$ (the sender), and the statistic counts how many ties they initiate. If the covariate is binary, this can test whether having a trait (e.g., being in a particular group) is associated with being more active.

2.	Covariate-Alter Effect\
This reflects a preference for forming ties to others with certain attribute values.
$$
s_i^{\text{alter}}(x', v) = \sum_j x'_{ij} v_j
$$
Actors are more likely to send ties to alters with higher (or lower) covariate values, depending on the sign of the corresponding parameter.


#### Dyadic Covariate Effects {.unnumbered}
SAOMs also allow modeling based on dyadic similarity, capturing whether actors prefer to connect with others who are similar in some attribute.

TRhe statistic for covariate-related similarity
$$
s_i^{\text{sim}}(x', v) = \sum_j x'_{ij} \left( 1 - \frac{|v_i - v_j|}{\text{Range}(v)} \right)
$$
increases when $i$ sends ties to actors $j$ who are similar in covariate values. The similarity is normalized to a [0,1] scale, where 1 means perfect similarity. For binary covariates, this simplifies to an indicator function:
$$
s_i^{\text{sim}}(x', v) = \sum_j x'_{ij} \cdot I(v_i = v_j)
$$

This captures simple homophily; a tendency to connect with others who share the same attribute (e.g., same gender or group membership).


### Choosing Effects for the Objective Function
When specifying a Stochastic Actor-Oriented Model (SAOM), a central task is selecting which effects to include in the actor's objective function. At a minimum, outdegree (a baseline tendency to form ties) and reciprocity (the preference for mutual ties) must always be included. Beyond these, additional effects should be guided by substantive theory or research hypotheses. For example, if theory suggests that individuals tend to befriend friends of their friends, a transitive triplet effect may be appropriate. If social similarity (homophily) based on covariates like gender is expected, covariate-related similarity effects can be added. These choices tailor the model to reflect specific structural patterns or attribute-driven mechanisms. While only a few examples are shown here, SAOMs offer a broad range of endogenous and exogenous effects that researchers can incorporate to test theoretically meaningful network processes.



