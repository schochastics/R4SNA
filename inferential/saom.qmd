# Stochastic Actor Oriented Models (SAOMs) {#sec-saom}
Up to this point, we have focused on modeling social networks as **cross-sectional data**, that is, as static snapshots of tie patterns observed at a single point in time. Models such as Erdős–Rényi, preferential attachment, and Exponential Random Graph Models (ERGMs) allow us to account for structural dependencies, attribute effects, and network complexity within such static networks.

However, social networks are inherently dynamic. Ties are not fixed; they emerge, dissolve, and evolve through time as a result of decisions made by individual actors. Treating network data as static ignores this core aspect of social life. A more realistic modeling approach must account for the sequential, actor-driven nature of network evolution.

This brings us to **Stochastic Actor-Oriented Models (SAOMs)**, a class of models explicitly designed for **longitudinal network data**. Unlike cross-sectional models that treat the network as a single outcome, SAOMs treat the network as a continuous-time stochastic process driven by individual actor decisions. In this framework, changes to the network occur one tie at a time, reflecting micro-level decisions made by actors based on preferences, opportunities, and constraints.

SAOMs are particularly well-suited for panel data, i.e., multiple observations of the same network over time where both tie structures and actor attributes may co-evolve. This allows researchers to address questions such as:

- How often do actors initiate or terminate ties?
- Do actors prefer to form ties based on similarity (selection) or become more similar after forming ties (influence)?
- How do structural tendencies like reciprocity or transitivity manifest through sequential decision-making?
- Can we simulate or predict how the network may evolve in future time points?


SAOMs are one of two main frameworks for dynamic network modeling. The other is the **Temporal Exponential Random Graph Model (TERGM)**. While TERGMs extend ERGMs to panel data by conditioning on past networks, SAOMs take a process-based view that explicitly models *how* ties change over time due to actor decisions.

@tbl-ergmsaom shows the core differences between ERGMs. While ERGMs model the global structure of a network at a single point in time, SAOMs treat network evolution as a dynamic process driven by actors making sequential decisions. In othr words, SAOMs extend the logic of ERGMs into the temporal domain, capturing the mechanisms of change rather than just the final structure.



::: {#tbl-ergmsaom}
|                     | ERGMs                | SAOMs              |
|------------------------------------------------------------|------------------------------------------------------------------------|--------------------------------------------------------------------------------|
| **Data type**              | Cross-sectional (static snapshot)                     | Longitudinal (panel data, repeated observations)             |
| **Unit of analysis**       | The entire network                                     | Individual actor decisions                                   |
| **Modeling approach**      | Statistical dependence among ties                     | Actor-based tie change process                              |
| **Time modeling**          | No time dimension                                     | Continuous-time change process                              |
| **Common mechanisms**      | Reciprocity, transitivity, nodal covariates           | Same, but expressed as actor preferences over changes        |
| **Estimation method**      | MCMC Maximum Likelihood (MCMCMLE)                     | Simulation-based Method of Moments (MoM)                    |
| **Outcome**                | Probability of the observed network                   | Simulated network trajectories over time                     |
| **In R**               | `ergm` (or `statnet`)                                   | `RSiena`                                        |

Comparison between Exponential Random Graph Models (ERGMs) and Stochastic Actor-Oriented Models (SAOMs).  The two approaches differ in their treatment of time, unit of analysis, estimation, and interpretation.
:::


In what follows, we focus on the SAOM framework, its components, estimation process, and how it can be used to explain and simulate network evolution.

## Packages Needed for this Chapter
```{r}
#| message: false
library(RSiena)
```

## Modeling Network Evolution

Most real-world social networks are dynamic systems. Relationships between individuals form, dissolve, and evolve as a result of ongoing social interactions, personal decisions, and contextual factors. When we observe a single snapshot of a network, we can only speculate about the processes that produced it. By contrast, longitudinal network data, repeated observations of a network over time, allow us to move from static description to dynamic explanation.

Longitudinal network data consist of a set of actors $N = \{1, 2, \dots, n\}$ and a series of observed adjacency matrices:
$$
x(t_0),\ x(t_1),\ \dots,\ x(t_M)
$$
where each matrix captures the presence or absence of ties between actors at a particular time point.

These repeated measures make it possible to ask and answer key questions:

- How frequently do actors change their ties?
- What drives the formation, maintenance, or dissolution of ties?
- How do individual attributes (e.g., gender, age, group membership) shape network dynamics?
- Can we predict how the network will evolve in the future?
- How do both endogenous (network-based) and exogenous (attribute-based) factors jointly shape the network?

Moreover, understanding *why* networks change requires distinguishing between competing explanations. For instance, if we observe that two similar actors are connected at time $t_1$, this might reflect selection (they formed a tie because of their similarity) or influence (they became similar after forming a tie). Only a longitudinal framework allows us to tease apart these mechanisms.

Time also matters for structural tendencies. Consider transitivity: when we observe a triadic closure (i.e., if $i$ is tied to $j$ and $j$ to $h$, then $i$ may become tied to $h$), we cannot know whether it reflects an intentional closure or merely a residual pattern without knowing the order in which ties appeared.

Stochastic Actor-Oriented Models (SAOMs) offer a solution by treating network change as a **continuous-time, actor-driven process**. In this framework:

- The network evolves through a sequence of micro-steps where individual actors have opportunities to change their outgoing ties.
- Each actor evaluates the current network and makes decisions based on preferences (e.g., for reciprocation, closure, or similarity).
- The model simulates the timing and direction of these changes between observation moments (waves).

This actor-oriented perspective aligns closely with how ties form in real life: individuals decide whom to connect with (or disconnect from), guided by structural cues and attribute-based tendencies.

In the next section, we will formalize this logic, introduce the core components of SAOMs, and show how the `RSiena` package implements these models for empirical analysis.


## Stochastic Processes and Continuous-Time Markov Chains
To understand the dynamics of network evolution in Stochastic Actor-Oriented Models (SAOMs), it is helpful to first grasp the concept of a **stochastic process**; a collection of random variables indexed by time:

$$
\{ X(t), t \in T \}
$$
where:

- $T$ is the index set (typically representing time),
- $S$ is the **state space**, the set of all possible values that $X(t)$ can take.

A particular type of stochastic process relevant to SAOMs is the **continuous-time Markov chain (CTMC)**. A CTMC is defined by:

1. A finite **state space** $S$ (e.g., actor states or network configurations),
2. A **continuous time domain** $t \in [0, \infty)$,
3. The **Markov property**: the future state depends only on the present state, not the past:
$$
P(X(t_j) = x_j \mid X(t) = x(t), \, \forall t \leq t_i) = P(X(t_j) = x_j \mid X(t_i) = x_i)
$$
This **memoryless** property allows us to model tie changes or actor decisions that depend only on the current configuration. 

The CTMC evolves through a sequence of randomly timed transitions. Each state is held for a random duration, and transitions to the next state are governed by probability. More formally, a CTMC is characterized by:

-	When a change occurs — governed by the holding time, typically modeled with an exponential distribution.
-	What the next state is — determined by the jump matrix, which specifies the transition probabilities between states.

Together, **holding time** and **jump probabilities** define the full behavior of the CTMC.

#### Example: A Cat’s Daily Activities   {.unnumbered}
To bring the concept of a continuous-time Markov chain (CTMC) to life (or to all nine lives of our cat) consider a model of a house cat’s daily activities. At any given moment, the cat is in one of the following behavioral states:

- $0$: Sleeping 
- $1$: Eating 
- $2$: Playing 
- $3$: Plotting chaos (e.g., knocking things off shelves) 

We define $X(t)$ as the cat’s current activity at time $t$. The process $\{X(t), t \geq 0\}$ satisfies:

- A finite state space $S = \{0, 1, 2, 3\}$  
- Continuous transitions over time  
- The Markov property: next state depends only on the current state  

The cat transitions between states at random times. Each stay in a state lasts for a random holding time, and transitions to the next state occur probabilistically based on a jump matrix. 

The holding time $T_i$ in state $i$ is modeled using an exponential distribution:
$$
f_{T_i}(t) = \lambda_i e^{-\lambda_i t}, \quad t > 0
$$

- $\lambda_i$ is the rate of leaving state $i$.
- $\mathbb{E}[T_i] = \frac{1}{\lambda_i}$ is the expected duration in state $i$.

The exponential distribution’s **memoryless property** means that the probability of remaining in a state is independent of how long the cat has already been in it:
$$
P(T_i > s + t \mid T_i > t) = P(T_i > s)
$$
So, even after two hours of napping, the chance that the cat naps another 30 minutes is the same as if it had just started.

Once the holding time ends, the cat jumps to a new state. The **transition matrix** $P = (p_{ij})$ governs this:
$$
p_{ij} = P(X(t') = j \mid X(t) = i)
$$
For each state $i$, the row of probabilities $p_{ij}$ must sum to 1:
$$
\sum_{j \in S} p_{ij} = 1
$$


@fig-catex illustrates a single realization of such a process. This visual shows how a process starting in state 0 might stay there for some time, then jump to state 2, then state 3, and so on, with irregular intervals between jumps.


::: {#fig-catex}
```{r}
#| fig-height: 4
#| fig-width: 7
#| message: false
#| echo: false

library(ggplot2)
jump_times <- c(0, 2, 5, 8, 14)
states <- c(0, 2, 3, 0, 1)  # Sleep → Play → Chaos → Sleep → Eat

segment_data <- data.frame(
  x = jump_times[-length(jump_times)],
  xend = jump_times[-1],
  y = states[-length(states)]
)

jump_points <- data.frame(
  x = jump_times[-1],
  y = states[-1]
)

ggplot() +
  geom_segment(data = segment_data, aes(x = x, xend = xend, y = y, yend = y),
               linewidth = 1.2, color = "black") +
  geom_point(data = jump_points, aes(x = x, y = y), shape = 21, fill = "black", size = 3) +
  annotate("segment", x = 0, xend = 2, y = -0.3, yend = -0.3,
           arrow = arrow(type = "closed", length = unit(0.2, "cm")),
           color = "#1c9099", linewidth = 1.5) +
  annotate("text", x = 1, y = -0.5, label = "Holding time", color = "#1c9099", size = 4) +
  annotate("segment", x = 8, xend = 8, y = 3, yend = 0.09,
           arrow = arrow(type = "closed", length = unit(0.2, "cm")),
           color = "darkorange", linewidth = 1.5) +
  annotate("text", x = 8.2, y = 1.5, label = "Jump", color = "darkorange", size = 4, angle = 90) +
  labs(x = "Time", y = "State") +
  scale_y_continuous(breaks = 0:3,
                     labels = c("Sleep", "Eat", "Play", "Chaos")) +
  theme_minimal(base_size = 10) +
  theme(panel.grid.minor = element_blank(),
        axis.title.y = element_text(angle = 0, vjust = 1.2))
```
A realization of a continuous-time Markov chain (CTMC)  showing the cat transitioning through behavioral states (Sleep → Play → Chaos → Sleep → Eat). Horizontal segments show how long each activity lasts (holding time), while vertical arrows show transitions (jumps) between states.

:::



Below is a hypothetical transition matrix $P$ for the cat's behavioral states. Each row corresponds to the current state, and each column to the next state:

| | Sleep | Eat | Play | Chaos |
|--------|:-----:|:---:|:----:|:-----:|
| **Sleep** | 0.00  | 0.70 | 0.83 | 0.56 |
| **Eat**   | 0.71  | 0.00 | 0.81 | 0.20 |
| **Play**  | 0.05  | 0.82 | 0.00 | 0.45 |
| **Chaos** | 0.91  | 0.46 | 0.61 | 0.00 |

> *Note: Diagonal entries (e.g., Sleep → Sleep) are set to zero for interpretability. They can be included to model the probability of no state change.*


Next, we can combine the states and transitions into a directed graph showing which states can be reached from one another, and with what likelihood. This is shown in @fig-catex2.



::: {#fig-catex2}
```{r}
#| fig-height: 5
#| fig-width: 5
#| message: false
#| echo: false
library(igraph)
library(ggraph)
library(tidygraph)
library(ggplot2)

# Define all 4 states
state_names <- c("Sleep", "Eat", "Play", "Chaos")

# Create all combinations of from → to (including self-loops)
transitions <- expand.grid(from = state_names, to = state_names, stringsAsFactors = FALSE)

# Assign random-ish transition probabilities (you can edit or normalize)
set.seed(77)
transitions$prob <- round(runif(nrow(transitions), min = 0.05, max = 0.95), 2)
# Create graph
g <- tbl_graph(nodes = data.frame(name = state_names), edges = transitions, directed = TRUE)

# Plot using geom_edge_arc to support curvature
ggraph(g, layout = "circle") +
  geom_edge_arc(aes(label = prob),
                arrow = arrow(length = unit(3, "mm"), type = "closed"),
                end_cap = circle(8, "mm"),
                start_cap = circle(8, "mm"),
                label_colour = "black",
                label_size = 3,
                alpha = 0.8,
                strength = 0.15,
                angle_calc = "along",
                label_dodge = unit(2.5, "mm"),
                show.legend = FALSE) +
  geom_node_circle(aes(r = 0.1), fill = "lightgrey", color = "black") +
  geom_node_text(aes(label = name), size = 3, fontface = "bold") +
  theme_void() 


```
Directed graph representation of the CTMC jump chain. Each node is a state (e.g., Sleep, Eat), and arrows represent possible transitions with associated probabilities.
:::

@fig-catex2 is a visual representation of this matrix. Each arrow in the graph corresponds to a non-zero entry $p_{ij}$ in the matrix. The curved edges indicate transitions between pairs of states, and the labels on the arrows match the values in the matrix. Together, the matrix and the graph describe a **jump chain** over the set of behavioral states. These transitions are stochastic (i.e., random), and their dynamics unfold in continuous time, which is what differentiates CTMCs from discrete-time Markov models.

In summary, a Continuous-Time Markov Chain (CTMC):

- Determines how long the system remains in a state using the holding time, typically modeled as exponentially distributed.
-	Uses a transition matrix (or jump matrix) to govern which state is entered next.
-	Is memoryless and evolves in continuous time, meaning the future depends only on the present state and not the past.

These principles underpin Stochastic Actor-Oriented Models (SAOMs), where actors make sequential and probabilistic changes to their network ties or attributes, driven solely by the current network state.


## Formal Definition of SAOMs as Continuous-Time Markov Chains
Stochastic Actor-Oriented Models (SAOMs), introduced by @snijders1996stochastic, provide a principled framework for analyzing how social networks evolve over time.  To formally define the model, we start by framing them as a type of continuous-time Markov chain (CTMC), operating on a network space.

As previously introduced, a Continuous-Time Markov Chain (CTMC) is characterized by three key components:

-	A finite state space: For SAOMs, this space includes all possible directed networks (i.e., all adjacency matrices) that can be constructed from $n$ actors.
-	A continuous-time process: Network changes (such as the creation or dissolution of ties) occur at random, unpredictable points in time.
-	The Markov property: The likelihood of a transition depends solely on the network’s current configuration, not on how it arrived there.

In the sections that follow, we explore how each of these components applies specifically to SAOMs.

### State Space of Networks
Let $X$ be the set of all possible adjacency matrices (i.e., network configurations) defined on $n$ actors. Each matrix corresponds to a different possible state of the network. The size of this space is:

$$
|X| = 2^{n(n-1)}
$$

This comes from the fact that each of the $n(n-1)$ possible directed ties between distinct actors can independently be either present (1) or absent (0).

For instance, with a 4-node directed network shown in @fig-saomex, we can represent different adjacency matrices as the network evolves over time. We see a step-by-step representation of how a network evolves through successive tie changes, starting from an empty network and progressing toward a fully connected one.

::: {#fig-saomex .center}
![](img/‎‎saom-state-ex.png){width="100%"}

An example of how a directed network grows step by step as ties are added. Each panel pairs the network with its adjacency matrix, where a 1 indicates a directed tie. The process moves from an empty network to a fully connected one.
:::

### Continuous-time process
At their core, SAOMs treat network evolution as a **continuous-time Markov process**, where changes to the network occur in small steps (microsteps) and are **actor-driven**. That is, ties are not formed or dissolved randomly or all at once, but rather sequentially by individual actors who react to the current network structure and their own preferences.


Although the underlying network evolution occurs in continuous time, in practice, we only observe the network at discrete time points (e.g., waves in a longitudinal study). This setup implies that between observations, the network could have changed multiple times, but we only see the snapshots. The true process is latent between waves.

image: network states evolve at $t_0$, $t_1$, …, but what we observe are discrete “snapshots” of this hidden continuous-time trajectory.

In the context of Stochastic Actor-Oriented Models (SAOMs), the network evolution is modeled as a continuous-time Markov chain (CTMC). The Markov property means that:

The future state of the network depends only on the current state, not on the full sequence of past states.

Formally, this can be expressed as:
$$
P(X(t + \Delta t) = x’ \mid X(t) = x, X(s < t)) = P(X(t + \Delta t) = x’ \mid X(t) = x)
$$

This simplifies the modeling of dynamic social networks by assuming that all relevant information about the past is captured in the current configuration.

Interpretation in SAOMs
	•	The network at time $t$ evolves probabilistically from its current state $x$ to a new state $x’$.
	•	The set of all possible networks is enormous (there are $2^{n(n-1)}$ possible directed networks with $n$ actors), so modeling the transition probabilities directly for every possible $x \rightarrow x’$ is infeasible.

How SAOMs Address This Complexity

To make modeling tractable, SAOMs incorporate several simplifying assumptions:
	•	Actor-oriented changes: At each moment in continuous time, only one actor is considered to have an opportunity to make a change.
	•	Sequential updates: Only one tie can change at a time—this avoids co-occurring tie changes and reflects the reality of individual-level decisions.
	•	Sender-determined decisions: The actor sending the tie decides whether to create, maintain, or dissolve it, based on a probabilistic evaluation of the current network.

⸻

These assumptions, combined with the Markov property, allow SAOMs to model dynamic networks with computational feasibility and theoretical clarity. They focus on micro-level tie changes that, over time, give rise to macro-level network structures.
Key features of SAOMs include:

- **Directed, non-reflexive ties**: SAOMs model directed relationships, such as friendship nominations, where actors cannot nominate themselves (no self-loops).
- **Ties tend to persist**: The models assume a degree of stability, i.e., ties formed in the past are likely to endure unless actors actively choose to dissolve them.
- **Microsteps between observations**: Although we observe networks only at discrete time points (e.g., waves in a panel study), SAOMs assume the underlying process evolves in **continuous time**, with actors making decisions one at a time in between observations.
- **Actor agency**: Each actor is assumed to make **sequential and intentional** decisions about adding, dropping, or maintaining ties based on an **objective function** that quantifies their preference over possible network configurations.

The process is driven by two main components:

1. **Rate function**: Determines how often an actor gets the opportunity to make a network change.
2. **Objective function**: Governs which network change the actor is likely to make, based on structural features (like reciprocity or transitivity) and covariates.

Formally, the SAOM framework assumes that actors act **myopically**: they evaluate the immediate consequences of a tie change and choose the one that increases (or is more likely to increase) their objective function. The model does not assume long-term strategic planning.

This actor-oriented perspective makes SAOMs particularly well-suited for modeling network dynamics in settings where individuals actively manage their ties, such as adolescent friendship networks, organizational communication, or online social platforms.

The SAOM thus builds directly on the ideas of **continuous-time Markov chains**, using them as a scaffold for representing the unfolding micro-dynamics of network change, with **actor choice and agency** at its core.
